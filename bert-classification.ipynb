{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePVSga5E_a-e"
   },
   "source": [
    "# Clasificación de secuencias de texto con BERT y Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_pqv14cAvww"
   },
   "source": [
    "> Objetivos:\n",
    "\n",
    "\n",
    "\n",
    "*   Procesar una base de datos de texto y adapatarla a lightning.\n",
    "*   Importar modelo BERT pre-entrenado.\n",
    "*   Hacer finetuning de BERT para detección de comentarios negativos o tóxicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M5FAipmhUBw"
   },
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade kagglehub\n",
    "# !pip install --upgrade polars\n",
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3736,
     "status": "ok",
     "timestamp": 1721169622374,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "y5mj8cpdM_O3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, RichProgressBar\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from torchmetrics import AUROC, Accuracy\n",
    "\n",
    "#nuevas librerias\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1721169622486,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "3vBmTBBIhn-Z",
    "outputId": "c173fa77-03b0-402a-8acf-8c9cf3d5000c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parametros para gráficos\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 8,6\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btZjckeJs5j7"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "El dataset IMDB contiene 50,000 reseñas de películas etiquetadas como positivas o negativas. Fue descargado desde Kaggle usando el repositorio [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12133,
     "status": "ok",
     "timestamp": 1721169634852,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "zpT48Rwl94aP",
    "outputId": "72e390c1-bf65-4fc6-d9be-af85ce939991"
   },
   "outputs": [],
   "source": [
    "# Descargar la base de datos\n",
    "#!gdown --id 1VuQ-U7TtggShMeuRSA_hzC8qGDl2LRkr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEQ-aGlT5yTU"
   },
   "source": [
    "Dado que son solo 68.8M de datos, los cargamos en memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n",
      "Using CSV: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\n",
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-83d4b1be-95e5-4f09-935d-676271fdb1c0\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83d4b1be-95e5-4f09-935d-676271fdb1c0')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-83d4b1be-95e5-4f09-935d-676271fdb1c0 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-83d4b1be-95e5-4f09-935d-676271fdb1c0');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEFORE importing kagglehub\n",
    "from google.colab import userdata\n",
    "\n",
    "# Mock userdata.get to return None immediately\n",
    "_original_get = userdata.get\n",
    "userdata.get = lambda key: None  # Skip all secret prompts\n",
    "\n",
    "# Descargar la última versión\n",
    "dataset_path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "\n",
    "# Si es un ZIP, extraer a directorio temporal\n",
    "if os.path.isfile(dataset_path) and dataset_path.lower().endswith(\".zip\"):\n",
    "    extract_dir = tempfile.mkdtemp()\n",
    "    with zipfile.ZipFile(dataset_path, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "    search_dir = extract_dir\n",
    "else:\n",
    "    search_dir = dataset_path\n",
    "\n",
    "# Buscar archivos CSV\n",
    "csv_files = glob.glob(os.path.join(search_dir, \"**\", \"*.csv\"), recursive=True)\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found under {search_dir}\")\n",
    "\n",
    "csv_file = csv_files[0]\n",
    "print(\"Using CSV:\", csv_file)\n",
    "\n",
    "# Leer con pandas (fallback de encoding si falla)\n",
    "try:\n",
    "    df = pd.read_csv(csv_file)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(csv_file, encoding=\"latin1\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1721169635652,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "6v7DW58wrm-w",
    "outputId": "1be84c8a-1e3c-4eb5-a26c-1dbf3f3e967c"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"toxic_comments.csv\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX26ij7N6Uv_"
   },
   "source": [
    "Las reseñas de películas están en la columna `review` y las etiquetas de sentimiento en la columna `sentiment` (positive/negative). \n",
    "\n",
    "Para adaptar el código original de clasificación multi-etiqueta a clasificación binaria de sentimientos, creamos una columna `is_offensive` que mapea:\n",
    "- negative → 1 (considerado como \"ofensivo\" para el modelo)  \n",
    "- positive → 0 (considerado como \"no ofensivo\" para el modelo)\n",
    "\n",
    "<!-- Descripción original comentada:\n",
    "Las secuencias de texto están en la columna comment_text. Existen seis etiquetas que clasifican cada secuencia como tóxica, severamente tóxica, obscena, etc. Las columnas con comentarios no ofensivos tienen un 0 en todas las clases.\n",
    "\n",
    "Separamos los comentarios ofensivos de los no ofensivos:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1721169648586,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "HWRU499BkPIe",
    "outputId": "e6158972-2467-428f-90d4-6b7e9d67ec1f"
   },
   "outputs": [],
   "source": [
    "#LABEL_COLUMNS = df.columns.tolist()[2:]\n",
    "#df_toxic = df[df[LABEL_COLUMNS].sum(axis=1) > 0]\n",
    "#df_clean = df[df[LABEL_COLUMNS].sum(axis=1) == 0]\n",
    "#print(\"Cantidad de secuencias de texto ofensivos: \", len(df_toxic))\n",
    "#print(\"Cantidad de secuencias de texto NO ofensivos: \", len(df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de reseñas negativas (tratadas como ofensivas):  25000\n",
      "Cantidad de reseñas positivas (tratadas como no ofensivas):  25000\n",
      "Distribución de sentimientos:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convertir el dataset IMDB para clasificación binaria\n",
    "# Mapea 'negative' a 1 (ofensivo) y 'positive' a 0 (no ofensivo)\n",
    "df['is_offensive'] = df['sentiment'].map({'negative': 1, 'positive': 0})\n",
    "\n",
    "# Renombrar columnas para compatibilidad con el código existente\n",
    "df = df.rename(columns={'review': 'comment_text'})\n",
    "\n",
    "# Crear columna de etiquetas numéricas para el modelo\n",
    "df['label'] = df['is_offensive']\n",
    "\n",
    "# Filtra por sentimiento\n",
    "df_origin = df[df['is_offensive'] == 1]  # Negativos (tratados como ofensivos)\n",
    "df_clean = df[df['is_offensive'] == 0]   # Positivos (tratados como no ofensivos)\n",
    "\n",
    "print(\"Cantidad de reseñas negativas (tratadas como ofensivas): \", len(df_origin))\n",
    "print(\"Cantidad de reseñas positivas (tratadas como no ofensivas): \", len(df_clean))\n",
    "print(\"Distribución de sentimientos:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy2jD4ntkqIx"
   },
   "source": [
    "El dataset IMDB ya está balanceado (25,000 reseñas positivas y 25,000 negativas), pero para mantener la compatibilidad con el código original, ajustamos el tamaño si es necesario.\n",
    "\n",
    "<!-- Comentario original:\n",
    "Debido al desbalance de datos, tomamos un número reducido de secuencias no ofensivas para evitar bias en el modelo.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1721169650229,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "NHydMQQ2lCTS",
    "outputId": "1fda53f9-e601-466b-c115-c38095bcf39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset filtrado: (50000, 4)\n",
      "Distribución final:\n",
      "sentiment\n",
      "negative    25000\n",
      "positive    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# El dataset IMDB ya está balanceado (25k positivas, 25k negativas)\n",
    "# Usar todo el dataset o una muestra si se requiere menos datos para pruebas rápidas\n",
    "\n",
    "# Para mantener compatibilidad con el código original:\n",
    "filter_df = pd.concat([\n",
    "    df_origin,      # Todas las reseñas negativas (25,000)\n",
    "    df_clean        # Todas las reseñas positivas (25,000)\n",
    "])\n",
    "\n",
    "# Si quieres usar solo una muestra para pruebas más rápidas, descomenta:\n",
    "# filter_df = pd.concat([\n",
    "#     df_origin.sample(5000, random_state=42),\n",
    "#     df_clean.sample(5000, random_state=42)\n",
    "# ])\n",
    "\n",
    "print(f\"Tamaño del dataset filtrado: {filter_df.shape}\")\n",
    "print(\"Distribución final:\")\n",
    "print(filter_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1721169650990,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "EtdEvIaPmdRi",
    "outputId": "cda6b360-063c-4912-c363-d6516d8a2faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      " 35000\n",
      "Validation DataFrame:\n",
      " 7500\n",
      "Test DataFrame:\n",
      " 7500\n"
     ]
    }
   ],
   "source": [
    "train_frac, val_frac, test_frac = 0.7, 0.15, 0.15\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "filter_df = filter_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Calcular los indices para separar los subsets\n",
    "train_end = int(train_frac * len(filter_df))\n",
    "val_end = train_end + int(val_frac * len(filter_df))\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df = filter_df[:train_end]\n",
    "val_df = filter_df[train_end:val_end]\n",
    "test_df = filter_df[val_end:]\n",
    "\n",
    "print(\"Train DataFrame:\\n\", len(train_df))\n",
    "print(\"Validation DataFrame:\\n\", len(val_df))\n",
    "print(\"Test DataFrame:\\n\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1721169659688,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "MjPnsRP6nGJP",
    "outputId": "ee04309e-e04a-41be-c622-524ea4d38d87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-53be15f9-2c21-4a8a-8366-67aed37a9345\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>is_offensive</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, I was wondering if anyone has a copy of...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My Caddy Limo was destroyed!!! Well, I had one...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A truly muddled incomprehensible mess. Most th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My friends and I rented this for \"Bad Movie Ni...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This was the very first kung fu movie that I h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53be15f9-2c21-4a8a-8366-67aed37a9345')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-53be15f9-2c21-4a8a-8366-67aed37a9345 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-53be15f9-2c21-4a8a-8366-67aed37a9345');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                        comment_text sentiment  is_offensive  \\\n",
       "0  Hello, I was wondering if anyone has a copy of...  positive             0   \n",
       "1  My Caddy Limo was destroyed!!! Well, I had one...  negative             1   \n",
       "2  A truly muddled incomprehensible mess. Most th...  negative             1   \n",
       "3  My friends and I rented this for \"Bad Movie Ni...  negative             1   \n",
       "4  This was the very first kung fu movie that I h...  positive             0   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EU9fW3KD7V0V"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "\n",
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwBpI_G0FjwO"
   },
   "source": [
    "\n",
    "Convertir texto en una lista de tokens. usamos BertTokenizer pre-entrenado.\n",
    "\n",
    "BertTokenizer: Pretrained model on English language using a masked language modeling (MLM) objective. This model is case-sensitive: it makes a difference between english and English.\n",
    "The texts are tokenized using WordPiece and a vocabulary size of 30,000.\n",
    "\n",
    "Más información en: https://huggingface.co/google-bert/bert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2327,
     "status": "ok",
     "timestamp": 1721169670726,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "fU8CYzKonYft",
    "outputId": "4fc97edb-ec7e-43b8-e05a-9e402f5a0a8d"
   },
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1721169671951,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "FzQ5MpcQpp4v",
    "outputId": "451d0ac7-8322-4bdc-f32f-3aee1401cf72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb6VLAmGHeEA"
   },
   "source": [
    "Tokenizando una secuencia de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1721169675106,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "e_URTylSXar1",
    "outputId": "d0581398-18d8-470a-f505-37a7c2132212"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_text    Despite some negative comments this film has g...\n",
       "sentiment                                                positive\n",
       "is_offensive                                                    0\n",
       "label                                                           0\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_df.iloc[5]\n",
    "sample_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1721169676238,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "3kDTQ5_6RnkI",
    "outputId": "961adc9e-f977-4eb3-bab5-cfa1355535bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimiento: positive\n",
      "Reseña: Despite some negative comments this film has garnered in the IMDb pages, it's still worth a look as this is a story about survival and camaraderie between two different men with different mentalities ...\n",
      "Etiqueta is_offensive: 0\n",
      "Etiqueta numérica (label): 0\n"
     ]
    }
   ],
   "source": [
    "sample_sentiment = sample_row.sentiment\n",
    "print(\"Sentimiento:\", sample_sentiment)\n",
    "\n",
    "sample_comment = sample_row.comment_text\n",
    "print(\"Reseña:\", sample_comment[:200] + \"...\" if len(sample_comment) > 200 else sample_comment)\n",
    "\n",
    "print(\"Etiqueta is_offensive:\", sample_row.is_offensive)\n",
    "print(\"Etiqueta numérica (label):\", sample_row.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1721169678834,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "DjLOQUgzUexM",
    "outputId": "0c2acd3a-1f9a-4f4d-9b4e-db2a1d454ae2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[  101,  2711,  1199,  4366,  7640,  1142,  1273,  1144, 13331,  1107,\n",
       "          1103,   146, 18219,  1830,  5097,   117,  1122,   112,   188,  1253,\n",
       "          3869,   170,  1440,  1112,  1142,  1110,   170,  1642,  1164,  8115,\n",
       "          1105, 11019, 20377,  2692,  1663,  1206,  1160,  1472,  1441,  1114,\n",
       "          1472,  4910,  4233,  1229,  1107,   170,  2846,  2862,  1107,  1103,\n",
       "          8392, 11091, 11800,   119,   133,  9304,   120,   135,   133,  9304,\n",
       "           120,   135, 16289,  1900,  6132,   149,  8867,  1161,  2274,  1366,\n",
       "          1373,  1106,  2824,  1142, 11826,  1383,  1107,  8392,   119,  1109,\n",
       "          1273,  1144,  1199,  1363,  4899,  1112, 22562,   117,  1103,  8230,\n",
       "          5243,   117,  2274,   170,  3599,  2474,  1299,   117,  3055,  1850,\n",
       "          1106,  2222,  1106, 11125,   170, 14140,  3850, 12411,  1883,  1246,\n",
       "          1105,  1103, 14644,  2306,  1704,  1150,  1547,  1129,  1103,  1397,\n",
       "          2084,  1104,  1103,  1583,   119,  1109,  1178,  2463,   117,  3902,\n",
       "           117,  1144,  1185,  2541,  1107,  1184,  1119,  1144,  1151, 19469,\n",
       "          1106,  5515,   119,   133,  9304,   120,   135,   133,  9304,   120,\n",
       "           135,  3902,   117,  1103, 17346,  3599,  2474,  1299,  1106,  1103,\n",
       "         11800,  1105,  1106,  1103, 18850,  9405,  1206,  1103,  1764,  1105,\n",
       "          3850, 17499,  1222,  1103,  1107,  8702,  6066,  5894,  1237,  4810,\n",
       "          1441,   117, 10123,   170,  7468, 11788,  1121, 22562,   119,  1327,\n",
       "          2736,  1363,  1107,  2749,   117,  1110, 25707,  1107,  1103, 11800,\n",
       "           119,   133,  9304,   120,   135,   133,  9304,   120,   135,  2545,\n",
       "          4108,  5123,  2895,   117,  1110,  1126,  2811,  1150,  2144,   112,\n",
       "           189,  8077,  1277,   117,  1112,  1119, 17617,  1107,  1142,  2523,\n",
       "           117,  1133,  1107,  1103,  5618,  1104,  1103,  2523,   117,  1119,\n",
       "          1110,  1268,  1112,   170,  1299,  1104,   170,  1374,  1734,   119,\n",
       "          4224,  6359,  1773,  3902,  1674,  1184,  1119,  1169,  1114,   170,\n",
       "          1648,  1115,  2144,   112,   189,  8658,  1140,  1251, 12887,  1235,\n",
       "          1103, 10268,  1322,   119,   133,  9304,   120,   135,   133,  9304,\n",
       "           120,   135,  1370, 12977,  1104,  2168,  1273,   117,   107,   156,\n",
       "          2605,  3365,   107,  3272,  1126, 11150,  1904,  1104,  2168,  1115,\n",
       "          1114,   170,  2113,  1104, 13373,  5031,  1156,  1138,  1189,   170,\n",
       "          1167, 18330,  2523,   119,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sample = tokenizer.encode_plus(\n",
    "  sample_comment,  # Usar sample_comment en lugar de sample_comment\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "encoding_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1721169680157,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "V0e_EWTzX0FD",
    "outputId": "4a1d945f-2769-4e13-ca50-d498c254d1f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2711,  1199,  4366,  7640,  1142,  1273,  1144, 13331,  1107,\n",
       "          1103,   146, 18219,  1830,  5097,   117,  1122,   112,   188,  1253,\n",
       "          3869,   170,  1440,  1112,  1142,  1110,   170,  1642,  1164,  8115,\n",
       "          1105, 11019, 20377,  2692,  1663,  1206,  1160,  1472,  1441,  1114,\n",
       "          1472,  4910,  4233,  1229,  1107,   170,  2846,  2862,  1107,  1103,\n",
       "          8392, 11091, 11800,   119,   133,  9304,   120,   135,   133,  9304,\n",
       "           120,   135, 16289,  1900,  6132,   149,  8867,  1161,  2274,  1366,\n",
       "          1373,  1106,  2824,  1142, 11826,  1383,  1107,  8392,   119,  1109,\n",
       "          1273,  1144,  1199,  1363,  4899,  1112, 22562,   117,  1103,  8230,\n",
       "          5243,   117,  2274,   170,  3599,  2474,  1299,   117,  3055,  1850,\n",
       "          1106,  2222,  1106, 11125,   170, 14140,  3850, 12411,  1883,  1246,\n",
       "          1105,  1103, 14644,  2306,  1704,  1150,  1547,  1129,  1103,  1397,\n",
       "          2084,  1104,  1103,  1583,   119,  1109,  1178,  2463,   117,  3902,\n",
       "           117,  1144,  1185,  2541,  1107,  1184,  1119,  1144,  1151, 19469,\n",
       "          1106,  5515,   119,   133,  9304,   120,   135,   133,  9304,   120,\n",
       "           135,  3902,   117,  1103, 17346,  3599,  2474,  1299,  1106,  1103,\n",
       "         11800,  1105,  1106,  1103, 18850,  9405,  1206,  1103,  1764,  1105,\n",
       "          3850, 17499,  1222,  1103,  1107,  8702,  6066,  5894,  1237,  4810,\n",
       "          1441,   117, 10123,   170,  7468, 11788,  1121, 22562,   119,  1327,\n",
       "          2736,  1363,  1107,  2749,   117,  1110, 25707,  1107,  1103, 11800,\n",
       "           119,   133,  9304,   120,   135,   133,  9304,   120,   135,  2545,\n",
       "          4108,  5123,  2895,   117,  1110,  1126,  2811,  1150,  2144,   112,\n",
       "           189,  8077,  1277,   117,  1112,  1119, 17617,  1107,  1142,  2523,\n",
       "           117,  1133,  1107,  1103,  5618,  1104,  1103,  2523,   117,  1119,\n",
       "          1110,  1268,  1112,   170,  1299,  1104,   170,  1374,  1734,   119,\n",
       "          4224,  6359,  1773,  3902,  1674,  1184,  1119,  1169,  1114,   170,\n",
       "          1648,  1115,  2144,   112,   189,  8658,  1140,  1251, 12887,  1235,\n",
       "          1103, 10268,  1322,   119,   133,  9304,   120,   135,   133,  9304,\n",
       "           120,   135,  1370, 12977,  1104,  2168,  1273,   117,   107,   156,\n",
       "          2605,  3365,   107,  3272,  1126, 11150,  1904,  1104,  2168,  1115,\n",
       "          1114,   170,  2113,  1104, 13373,  5031,  1156,  1138,  1189,   170,\n",
       "          1167, 18330,  2523,   119,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sample.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1721169684465,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "tlqeyA3FZJaz",
    "outputId": "1e765808-21db-4cb8-c3b8-4ee4a455d62c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sample.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1721169686119,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "t2Qoj-RJU_KR",
    "outputId": "5d415e8d-ad48-4452-ee28-38f8b9ac7293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sample[\"input_ids\"].shape, encoding_sample[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfjz0npA3cVR"
   },
   "source": [
    "La tokenización entrega un diccionario con los IDs de los tokens en la key `input_ids` y las máscaras de atención en `attention_mask`.\n",
    "\n",
    "También podemos convertir los IDs a las subpalabras originales:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1721169689584,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "ib74R7MVsjmS",
    "outputId": "b20b9f2b-3b57-4b61-a3e5-5c407aef8888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Despite', 'some', 'negative', 'comments', 'this', 'film', 'has', 'garnered', 'in', 'the', 'I', '##MD', '##b', 'pages', ',', 'it', \"'\", 's', 'still', 'worth', 'a', 'look', 'as', 'this', 'is', 'a', 'story', 'about', 'survival', 'and', 'ca', '##mara', '##der', '##ie', 'between', 'two', 'different', 'men', 'with', 'different', 'mental', '##ities', 'while', 'in', 'a', 'difficult', 'mission', 'in', 'the', 'Panama', '##nian', 'jungle', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Peruvian', 'director', 'Luis', 'L', '##los', '##a', 'takes', 'us', 'along', 'to', 'watch', 'this', 'thriller', 'set', 'in', 'Panama', '.', 'The', 'film', 'has', 'some', 'good', 'moments', 'as', 'Beckett', ',', 'the', 'veteran', 'marine', ',', 'takes', 'a', 'newly', 'arrived', 'man', ',', 'recently', 'sent', 'to', 'try', 'to', 'eliminate', 'a', 'notorious', 'drug', 'cart', '##el', 'head', 'and', 'the', 'corrupt', 'army', 'general', 'who', 'might', 'be', 'the', 'next', 'president', 'of', 'the', 'country', '.', 'The', 'only', 'problem', ',', 'Miller', ',', 'has', 'no', 'experience', 'in', 'what', 'he', 'has', 'been', 'entrusted', 'to', 'achieve', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Miller', ',', 'the', 'arrogant', 'newly', 'arrived', 'man', 'to', 'the', 'jungle', 'and', 'to', 'the', 'guerrilla', 'warfare', 'between', 'the', 'military', 'and', 'drug', 'lords', 'against', 'the', 'in', '##fi', '##lt', '##rated', 'American', 'intelligence', 'men', ',', 'learns', 'a', 'valuable', 'lesson', 'from', 'Beckett', '.', 'What', 'looks', 'good', 'in', 'theory', ',', 'is', 'irrelevant', 'in', 'the', 'jungle', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Tom', 'Be', '##ren', '##ger', ',', 'is', 'an', 'actor', 'who', 'doesn', \"'\", 't', 'register', 'much', ',', 'as', 'he', 'proves', 'in', 'this', 'movie', ',', 'but', 'in', 'the', 'context', 'of', 'the', 'movie', ',', 'he', 'is', 'right', 'as', 'a', 'man', 'of', 'a', 'few', 'words', '.', 'Billy', 'Zane', 'playing', 'Miller', 'does', 'what', 'he', 'can', 'with', 'a', 'role', 'that', 'doesn', \"'\", 't', 'afford', 'him', 'any', 'glory', 'until', 'the', 'crucial', 'end', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'For', 'lovers', 'of', 'action', 'film', ',', '\"', 'S', '##ni', '##per', '\"', 'offers', 'an', '112', 'minutes', 'of', 'action', 'that', 'with', 'a', 'bit', 'of', 'trim', '##ming', 'would', 'have', 'made', 'a', 'more', 'satisfying', 'movie', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding_sample[\"input_ids\"].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCh7o863Zcuc"
   },
   "source": [
    "[CLS] indica clasificacion, en este caso a nivel de oracion.\n",
    "\n",
    "Classify Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YALfbA2qKNp7"
   },
   "source": [
    "Especificamos el máximo número de tokens por secuencia: 512.\n",
    "\n",
    "Analizamos la longitud de las secuencias en el grupo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 6769,
     "status": "ok",
     "timestamp": 1721169725148,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "ymXBT82pIlpy"
   },
   "outputs": [],
   "source": [
    "token_counts = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "  token_count = len(tokenizer.encode(\n",
    "    row[\"comment_text\"],\n",
    "    max_length=1024,\n",
    "    truncation=True\n",
    "  ))\n",
    "  token_counts.append(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1721169725154,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "c4x1nEccsvaO",
    "outputId": "0edbd8e8-f415-4c79-81d6-915d7640586e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean token count: 314.1051142857143\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos la media\n",
    "mean = np.mean(token_counts)\n",
    "print(\"Mean token count:\", mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1721169725837,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "Lm8tHSuhJWNM",
    "outputId": "293ac0bc-8de7-462d-8a0a-a6ee509e2ac8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 513,
       "width": 734
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(token_counts)\n",
    "plt.xlim([0, 1024]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1721169729115,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "D63Sg9SzejIZ"
   },
   "outputs": [],
   "source": [
    "# Definimos hiperparámetro de máximo número de tokens en la secuencia\n",
    "MAX_TOKEN_COUNT = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgXNPSXU77oc"
   },
   "source": [
    "### Modulo Dataset que incluya el tokenizador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1721170110929,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "CQ1YdjRlYAxF"
   },
   "outputs": [],
   "source": [
    "from utils.data import IMDBDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nyqOozs9anR"
   },
   "source": [
    "Tomando una muestra del Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1721170112102,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "M72dsvC-CDZD",
    "outputId": "1ba70f2e-6e7a-40ba-d4d6-37eb4afc2e42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usar la nueva clase IMDBDataset\n",
    "train_dataset = IMDBDataset(train_df, tokenizer, max_token_len=MAX_TOKEN_COUNT)\n",
    "\n",
    "sample_item = train_dataset[0]\n",
    "sample_item.keys()\n",
    "\n",
    "# Código original comentado:\n",
    "# train_dataset = ToxicCommentsDataset(train_df, tokenizer, max_token_len=MAX_TOKEN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1721170117754,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "R6L8exQSiYM-",
    "outputId": "488e4d5a-daad-4ebd-d08c-5becd6dbda4b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello, I was wondering if anyone has a copy of the movie Broken Promise?? I loved this movie growing up and watched it every time it was on. It has been YEARS since I have seen it and would love to get a copy. I have checked all of the internet and have been unable to find it. If anyone has a copy to trade or sell em please email me at NoelGypsy@Yahoo.com... Thank you and have a great night!! Christine --------- The \"broken promise\" was made to eleven-year-old Melissa Michaelsen, whose parents have deserted her and her siblings. Taken in by the County, Michaelsen has had to watch helplessly as her brothers and sisters are split up and farmed out to different families. One of the kids is even institutionalized. Juvenile court officer Chris Sarandon joins Michaelsen in her struggle to reunite her family under one roof. Broken Promise was originally offered as a \"General Foods Golden Showcase\" presentation. It was first telecast May 5, 1981.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1721170121481,
     "user": {
      "displayName": "Karen Rosero",
      "userId": "14192812651435042745"
     },
     "user_tz": 300
    },
    "id": "ncSfaKkqiaFB",
    "outputId": "ba904c37-9fda-4c54-cce5-d27022a1daa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentimentClassifier\n",
    "from utils.model import SentimentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import SentimentDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 2187\n",
      "Total training steps: 6561\n",
      "Warmup steps: 1312\n",
      "Modelo y DataModule creados exitosamente para clasificación de sentimientos IMDB\n"
     ]
    }
   ],
   "source": [
    "# Configuración de entrenamiento actualizada para clasificación binaria IMDB\n",
    "N_EPOCHS = 3  # Reducido para pruebas más rápidas\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Crear DataModule\n",
    "data_module = SentimentDataModule(\n",
    "    train_df,\n",
    "    val_df, \n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "# Calcular pasos de entrenamiento\n",
    "steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total training steps: {total_training_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Crear modelo\n",
    "model = SentimentClassifier(\n",
    "    n_warmup_steps=warmup_steps,\n",
    "    n_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "print(\"Modelo y DataModule creados exitosamente para clasificación de sentimientos IMDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento del modelo BERT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-06 19:02:05.517127: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757185325.540454  747599 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757185325.547794  747599 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757185325.565973  747599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185325.566001  747599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185325.566005  747599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185325.566007  747599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Dataset path: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n",
      "Using CSV: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\n",
      "(50000, 2)\n",
      "Train DataFrame:\n",
      " 35000\n",
      "Validation DataFrame:\n",
      " 7500\n",
      "Test DataFrame:\n",
      " 7500\n",
      "Steps per epoch: 2187\n",
      "Total training steps: 6561\n",
      "Warmup steps: 1312\n",
      "Modelo y DataModule creados exitosamente para clasificación de sentimientos IMDB\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer configurado para clasificación de sentimientos IMDB\n",
      "Iniciando entrenamiento del modelo de clasificación de sentimientos...\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "2025-09-06 19:02:17.645338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757185337.668498  747942 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757185337.675821  747942 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757185337.693803  747942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.693831  747942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.693833  747942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.693835  747942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-06 19:02:17.722837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757185337.745945  747943 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757185337.753064  747943 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-06 19:02:17.768082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "W0000 00:00:1757185337.770796  747943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.770828  747943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.770830  747943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.770832  747943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757185337.790970  747944 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757185337.798007  747944 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757185337.815753  747944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.815783  747944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.815785  747944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757185337.815787  747944 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Dataset path: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n",
      "Using CSV: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\n",
      "Dataset path: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n",
      "Using CSV: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\n",
      "Dataset path: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n",
      "Using CSV: /home/eaguayo/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\n",
      "(50000, 2)\n",
      "(50000, 2)\n",
      "Train DataFrame:\n",
      " 35000\n",
      "Validation DataFrame:\n",
      " 7500\n",
      "Test DataFrame:\n",
      " 7500\n",
      "Train DataFrame:\n",
      " 35000\n",
      "Validation DataFrame:\n",
      " 7500\n",
      "Test DataFrame:\n",
      " 7500\n",
      "(50000, 2)\n",
      "Train DataFrame:\n",
      " 35000\n",
      "Validation DataFrame:\n",
      " 7500\n",
      "Test DataFrame:\n",
      " 7500\n",
      "Steps per epoch: 2187\n",
      "Total training steps: 6561\n",
      "Warmup steps: 1312\n",
      "Steps per epoch: 2187\n",
      "Total training steps: 6561\n",
      "Warmup steps: 1312\n",
      "Steps per epoch: 2187\n",
      "Total training steps: 6561\n",
      "Warmup steps: 1312\n",
      "Modelo y DataModule creados exitosamente para clasificación de sentimientos IMDB\n",
      "Modelo y DataModule creados exitosamente para clasificación de sentimientos IMDB\n",
      "Modelo y DataModule creados exitosamente para clasificación de sentimientos IMDB\n",
      "Trainer configurado para clasificación de sentimientos IMDB\n",
      "Iniciando entrenamiento del modelo de clasificación de sentimientos...\n",
      "Trainer configurado para clasificación de sentimientos IMDB\n",
      "Iniciando entrenamiento del modelo de clasificación de sentimientos...\n",
      "Trainer configurado para clasificación de sentimientos IMDB\n",
      "Iniciando entrenamiento del modelo de clasificación de sentimientos...\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/eaguayo/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:701: Checkpoint directory /home/eaguayo/workspace/DeepLearning/Week3/bert-classifier/sentiment_checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name        | Type              | Params | Mode\n",
      "----------------------------------------------------------\n",
      "0 | bert        | BertModel         | 108 M  | eval\n",
      "1 | classifier  | Linear            | 769    | train\n",
      "2 | criterion   | BCEWithLogitsLoss | 0      | train\n",
      "3 | train_auroc | BinaryAUROC       | 0      | train\n",
      "4 | val_auroc   | BinaryAUROC       | 0      | train\n",
      "----------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.244   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "228       Modules in eval mode\n",
      "\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "\n",
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]/home/eaguayo/.local/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  2.37it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]/home/eaguayo/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\n",
      "\n",
      "\n",
      "Training: |          | 0/? [00:00<?, ?it/s]\n",
      "Training:   0%|          | 0/547 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/547 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 1/547 [00:01<11:08,  0.82it/s]\n",
      "Epoch 0:   0%|          | 1/547 [00:01<11:10,  0.81it/s, v_num=3, train/loss_step=0.668]\n",
      "Epoch 0:   0%|          | 2/547 [00:01<06:42,  1.36it/s, v_num=3, train/loss_step=0.668]\n",
      "Epoch 0:   0%|          | 2/547 [00:02<10:33,  0.86it/s, v_num=3, train/loss_step=0.754]\n",
      "Epoch 0:   1%|          | 3/547 [00:02<07:46,  1.17it/s, v_num=3, train/loss_step=0.754]\n",
      "Epoch 0:   1%|          | 3/547 [00:03<10:18,  0.88it/s, v_num=3, train/loss_step=0.825]\n",
      "Epoch 0:   1%|          | 4/547 [00:03<08:16,  1.09it/s, v_num=3, train/loss_step=0.825]\n",
      "Epoch 0:   1%|          | 4/547 [00:04<10:10,  0.89it/s, v_num=3, train/loss_step=0.862]\n",
      "Epoch 0:   1%|          | 5/547 [00:04<08:34,  1.05it/s, v_num=3, train/loss_step=0.862]\n",
      "Epoch 0:   1%|          | 5/547 [00:05<10:06,  0.89it/s, v_num=3, train/loss_step=0.698]\n",
      "Epoch 0:   1%|          | 6/547 [00:05<08:46,  1.03it/s, v_num=3, train/loss_step=0.698]\n",
      "Epoch 0:   1%|          | 6/547 [00:06<10:03,  0.90it/s, v_num=3, train/loss_step=0.711]\n",
      "Epoch 0:   1%|▏         | 7/547 [00:06<08:55,  1.01it/s, v_num=3, train/loss_step=0.711]\n",
      "Epoch 0:   1%|▏         | 7/547 [00:07<10:00,  0.90it/s, v_num=3, train/loss_step=0.805]\n",
      "Epoch 0:   1%|▏         | 8/547 [00:08<09:00,  1.00it/s, v_num=3, train/loss_step=0.805]\n",
      "Epoch 0:   1%|▏         | 8/547 [00:08<09:57,  0.90it/s, v_num=3, train/loss_step=0.719]\n",
      "Epoch 0:   2%|▏         | 9/547 [00:09<09:04,  0.99it/s, v_num=3, train/loss_step=0.719]\n",
      "Epoch 0:   2%|▏         | 9/547 [00:09<09:55,  0.90it/s, v_num=3, train/loss_step=0.959]\n",
      "Epoch 0:   2%|▏         | 10/547 [00:10<09:07,  0.98it/s, v_num=3, train/loss_step=0.959]\n",
      "Epoch 0:   2%|▏         | 10/547 [00:11<09:53,  0.91it/s, v_num=3, train/loss_step=0.657]\n",
      "Epoch 0:   2%|▏         | 11/547 [00:11<09:10,  0.97it/s, v_num=3, train/loss_step=0.657]\n",
      "Epoch 0:   2%|▏         | 11/547 [00:12<09:51,  0.91it/s, v_num=3, train/loss_step=0.656]\n",
      "Epoch 0:   2%|▏         | 12/547 [00:12<09:11,  0.97it/s, v_num=3, train/loss_step=0.656]\n",
      "Epoch 0:   2%|▏         | 12/547 [00:13<09:49,  0.91it/s, v_num=3, train/loss_step=0.854]\n",
      "Epoch 0:   2%|▏         | 13/547 [00:13<09:13,  0.97it/s, v_num=3, train/loss_step=0.854]\n",
      "Epoch 0:   2%|▏         | 13/547 [00:14<09:47,  0.91it/s, v_num=3, train/loss_step=0.884]\n",
      "Epoch 0:   3%|▎         | 14/547 [00:14<09:13,  0.96it/s, v_num=3, train/loss_step=0.884]\n",
      "Epoch 0:   3%|▎         | 14/547 [00:15<09:46,  0.91it/s, v_num=3, train/loss_step=0.609]\n",
      "Epoch 0:   3%|▎         | 15/547 [00:15<09:14,  0.96it/s, v_num=3, train/loss_step=0.609]\n",
      "Epoch 0:   3%|▎         | 15/547 [00:16<09:44,  0.91it/s, v_num=3, train/loss_step=0.782]\n",
      "Epoch 0:   3%|▎         | 16/547 [00:16<09:15,  0.96it/s, v_num=3, train/loss_step=0.782]\n",
      "Epoch 0:   3%|▎         | 16/547 [00:17<09:43,  0.91it/s, v_num=3, train/loss_step=0.655]\n",
      "Epoch 0:   3%|▎         | 17/547 [00:17<09:15,  0.95it/s, v_num=3, train/loss_step=0.655]\n",
      "Epoch 0:   3%|▎         | 17/547 [00:18<09:41,  0.91it/s, v_num=3, train/loss_step=0.740]\n",
      "Epoch 0:   3%|▎         | 18/547 [00:18<09:15,  0.95it/s, v_num=3, train/loss_step=0.740]\n",
      "Epoch 0:   3%|▎         | 18/547 [00:19<09:40,  0.91it/s, v_num=3, train/loss_step=0.693]\n",
      "Epoch 0:   3%|▎         | 19/547 [00:19<09:15,  0.95it/s, v_num=3, train/loss_step=0.693]\n",
      "Epoch 0:   3%|▎         | 19/547 [00:20<09:39,  0.91it/s, v_num=3, train/loss_step=0.738]\n",
      "Epoch 0:   4%|▎         | 20/547 [00:21<09:15,  0.95it/s, v_num=3, train/loss_step=0.738]\n",
      "Epoch 0:   4%|▎         | 20/547 [00:21<09:38,  0.91it/s, v_num=3, train/loss_step=0.837]\n",
      "Epoch 0:   4%|▍         | 21/547 [00:22<09:15,  0.95it/s, v_num=3, train/loss_step=0.837]\n",
      "Epoch 0:   4%|▍         | 21/547 [00:23<09:36,  0.91it/s, v_num=3, train/loss_step=0.735]\n",
      "Epoch 0:   4%|▍         | 22/547 [00:23<09:15,  0.95it/s, v_num=3, train/loss_step=0.735]\n",
      "Epoch 0:   4%|▍         | 22/547 [00:24<09:35,  0.91it/s, v_num=3, train/loss_step=0.837]\n",
      "Epoch 0:   4%|▍         | 23/547 [00:24<09:15,  0.94it/s, v_num=3, train/loss_step=0.837]\n",
      "Epoch 0:   4%|▍         | 23/547 [00:25<09:34,  0.91it/s, v_num=3, train/loss_step=0.822]\n",
      "Epoch 0:   4%|▍         | 24/547 [00:25<09:14,  0.94it/s, v_num=3, train/loss_step=0.822]\n",
      "Epoch 0:   4%|▍         | 24/547 [00:26<09:33,  0.91it/s, v_num=3, train/loss_step=0.842]\n",
      "Epoch 0:   5%|▍         | 25/547 [00:26<09:14,  0.94it/s, v_num=3, train/loss_step=0.842]\n",
      "Epoch 0:   5%|▍         | 25/547 [00:27<09:32,  0.91it/s, v_num=3, train/loss_step=0.898]\n",
      "Epoch 0:   5%|▍         | 26/547 [00:27<09:13,  0.94it/s, v_num=3, train/loss_step=0.898]\n",
      "Epoch 0:   5%|▍         | 26/547 [00:28<09:30,  0.91it/s, v_num=3, train/loss_step=0.762]\n",
      "Epoch 0:   5%|▍         | 27/547 [00:28<09:13,  0.94it/s, v_num=3, train/loss_step=0.762]\n",
      "Epoch 0:   5%|▍         | 27/547 [00:29<09:29,  0.91it/s, v_num=3, train/loss_step=0.635]\n",
      "Epoch 0:   5%|▌         | 28/547 [00:29<09:12,  0.94it/s, v_num=3, train/loss_step=0.635]\n",
      "Epoch 0:   5%|▌         | 28/547 [00:30<09:28,  0.91it/s, v_num=3, train/loss_step=0.678]\n",
      "Epoch 0:   5%|▌         | 29/547 [00:30<09:12,  0.94it/s, v_num=3, train/loss_step=0.678]\n",
      "Epoch 0:   5%|▌         | 29/547 [00:31<09:27,  0.91it/s, v_num=3, train/loss_step=0.859]\n",
      "Epoch 0:   5%|▌         | 30/547 [00:32<09:11,  0.94it/s, v_num=3, train/loss_step=0.859]\n",
      "Epoch 0:   5%|▌         | 30/547 [00:32<09:26,  0.91it/s, v_num=3, train/loss_step=0.710]\n",
      "Epoch 0:   6%|▌         | 31/547 [00:33<09:10,  0.94it/s, v_num=3, train/loss_step=0.710]\n",
      "Epoch 0:   6%|▌         | 31/547 [00:33<09:25,  0.91it/s, v_num=3, train/loss_step=0.806]\n",
      "Epoch 0:   6%|▌         | 32/547 [00:34<09:10,  0.94it/s, v_num=3, train/loss_step=0.806]\n",
      "Epoch 0:   6%|▌         | 32/547 [00:35<09:23,  0.91it/s, v_num=3, train/loss_step=0.670]\n",
      "Epoch 0:   6%|▌         | 33/547 [00:35<09:09,  0.94it/s, v_num=3, train/loss_step=0.670]\n",
      "Epoch 0:   6%|▌         | 33/547 [00:36<09:22,  0.91it/s, v_num=3, train/loss_step=0.857]\n",
      "Epoch 0:   6%|▌         | 34/547 [00:36<09:08,  0.93it/s, v_num=3, train/loss_step=0.857]\n",
      "Epoch 0:   6%|▌         | 34/547 [00:37<09:21,  0.91it/s, v_num=3, train/loss_step=0.780]\n",
      "Epoch 0:   6%|▋         | 35/547 [00:37<09:08,  0.93it/s, v_num=3, train/loss_step=0.780]\n",
      "Epoch 0:   6%|▋         | 35/547 [00:38<09:20,  0.91it/s, v_num=3, train/loss_step=0.799]\n",
      "Epoch 0:   7%|▋         | 36/547 [00:38<09:07,  0.93it/s, v_num=3, train/loss_step=0.799]\n",
      "Epoch 0:   7%|▋         | 36/547 [00:39<09:19,  0.91it/s, v_num=3, train/loss_step=0.745]\n",
      "Epoch 0:   7%|▋         | 37/547 [00:39<09:06,  0.93it/s, v_num=3, train/loss_step=0.745]\n",
      "Epoch 0:   7%|▋         | 37/547 [00:40<09:18,  0.91it/s, v_num=3, train/loss_step=0.662]\n",
      "Epoch 0:   7%|▋         | 38/547 [00:40<09:05,  0.93it/s, v_num=3, train/loss_step=0.662]\n",
      "Epoch 0:   7%|▋         | 38/547 [00:41<09:17,  0.91it/s, v_num=3, train/loss_step=0.747]\n",
      "Epoch 0:   7%|▋         | 39/547 [00:41<09:04,  0.93it/s, v_num=3, train/loss_step=0.747]\n",
      "Epoch 0:   7%|▋         | 39/547 [00:42<09:16,  0.91it/s, v_num=3, train/loss_step=0.711]\n",
      "Epoch 0:   7%|▋         | 40/547 [00:42<09:04,  0.93it/s, v_num=3, train/loss_step=0.711]\n",
      "Epoch 0:   7%|▋         | 40/547 [00:43<09:14,  0.91it/s, v_num=3, train/loss_step=0.652]\n",
      "Epoch 0:   7%|▋         | 41/547 [00:44<09:03,  0.93it/s, v_num=3, train/loss_step=0.652]\n",
      "Epoch 0:   7%|▋         | 41/547 [00:44<09:13,  0.91it/s, v_num=3, train/loss_step=0.670]\n",
      "Epoch 0:   8%|▊         | 42/547 [00:45<09:02,  0.93it/s, v_num=3, train/loss_step=0.670]\n",
      "Epoch 0:   8%|▊         | 42/547 [00:45<09:12,  0.91it/s, v_num=3, train/loss_step=0.725]\n",
      "Epoch 0:   8%|▊         | 43/547 [00:46<09:01,  0.93it/s, v_num=3, train/loss_step=0.725]\n",
      "Epoch 0:   8%|▊         | 43/547 [00:47<09:11,  0.91it/s, v_num=3, train/loss_step=0.766]\n",
      "Epoch 0:   8%|▊         | 44/547 [00:47<09:00,  0.93it/s, v_num=3, train/loss_step=0.766]\n",
      "Epoch 0:   8%|▊         | 44/547 [00:48<09:10,  0.91it/s, v_num=3, train/loss_step=0.692]\n",
      "Epoch 0:   8%|▊         | 45/547 [00:48<08:59,  0.93it/s, v_num=3, train/loss_step=0.692]\n",
      "Epoch 0:   8%|▊         | 45/547 [00:49<09:09,  0.91it/s, v_num=3, train/loss_step=0.694]\n",
      "Epoch 0:   8%|▊         | 46/547 [00:49<08:58,  0.93it/s, v_num=3, train/loss_step=0.694]\n",
      "Epoch 0:   8%|▊         | 46/547 [00:50<09:07,  0.91it/s, v_num=3, train/loss_step=0.626]\n",
      "Epoch 0:   9%|▊         | 47/547 [00:50<08:57,  0.93it/s, v_num=3, train/loss_step=0.626]\n",
      "Epoch 0:   9%|▊         | 47/547 [00:51<09:06,  0.91it/s, v_num=3, train/loss_step=0.557]\n",
      "Epoch 0:   9%|▉         | 48/547 [00:51<08:56,  0.93it/s, v_num=3, train/loss_step=0.557]\n",
      "Epoch 0:   9%|▉         | 48/547 [00:52<09:05,  0.91it/s, v_num=3, train/loss_step=0.762]\n",
      "Epoch 0:   9%|▉         | 49/547 [00:52<08:56,  0.93it/s, v_num=3, train/loss_step=0.762]\n",
      "Epoch 0:   9%|▉         | 49/547 [00:53<09:04,  0.91it/s, v_num=3, train/loss_step=0.596]\n",
      "Epoch 0:   9%|▉         | 50/547 [00:53<08:55,  0.93it/s, v_num=3, train/loss_step=0.596]\n",
      "Epoch 0:   9%|▉         | 50/547 [00:54<09:03,  0.91it/s, v_num=3, train/loss_step=0.639]\n",
      "Epoch 0:   9%|▉         | 51/547 [00:54<08:54,  0.93it/s, v_num=3, train/loss_step=0.639]\n",
      "Epoch 0:   9%|▉         | 51/547 [00:55<09:02,  0.91it/s, v_num=3, train/loss_step=0.720]\n",
      "Epoch 0:  10%|▉         | 52/547 [00:56<08:53,  0.93it/s, v_num=3, train/loss_step=0.720]\n",
      "Epoch 0:  10%|▉         | 52/547 [00:56<09:01,  0.91it/s, v_num=3, train/loss_step=0.700]\n",
      "Epoch 0:  10%|▉         | 53/547 [00:57<08:52,  0.93it/s, v_num=3, train/loss_step=0.700]\n",
      "Epoch 0:  10%|▉         | 53/547 [00:57<09:00,  0.91it/s, v_num=3, train/loss_step=0.694]\n",
      "Epoch 0:  10%|▉         | 54/547 [00:58<08:51,  0.93it/s, v_num=3, train/loss_step=0.694]\n",
      "Epoch 0:  10%|▉         | 54/547 [00:59<08:59,  0.91it/s, v_num=3, train/loss_step=0.718]\n",
      "Epoch 0:  10%|█         | 55/547 [00:59<08:50,  0.93it/s, v_num=3, train/loss_step=0.718]\n",
      "Epoch 0:  10%|█         | 55/547 [01:00<08:58,  0.91it/s, v_num=3, train/loss_step=0.706]\n",
      "Epoch 0:  10%|█         | 56/547 [01:00<08:49,  0.93it/s, v_num=3, train/loss_step=0.706]\n",
      "Epoch 0:  10%|█         | 56/547 [01:01<08:57,  0.91it/s, v_num=3, train/loss_step=0.753]\n",
      "Epoch 0:  10%|█         | 57/547 [01:01<08:48,  0.93it/s, v_num=3, train/loss_step=0.753]\n",
      "Epoch 0:  10%|█         | 57/547 [01:02<08:55,  0.91it/s, v_num=3, train/loss_step=0.705]\n",
      "Epoch 0:  11%|█         | 58/547 [01:02<08:47,  0.93it/s, v_num=3, train/loss_step=0.705]\n",
      "Epoch 0:  11%|█         | 58/547 [01:03<08:54,  0.91it/s, v_num=3, train/loss_step=0.696]\n",
      "Epoch 0:  11%|█         | 59/547 [01:03<08:46,  0.93it/s, v_num=3, train/loss_step=0.696]\n",
      "Epoch 0:  11%|█         | 59/547 [01:04<08:53,  0.91it/s, v_num=3, train/loss_step=0.665]\n",
      "Epoch 0:  11%|█         | 60/547 [01:04<08:45,  0.93it/s, v_num=3, train/loss_step=0.665]\n",
      "Epoch 0:  11%|█         | 60/547 [01:05<08:52,  0.91it/s, v_num=3, train/loss_step=0.691]\n",
      "Epoch 0:  11%|█         | 61/547 [01:05<08:44,  0.93it/s, v_num=3, train/loss_step=0.691]\n",
      "Epoch 0:  11%|█         | 61/547 [01:06<08:51,  0.91it/s, v_num=3, train/loss_step=0.651]\n",
      "Epoch 0:  11%|█▏        | 62/547 [01:06<08:43,  0.93it/s, v_num=3, train/loss_step=0.651]\n",
      "Epoch 0:  11%|█▏        | 62/547 [01:07<08:50,  0.91it/s, v_num=3, train/loss_step=0.662]\n",
      "Epoch 0:  12%|█▏        | 63/547 [01:08<08:42,  0.93it/s, v_num=3, train/loss_step=0.662]\n",
      "Epoch 0:  12%|█▏        | 63/547 [01:08<08:49,  0.91it/s, v_num=3, train/loss_step=0.676]\n",
      "Epoch 0:  12%|█▏        | 64/547 [01:09<08:41,  0.93it/s, v_num=3, train/loss_step=0.676]\n",
      "Epoch 0:  12%|█▏        | 64/547 [01:09<08:48,  0.91it/s, v_num=3, train/loss_step=0.629]\n",
      "Epoch 0:  12%|█▏        | 65/547 [01:10<08:40,  0.93it/s, v_num=3, train/loss_step=0.629]\n",
      "Epoch 0:  12%|█▏        | 65/547 [01:11<08:47,  0.91it/s, v_num=3, train/loss_step=0.655]\n",
      "Epoch 0:  12%|█▏        | 66/547 [01:11<08:39,  0.93it/s, v_num=3, train/loss_step=0.655]\n",
      "Epoch 0:  12%|█▏        | 66/547 [01:12<08:46,  0.91it/s, v_num=3, train/loss_step=0.673]\n",
      "Epoch 0:  12%|█▏        | 67/547 [01:12<08:38,  0.93it/s, v_num=3, train/loss_step=0.673]\n",
      "Epoch 0:  12%|█▏        | 67/547 [01:13<08:44,  0.91it/s, v_num=3, train/loss_step=0.678]\n",
      "Epoch 0:  12%|█▏        | 68/547 [01:13<08:37,  0.93it/s, v_num=3, train/loss_step=0.678]\n",
      "Epoch 0:  12%|█▏        | 68/547 [01:14<08:43,  0.91it/s, v_num=3, train/loss_step=0.653]\n",
      "Epoch 0:  13%|█▎        | 69/547 [01:14<08:36,  0.92it/s, v_num=3, train/loss_step=0.653]\n",
      "Epoch 0:  13%|█▎        | 69/547 [01:15<08:42,  0.91it/s, v_num=3, train/loss_step=0.643]\n",
      "Epoch 0:  13%|█▎        | 70/547 [01:15<08:35,  0.92it/s, v_num=3, train/loss_step=0.643]\n",
      "Epoch 0:  13%|█▎        | 70/547 [01:16<08:41,  0.91it/s, v_num=3, train/loss_step=0.641]\n",
      "Epoch 0:  13%|█▎        | 71/547 [01:16<08:34,  0.92it/s, v_num=3, train/loss_step=0.641]\n",
      "Epoch 0:  13%|█▎        | 71/547 [01:17<08:40,  0.91it/s, v_num=3, train/loss_step=0.725]\n",
      "Epoch 0:  13%|█▎        | 72/547 [01:17<08:33,  0.92it/s, v_num=3, train/loss_step=0.725]\n",
      "Epoch 0:  13%|█▎        | 72/547 [01:18<08:39,  0.91it/s, v_num=3, train/loss_step=0.608]\n",
      "Epoch 0:  13%|█▎        | 73/547 [01:18<08:32,  0.92it/s, v_num=3, train/loss_step=0.608]\n",
      "Epoch 0:  13%|█▎        | 73/547 [01:19<08:38,  0.91it/s, v_num=3, train/loss_step=0.656]\n",
      "Epoch 0:  14%|█▎        | 74/547 [01:20<08:31,  0.92it/s, v_num=3, train/loss_step=0.656]\n",
      "Epoch 0:  14%|█▎        | 74/547 [01:20<08:37,  0.91it/s, v_num=3, train/loss_step=0.689]\n",
      "Epoch 0:  14%|█▎        | 75/547 [01:21<08:30,  0.92it/s, v_num=3, train/loss_step=0.689]\n",
      "Epoch 0:  14%|█▎        | 75/547 [01:22<08:36,  0.91it/s, v_num=3, train/loss_step=0.713]\n",
      "Epoch 0:  14%|█▍        | 76/547 [01:22<08:29,  0.92it/s, v_num=3, train/loss_step=0.713]\n",
      "Epoch 0:  14%|█▍        | 76/547 [01:23<08:35,  0.91it/s, v_num=3, train/loss_step=0.658]\n",
      "Epoch 0:  14%|█▍        | 77/547 [01:23<08:28,  0.92it/s, v_num=3, train/loss_step=0.658]\n",
      "Epoch 0:  14%|█▍        | 77/547 [01:24<08:33,  0.91it/s, v_num=3, train/loss_step=0.648]\n",
      "Epoch 0:  14%|█▍        | 78/547 [01:24<08:27,  0.92it/s, v_num=3, train/loss_step=0.648]\n",
      "Epoch 0:  14%|█▍        | 78/547 [01:25<08:32,  0.91it/s, v_num=3, train/loss_step=0.618]\n",
      "Epoch 0:  14%|█▍        | 79/547 [01:25<08:26,  0.92it/s, v_num=3, train/loss_step=0.618]\n",
      "Epoch 0:  14%|█▍        | 79/547 [01:26<08:31,  0.91it/s, v_num=3, train/loss_step=0.621]\n",
      "Epoch 0:  15%|█▍        | 80/547 [01:26<08:25,  0.92it/s, v_num=3, train/loss_step=0.621]\n",
      "Epoch 0:  15%|█▍        | 80/547 [01:27<08:30,  0.91it/s, v_num=3, train/loss_step=0.646]\n",
      "Epoch 0:  15%|█▍        | 81/547 [01:27<08:24,  0.92it/s, v_num=3, train/loss_step=0.646]\n",
      "Epoch 0:  15%|█▍        | 81/547 [01:28<08:29,  0.91it/s, v_num=3, train/loss_step=0.724]\n",
      "Epoch 0:  15%|█▍        | 82/547 [01:28<08:23,  0.92it/s, v_num=3, train/loss_step=0.724]\n",
      "Epoch 0:  15%|█▍        | 82/547 [01:29<08:28,  0.91it/s, v_num=3, train/loss_step=0.708]\n",
      "Epoch 0:  15%|█▌        | 83/547 [01:29<08:22,  0.92it/s, v_num=3, train/loss_step=0.708]\n",
      "Epoch 0:  15%|█▌        | 83/547 [01:30<08:27,  0.91it/s, v_num=3, train/loss_step=0.635]\n",
      "Epoch 0:  15%|█▌        | 84/547 [01:31<08:21,  0.92it/s, v_num=3, train/loss_step=0.635]\n",
      "Epoch 0:  15%|█▌        | 84/547 [01:31<08:26,  0.91it/s, v_num=3, train/loss_step=0.668]\n",
      "Epoch 0:  16%|█▌        | 85/547 [01:32<08:20,  0.92it/s, v_num=3, train/loss_step=0.668]\n",
      "Epoch 0:  16%|█▌        | 85/547 [01:32<08:25,  0.91it/s, v_num=3, train/loss_step=0.625]\n",
      "Epoch 0:  16%|█▌        | 86/547 [01:33<08:19,  0.92it/s, v_num=3, train/loss_step=0.625]\n",
      "Epoch 0:  16%|█▌        | 86/547 [01:34<08:24,  0.91it/s, v_num=3, train/loss_step=0.631]\n",
      "Epoch 0:  16%|█▌        | 87/547 [01:34<08:18,  0.92it/s, v_num=3, train/loss_step=0.631]\n",
      "Epoch 0:  16%|█▌        | 87/547 [01:35<08:22,  0.91it/s, v_num=3, train/loss_step=0.717]\n",
      "Epoch 0:  16%|█▌        | 88/547 [01:35<08:17,  0.92it/s, v_num=3, train/loss_step=0.717]\n",
      "Epoch 0:  16%|█▌        | 88/547 [01:36<08:21,  0.91it/s, v_num=3, train/loss_step=0.647]\n",
      "Epoch 0:  16%|█▋        | 89/547 [01:36<08:16,  0.92it/s, v_num=3, train/loss_step=0.647]\n",
      "Epoch 0:  16%|█▋        | 89/547 [01:37<08:20,  0.91it/s, v_num=3, train/loss_step=0.683]\n",
      "Epoch 0:  16%|█▋        | 90/547 [01:37<08:15,  0.92it/s, v_num=3, train/loss_step=0.683]\n",
      "Epoch 0:  16%|█▋        | 90/547 [01:38<08:19,  0.91it/s, v_num=3, train/loss_step=0.725]\n",
      "Epoch 0:  17%|█▋        | 91/547 [01:38<08:14,  0.92it/s, v_num=3, train/loss_step=0.725]\n",
      "Epoch 0:  17%|█▋        | 91/547 [01:39<08:18,  0.91it/s, v_num=3, train/loss_step=0.675]\n",
      "Epoch 0:  17%|█▋        | 92/547 [01:39<08:13,  0.92it/s, v_num=3, train/loss_step=0.675]\n",
      "Epoch 0:  17%|█▋        | 92/547 [01:40<08:17,  0.91it/s, v_num=3, train/loss_step=0.590]\n",
      "Epoch 0:  17%|█▋        | 93/547 [01:40<08:12,  0.92it/s, v_num=3, train/loss_step=0.590]\n",
      "Epoch 0:  17%|█▋        | 93/547 [01:41<08:16,  0.91it/s, v_num=3, train/loss_step=0.638]\n",
      "Epoch 0:  17%|█▋        | 94/547 [01:41<08:11,  0.92it/s, v_num=3, train/loss_step=0.638]\n",
      "Epoch 0:  17%|█▋        | 94/547 [01:42<08:15,  0.91it/s, v_num=3, train/loss_step=0.676]\n",
      "Epoch 0:  17%|█▋        | 95/547 [01:43<08:10,  0.92it/s, v_num=3, train/loss_step=0.676]\n",
      "Epoch 0:  17%|█▋        | 95/547 [01:43<08:14,  0.91it/s, v_num=3, train/loss_step=0.655]\n",
      "Epoch 0:  18%|█▊        | 96/547 [01:44<08:09,  0.92it/s, v_num=3, train/loss_step=0.655]\n",
      "Epoch 0:  18%|█▊        | 96/547 [01:44<08:13,  0.91it/s, v_num=3, train/loss_step=0.641]\n",
      "Epoch 0:  18%|█▊        | 97/547 [01:45<08:08,  0.92it/s, v_num=3, train/loss_step=0.641]\n",
      "Epoch 0:  18%|█▊        | 97/547 [01:46<08:12,  0.91it/s, v_num=3, train/loss_step=0.614]\n",
      "Epoch 0:  18%|█▊        | 98/547 [01:46<08:07,  0.92it/s, v_num=3, train/loss_step=0.614]\n",
      "Epoch 0:  18%|█▊        | 98/547 [01:47<08:10,  0.91it/s, v_num=3, train/loss_step=0.624]\n",
      "Epoch 0:  18%|█▊        | 99/547 [01:47<08:06,  0.92it/s, v_num=3, train/loss_step=0.624]\n",
      "Epoch 0:  18%|█▊        | 99/547 [01:48<08:09,  0.91it/s, v_num=3, train/loss_step=0.604]\n",
      "Epoch 0:  18%|█▊        | 100/547 [01:48<08:04,  0.92it/s, v_num=3, train/loss_step=0.604]\n",
      "Epoch 0:  18%|█▊        | 100/547 [01:49<08:08,  0.91it/s, v_num=3, train/loss_step=0.665]\n",
      "Epoch 0:  18%|█▊        | 101/547 [01:49<08:03,  0.92it/s, v_num=3, train/loss_step=0.665]\n",
      "Epoch 0:  18%|█▊        | 101/547 [01:50<08:07,  0.91it/s, v_num=3, train/loss_step=0.590]\n",
      "Epoch 0:  19%|█▊        | 102/547 [01:50<08:02,  0.92it/s, v_num=3, train/loss_step=0.590]\n",
      "Epoch 0:  19%|█▊        | 102/547 [01:51<08:06,  0.91it/s, v_num=3, train/loss_step=0.677]\n",
      "Epoch 0:  19%|█▉        | 103/547 [01:51<08:01,  0.92it/s, v_num=3, train/loss_step=0.677]\n",
      "Epoch 0:  19%|█▉        | 103/547 [01:52<08:05,  0.91it/s, v_num=3, train/loss_step=0.647]\n",
      "Epoch 0:  19%|█▉        | 104/547 [01:52<08:00,  0.92it/s, v_num=3, train/loss_step=0.647]\n",
      "Epoch 0:  19%|█▉        | 104/547 [01:53<08:04,  0.91it/s, v_num=3, train/loss_step=0.604]\n",
      "Epoch 0:  19%|█▉        | 105/547 [01:53<07:59,  0.92it/s, v_num=3, train/loss_step=0.604]\n",
      "Epoch 0:  19%|█▉        | 105/547 [01:54<08:03,  0.91it/s, v_num=3, train/loss_step=0.645]\n",
      "Epoch 0:  19%|█▉        | 106/547 [01:55<07:58,  0.92it/s, v_num=3, train/loss_step=0.645]\n",
      "Epoch 0:  19%|█▉        | 106/547 [01:55<08:02,  0.91it/s, v_num=3, train/loss_step=0.613]\n",
      "Epoch 0:  20%|█▉        | 107/547 [01:56<07:57,  0.92it/s, v_num=3, train/loss_step=0.613]\n",
      "Epoch 0:  20%|█▉        | 107/547 [01:56<08:01,  0.91it/s, v_num=3, train/loss_step=0.672]\n",
      "Epoch 0:  20%|█▉        | 108/547 [01:57<07:56,  0.92it/s, v_num=3, train/loss_step=0.672]\n",
      "Epoch 0:  20%|█▉        | 108/547 [01:58<07:59,  0.91it/s, v_num=3, train/loss_step=0.636]\n",
      "Epoch 0:  20%|█▉        | 109/547 [01:58<07:55,  0.92it/s, v_num=3, train/loss_step=0.636]\n",
      "Epoch 0:  20%|█▉        | 109/547 [01:59<07:58,  0.91it/s, v_num=3, train/loss_step=0.653]\n",
      "Epoch 0:  20%|██        | 110/547 [01:59<07:54,  0.92it/s, v_num=3, train/loss_step=0.653]\n",
      "Epoch 0:  20%|██        | 110/547 [02:00<07:57,  0.91it/s, v_num=3, train/loss_step=0.571]\n",
      "Epoch 0:  20%|██        | 111/547 [02:00<07:53,  0.92it/s, v_num=3, train/loss_step=0.571]\n",
      "Epoch 0:  20%|██        | 111/547 [02:01<07:56,  0.91it/s, v_num=3, train/loss_step=0.613]\n",
      "Epoch 0:  20%|██        | 112/547 [02:01<07:52,  0.92it/s, v_num=3, train/loss_step=0.613]\n",
      "Epoch 0:  20%|██        | 112/547 [02:02<07:55,  0.91it/s, v_num=3, train/loss_step=0.642]\n",
      "Epoch 0:  21%|██        | 113/547 [02:02<07:51,  0.92it/s, v_num=3, train/loss_step=0.642]\n",
      "Epoch 0:  21%|██        | 113/547 [02:03<07:54,  0.91it/s, v_num=3, train/loss_step=0.639]\n",
      "Epoch 0:  21%|██        | 114/547 [02:03<07:50,  0.92it/s, v_num=3, train/loss_step=0.639]\n",
      "Epoch 0:  21%|██        | 114/547 [02:04<07:53,  0.91it/s, v_num=3, train/loss_step=0.553]\n",
      "Epoch 0:  21%|██        | 115/547 [02:04<07:49,  0.92it/s, v_num=3, train/loss_step=0.553]\n",
      "Epoch 0:  21%|██        | 115/547 [02:05<07:52,  0.91it/s, v_num=3, train/loss_step=0.599]\n",
      "Epoch 0:  21%|██        | 116/547 [02:05<07:47,  0.92it/s, v_num=3, train/loss_step=0.599]\n",
      "Epoch 0:  21%|██        | 116/547 [02:06<07:51,  0.91it/s, v_num=3, train/loss_step=0.663]\n",
      "Epoch 0:  21%|██▏       | 117/547 [02:07<07:46,  0.92it/s, v_num=3, train/loss_step=0.663]\n",
      "Epoch 0:  21%|██▏       | 117/547 [02:07<07:50,  0.91it/s, v_num=3, train/loss_step=0.583]\n",
      "Epoch 0:  22%|██▏       | 118/547 [02:08<07:45,  0.92it/s, v_num=3, train/loss_step=0.583]\n",
      "Epoch 0:  22%|██▏       | 118/547 [02:08<07:48,  0.91it/s, v_num=3, train/loss_step=0.671]\n",
      "Epoch 0:  22%|██▏       | 119/547 [02:09<07:44,  0.92it/s, v_num=3, train/loss_step=0.671]\n",
      "Epoch 0:  22%|██▏       | 119/547 [02:10<07:47,  0.91it/s, v_num=3, train/loss_step=0.633]\n",
      "Epoch 0:  22%|██▏       | 120/547 [02:10<07:43,  0.92it/s, v_num=3, train/loss_step=0.633]\n",
      "Epoch 0:  22%|██▏       | 120/547 [02:11<07:46,  0.91it/s, v_num=3, train/loss_step=0.545]\n",
      "Epoch 0:  22%|██▏       | 121/547 [02:11<07:42,  0.92it/s, v_num=3, train/loss_step=0.545]\n",
      "Epoch 0:  22%|██▏       | 121/547 [02:12<07:45,  0.91it/s, v_num=3, train/loss_step=0.596]\n",
      "Epoch 0:  22%|██▏       | 122/547 [02:12<07:41,  0.92it/s, v_num=3, train/loss_step=0.596]\n",
      "Epoch 0:  22%|██▏       | 122/547 [02:13<07:44,  0.91it/s, v_num=3, train/loss_step=0.600]\n",
      "Epoch 0:  22%|██▏       | 123/547 [02:13<07:40,  0.92it/s, v_num=3, train/loss_step=0.600]\n",
      "Epoch 0:  22%|██▏       | 123/547 [02:14<07:43,  0.91it/s, v_num=3, train/loss_step=0.576]\n",
      "Epoch 0:  23%|██▎       | 124/547 [02:14<07:39,  0.92it/s, v_num=3, train/loss_step=0.576]\n",
      "Epoch 0:  23%|██▎       | 124/547 [02:15<07:42,  0.91it/s, v_num=3, train/loss_step=0.597]\n",
      "Epoch 0:  23%|██▎       | 125/547 [02:15<07:38,  0.92it/s, v_num=3, train/loss_step=0.597]\n",
      "Epoch 0:  23%|██▎       | 125/547 [02:16<07:41,  0.91it/s, v_num=3, train/loss_step=0.535]\n",
      "Epoch 0:  23%|██▎       | 126/547 [02:16<07:37,  0.92it/s, v_num=3, train/loss_step=0.535]\n",
      "Epoch 0:  23%|██▎       | 126/547 [02:17<07:40,  0.91it/s, v_num=3, train/loss_step=0.583]\n",
      "Epoch 0:  23%|██▎       | 127/547 [02:17<07:36,  0.92it/s, v_num=3, train/loss_step=0.583]\n",
      "Epoch 0:  23%|██▎       | 127/547 [02:18<07:39,  0.91it/s, v_num=3, train/loss_step=0.513]\n",
      "Epoch 0:  23%|██▎       | 128/547 [02:19<07:35,  0.92it/s, v_num=3, train/loss_step=0.513]\n",
      "Epoch 0:  23%|██▎       | 128/547 [02:19<07:38,  0.91it/s, v_num=3, train/loss_step=0.587]\n",
      "Epoch 0:  24%|██▎       | 129/547 [02:20<07:34,  0.92it/s, v_num=3, train/loss_step=0.587]\n",
      "Epoch 0:  24%|██▎       | 129/547 [02:21<07:37,  0.91it/s, v_num=3, train/loss_step=0.636]\n",
      "Epoch 0:  24%|██▍       | 130/547 [02:21<07:33,  0.92it/s, v_num=3, train/loss_step=0.636]\n",
      "Epoch 0:  24%|██▍       | 130/547 [02:22<07:35,  0.91it/s, v_num=3, train/loss_step=0.521]\n",
      "Epoch 0:  24%|██▍       | 131/547 [02:22<07:32,  0.92it/s, v_num=3, train/loss_step=0.521]\n",
      "Epoch 0:  24%|██▍       | 131/547 [02:23<07:34,  0.91it/s, v_num=3, train/loss_step=0.591]\n",
      "Epoch 0:  24%|██▍       | 132/547 [02:23<07:31,  0.92it/s, v_num=3, train/loss_step=0.591]\n",
      "Epoch 0:  24%|██▍       | 132/547 [02:24<07:33,  0.91it/s, v_num=3, train/loss_step=0.667]\n",
      "Epoch 0:  24%|██▍       | 133/547 [02:24<07:30,  0.92it/s, v_num=3, train/loss_step=0.667]\n",
      "Epoch 0:  24%|██▍       | 133/547 [02:25<07:32,  0.91it/s, v_num=3, train/loss_step=0.599]\n",
      "Epoch 0:  24%|██▍       | 134/547 [02:25<07:28,  0.92it/s, v_num=3, train/loss_step=0.599]\n",
      "Epoch 0:  24%|██▍       | 134/547 [02:26<07:31,  0.91it/s, v_num=3, train/loss_step=0.564]\n",
      "Epoch 0:  25%|██▍       | 135/547 [02:26<07:27,  0.92it/s, v_num=3, train/loss_step=0.564]\n",
      "Epoch 0:  25%|██▍       | 135/547 [02:27<07:30,  0.91it/s, v_num=3, train/loss_step=0.558]\n",
      "Epoch 0:  25%|██▍       | 136/547 [02:27<07:26,  0.92it/s, v_num=3, train/loss_step=0.558]\n",
      "Epoch 0:  25%|██▍       | 136/547 [02:28<07:29,  0.91it/s, v_num=3, train/loss_step=0.512]\n",
      "Epoch 0:  25%|██▌       | 137/547 [02:28<07:25,  0.92it/s, v_num=3, train/loss_step=0.512]\n",
      "Epoch 0:  25%|██▌       | 137/547 [02:29<07:28,  0.91it/s, v_num=3, train/loss_step=0.728]\n",
      "Epoch 0:  25%|██▌       | 138/547 [02:30<07:24,  0.92it/s, v_num=3, train/loss_step=0.728]\n",
      "Epoch 0:  25%|██▌       | 138/547 [02:30<07:27,  0.91it/s, v_num=3, train/loss_step=0.526]\n",
      "Epoch 0:  25%|██▌       | 139/547 [02:31<07:23,  0.92it/s, v_num=3, train/loss_step=0.526]\n",
      "Epoch 0:  25%|██▌       | 139/547 [02:32<07:26,  0.91it/s, v_num=3, train/loss_step=0.603]\n",
      "Epoch 0:  26%|██▌       | 140/547 [02:32<07:22,  0.92it/s, v_num=3, train/loss_step=0.603]\n",
      "Epoch 0:  26%|██▌       | 140/547 [02:33<07:25,  0.91it/s, v_num=3, train/loss_step=0.589]\n",
      "Epoch 0:  26%|██▌       | 141/547 [02:33<07:21,  0.92it/s, v_num=3, train/loss_step=0.589]\n",
      "Epoch 0:  26%|██▌       | 141/547 [02:34<07:24,  0.91it/s, v_num=3, train/loss_step=0.545]\n",
      "Epoch 0:  26%|██▌       | 142/547 [02:34<07:20,  0.92it/s, v_num=3, train/loss_step=0.545]\n",
      "Epoch 0:  26%|██▌       | 142/547 [02:35<07:22,  0.91it/s, v_num=3, train/loss_step=0.542]\n",
      "Epoch 0:  26%|██▌       | 143/547 [02:35<07:19,  0.92it/s, v_num=3, train/loss_step=0.542]\n",
      "Epoch 0:  26%|██▌       | 143/547 [02:36<07:21,  0.91it/s, v_num=3, train/loss_step=0.591]\n",
      "Epoch 0:  26%|██▋       | 144/547 [02:36<07:18,  0.92it/s, v_num=3, train/loss_step=0.591]\n",
      "Epoch 0:  26%|██▋       | 144/547 [02:37<07:20,  0.91it/s, v_num=3, train/loss_step=0.485]\n",
      "Epoch 0:  27%|██▋       | 145/547 [02:37<07:17,  0.92it/s, v_num=3, train/loss_step=0.485]\n",
      "Epoch 0:  27%|██▋       | 145/547 [02:38<07:19,  0.91it/s, v_num=3, train/loss_step=0.535]\n",
      "Epoch 0:  27%|██▋       | 146/547 [02:38<07:16,  0.92it/s, v_num=3, train/loss_step=0.535]\n",
      "Epoch 0:  27%|██▋       | 146/547 [02:39<07:18,  0.91it/s, v_num=3, train/loss_step=0.629]\n",
      "Epoch 0:  27%|██▋       | 147/547 [02:39<07:15,  0.92it/s, v_num=3, train/loss_step=0.629]\n",
      "Epoch 0:  27%|██▋       | 147/547 [02:40<07:17,  0.91it/s, v_num=3, train/loss_step=0.511]\n",
      "Epoch 0:  27%|██▋       | 148/547 [02:41<07:14,  0.92it/s, v_num=3, train/loss_step=0.511]\n",
      "Epoch 0:  27%|██▋       | 148/547 [02:41<07:16,  0.91it/s, v_num=3, train/loss_step=0.571]\n",
      "Epoch 0:  27%|██▋       | 149/547 [02:42<07:13,  0.92it/s, v_num=3, train/loss_step=0.571]\n",
      "Epoch 0:  27%|██▋       | 149/547 [02:42<07:15,  0.91it/s, v_num=3, train/loss_step=0.538]\n",
      "Epoch 0:  27%|██▋       | 150/547 [02:43<07:11,  0.92it/s, v_num=3, train/loss_step=0.538]\n",
      "Epoch 0:  27%|██▋       | 150/547 [02:44<07:14,  0.91it/s, v_num=3, train/loss_step=0.557]\n",
      "Epoch 0:  28%|██▊       | 151/547 [02:44<07:10,  0.92it/s, v_num=3, train/loss_step=0.557]\n",
      "Epoch 0:  28%|██▊       | 151/547 [02:45<07:13,  0.91it/s, v_num=3, train/loss_step=0.536]\n",
      "Epoch 0:  28%|██▊       | 152/547 [02:45<07:09,  0.92it/s, v_num=3, train/loss_step=0.536]\n",
      "Epoch 0:  28%|██▊       | 152/547 [02:46<07:12,  0.91it/s, v_num=3, train/loss_step=0.514]\n",
      "Epoch 0:  28%|██▊       | 153/547 [02:46<07:08,  0.92it/s, v_num=3, train/loss_step=0.514]\n",
      "Epoch 0:  28%|██▊       | 153/547 [02:47<07:10,  0.91it/s, v_num=3, train/loss_step=0.482]\n",
      "Epoch 0:  28%|██▊       | 154/547 [02:47<07:07,  0.92it/s, v_num=3, train/loss_step=0.482]\n",
      "Epoch 0:  28%|██▊       | 154/547 [02:48<07:09,  0.91it/s, v_num=3, train/loss_step=0.511]\n",
      "Epoch 0:  28%|██▊       | 155/547 [02:48<07:06,  0.92it/s, v_num=3, train/loss_step=0.511]\n",
      "Epoch 0:  28%|██▊       | 155/547 [02:49<07:08,  0.91it/s, v_num=3, train/loss_step=0.509]\n",
      "Epoch 0:  29%|██▊       | 156/547 [02:49<07:05,  0.92it/s, v_num=3, train/loss_step=0.509]\n",
      "Epoch 0:  29%|██▊       | 156/547 [02:50<07:07,  0.91it/s, v_num=3, train/loss_step=0.574]\n",
      "Epoch 0:  29%|██▊       | 157/547 [02:50<07:04,  0.92it/s, v_num=3, train/loss_step=0.574]\n",
      "Epoch 0:  29%|██▊       | 157/547 [02:51<07:06,  0.91it/s, v_num=3, train/loss_step=0.600]\n",
      "Epoch 0:  29%|██▉       | 158/547 [02:51<07:03,  0.92it/s, v_num=3, train/loss_step=0.600]\n",
      "Epoch 0:  29%|██▉       | 158/547 [02:52<07:05,  0.91it/s, v_num=3, train/loss_step=0.605]\n",
      "Epoch 0:  29%|██▉       | 159/547 [02:53<07:02,  0.92it/s, v_num=3, train/loss_step=0.605]\n",
      "Epoch 0:  29%|██▉       | 159/547 [02:53<07:04,  0.91it/s, v_num=3, train/loss_step=0.520]\n",
      "Epoch 0:  29%|██▉       | 160/547 [02:54<07:01,  0.92it/s, v_num=3, train/loss_step=0.520]\n",
      "Epoch 0:  29%|██▉       | 160/547 [02:55<07:03,  0.91it/s, v_num=3, train/loss_step=0.472]\n",
      "Epoch 0:  29%|██▉       | 161/547 [02:55<07:00,  0.92it/s, v_num=3, train/loss_step=0.472]\n",
      "Epoch 0:  29%|██▉       | 161/547 [02:56<07:02,  0.91it/s, v_num=3, train/loss_step=0.497]\n",
      "Epoch 0:  30%|██▉       | 162/547 [02:56<06:59,  0.92it/s, v_num=3, train/loss_step=0.497]\n",
      "Epoch 0:  30%|██▉       | 162/547 [02:57<07:01,  0.91it/s, v_num=3, train/loss_step=0.504]\n",
      "Epoch 0:  30%|██▉       | 163/547 [02:57<06:58,  0.92it/s, v_num=3, train/loss_step=0.504]\n",
      "Epoch 0:  30%|██▉       | 163/547 [02:58<07:00,  0.91it/s, v_num=3, train/loss_step=0.505]\n",
      "Epoch 0:  30%|██▉       | 164/547 [02:58<06:57,  0.92it/s, v_num=3, train/loss_step=0.505]\n",
      "Epoch 0:  30%|██▉       | 164/547 [02:59<06:59,  0.91it/s, v_num=3, train/loss_step=0.455]\n",
      "Epoch 0:  30%|███       | 165/547 [02:59<06:55,  0.92it/s, v_num=3, train/loss_step=0.455]\n",
      "Epoch 0:  30%|███       | 165/547 [03:00<06:57,  0.91it/s, v_num=3, train/loss_step=0.500]\n",
      "Epoch 0:  30%|███       | 166/547 [03:00<06:54,  0.92it/s, v_num=3, train/loss_step=0.500]\n",
      "Epoch 0:  30%|███       | 166/547 [03:01<06:56,  0.91it/s, v_num=3, train/loss_step=0.489]\n",
      "Epoch 0:  31%|███       | 167/547 [03:01<06:53,  0.92it/s, v_num=3, train/loss_step=0.489]\n",
      "Epoch 0:  31%|███       | 167/547 [03:02<06:55,  0.91it/s, v_num=3, train/loss_step=0.470]\n",
      "Epoch 0:  31%|███       | 168/547 [03:02<06:52,  0.92it/s, v_num=3, train/loss_step=0.470]\n",
      "Epoch 0:  31%|███       | 168/547 [03:03<06:54,  0.91it/s, v_num=3, train/loss_step=0.499]\n",
      "Epoch 0:  31%|███       | 169/547 [03:04<06:51,  0.92it/s, v_num=3, train/loss_step=0.499]\n",
      "Epoch 0:  31%|███       | 169/547 [03:04<06:53,  0.91it/s, v_num=3, train/loss_step=0.429]\n",
      "Epoch 0:  31%|███       | 170/547 [03:05<06:50,  0.92it/s, v_num=3, train/loss_step=0.429]\n",
      "Epoch 0:  31%|███       | 170/547 [03:05<06:52,  0.91it/s, v_num=3, train/loss_step=0.474]\n",
      "Epoch 0:  31%|███▏      | 171/547 [03:06<06:49,  0.92it/s, v_num=3, train/loss_step=0.474]\n",
      "Epoch 0:  31%|███▏      | 171/547 [03:07<06:51,  0.91it/s, v_num=3, train/loss_step=0.452]\n",
      "Epoch 0:  31%|███▏      | 172/547 [03:07<06:48,  0.92it/s, v_num=3, train/loss_step=0.452]\n",
      "Epoch 0:  31%|███▏      | 172/547 [03:08<06:50,  0.91it/s, v_num=3, train/loss_step=0.590]\n",
      "Epoch 0:  32%|███▏      | 173/547 [03:08<06:47,  0.92it/s, v_num=3, train/loss_step=0.590]\n",
      "Epoch 0:  32%|███▏      | 173/547 [03:09<06:49,  0.91it/s, v_num=3, train/loss_step=0.434]\n",
      "Epoch 0:  32%|███▏      | 174/547 [03:09<06:46,  0.92it/s, v_num=3, train/loss_step=0.434]\n",
      "Epoch 0:  32%|███▏      | 174/547 [03:10<06:48,  0.91it/s, v_num=3, train/loss_step=0.396]\n",
      "Epoch 0:  32%|███▏      | 175/547 [03:10<06:45,  0.92it/s, v_num=3, train/loss_step=0.396]\n",
      "Epoch 0:  32%|███▏      | 175/547 [03:11<06:47,  0.91it/s, v_num=3, train/loss_step=0.512]\n",
      "Epoch 0:  32%|███▏      | 176/547 [03:11<06:44,  0.92it/s, v_num=3, train/loss_step=0.512]\n",
      "Epoch 0:  32%|███▏      | 176/547 [03:12<06:45,  0.91it/s, v_num=3, train/loss_step=0.414]\n",
      "Epoch 0:  32%|███▏      | 177/547 [03:12<06:43,  0.92it/s, v_num=3, train/loss_step=0.414]\n",
      "Epoch 0:  32%|███▏      | 177/547 [03:13<06:44,  0.91it/s, v_num=3, train/loss_step=0.451]\n",
      "Epoch 0:  33%|███▎      | 178/547 [03:13<06:41,  0.92it/s, v_num=3, train/loss_step=0.451]\n",
      "Epoch 0:  33%|███▎      | 178/547 [03:14<06:43,  0.91it/s, v_num=3, train/loss_step=0.351]\n",
      "Epoch 0:  33%|███▎      | 179/547 [03:15<06:40,  0.92it/s, v_num=3, train/loss_step=0.351]\n",
      "Epoch 0:  33%|███▎      | 179/547 [03:15<06:42,  0.91it/s, v_num=3, train/loss_step=0.485]\n",
      "Epoch 0:  33%|███▎      | 180/547 [03:16<06:39,  0.92it/s, v_num=3, train/loss_step=0.485]\n",
      "Epoch 0:  33%|███▎      | 180/547 [03:16<06:41,  0.91it/s, v_num=3, train/loss_step=0.442]\n",
      "Epoch 0:  33%|███▎      | 181/547 [03:17<06:38,  0.92it/s, v_num=3, train/loss_step=0.442]\n",
      "Epoch 0:  33%|███▎      | 181/547 [03:18<06:40,  0.91it/s, v_num=3, train/loss_step=0.396]\n",
      "Epoch 0:  33%|███▎      | 182/547 [03:18<06:37,  0.92it/s, v_num=3, train/loss_step=0.396]\n",
      "Epoch 0:  33%|███▎      | 182/547 [03:19<06:39,  0.91it/s, v_num=3, train/loss_step=0.418]\n",
      "Epoch 0:  33%|███▎      | 183/547 [03:19<06:36,  0.92it/s, v_num=3, train/loss_step=0.418]\n",
      "Epoch 0:  33%|███▎      | 183/547 [03:20<06:38,  0.91it/s, v_num=3, train/loss_step=0.515]\n",
      "Epoch 0:  34%|███▎      | 184/547 [03:20<06:35,  0.92it/s, v_num=3, train/loss_step=0.515]\n",
      "Epoch 0:  34%|███▎      | 184/547 [03:21<06:37,  0.91it/s, v_num=3, train/loss_step=0.458]\n",
      "Epoch 0:  34%|███▍      | 185/547 [03:21<06:34,  0.92it/s, v_num=3, train/loss_step=0.458]\n",
      "Epoch 0:  34%|███▍      | 185/547 [03:22<06:36,  0.91it/s, v_num=3, train/loss_step=0.411]\n",
      "Epoch 0:  34%|███▍      | 186/547 [03:22<06:33,  0.92it/s, v_num=3, train/loss_step=0.411]\n",
      "Epoch 0:  34%|███▍      | 186/547 [03:23<06:35,  0.91it/s, v_num=3, train/loss_step=0.497]\n",
      "Epoch 0:  34%|███▍      | 187/547 [03:23<06:32,  0.92it/s, v_num=3, train/loss_step=0.497]\n",
      "Epoch 0:  34%|███▍      | 187/547 [03:24<06:33,  0.91it/s, v_num=3, train/loss_step=0.341]\n",
      "Epoch 0:  34%|███▍      | 188/547 [03:24<06:31,  0.92it/s, v_num=3, train/loss_step=0.341]\n",
      "Epoch 0:  34%|███▍      | 188/547 [03:25<06:32,  0.91it/s, v_num=3, train/loss_step=0.472]\n",
      "Epoch 0:  35%|███▍      | 189/547 [03:25<06:30,  0.92it/s, v_num=3, train/loss_step=0.472]\n",
      "Epoch 0:  35%|███▍      | 189/547 [03:26<06:31,  0.91it/s, v_num=3, train/loss_step=0.501]\n",
      "Epoch 0:  35%|███▍      | 190/547 [03:27<06:29,  0.92it/s, v_num=3, train/loss_step=0.501]\n",
      "Epoch 0:  35%|███▍      | 190/547 [03:27<06:30,  0.91it/s, v_num=3, train/loss_step=0.337]\n",
      "Epoch 0:  35%|███▍      | 191/547 [03:28<06:28,  0.92it/s, v_num=3, train/loss_step=0.337]\n",
      "Epoch 0:  35%|███▍      | 191/547 [03:29<06:29,  0.91it/s, v_num=3, train/loss_step=0.372]\n",
      "Epoch 0:  35%|███▌      | 192/547 [03:29<06:26,  0.92it/s, v_num=3, train/loss_step=0.372]\n",
      "Epoch 0:  35%|███▌      | 192/547 [03:30<06:28,  0.91it/s, v_num=3, train/loss_step=0.377]\n",
      "Epoch 0:  35%|███▌      | 193/547 [03:30<06:25,  0.92it/s, v_num=3, train/loss_step=0.377]\n",
      "Epoch 0:  35%|███▌      | 193/547 [03:31<06:27,  0.91it/s, v_num=3, train/loss_step=0.463]\n",
      "Epoch 0:  35%|███▌      | 194/547 [03:31<06:24,  0.92it/s, v_num=3, train/loss_step=0.463]\n",
      "Epoch 0:  35%|███▌      | 194/547 [03:32<06:26,  0.91it/s, v_num=3, train/loss_step=0.451]\n",
      "Epoch 0:  36%|███▌      | 195/547 [03:32<06:23,  0.92it/s, v_num=3, train/loss_step=0.451]\n",
      "Epoch 0:  36%|███▌      | 195/547 [03:33<06:25,  0.91it/s, v_num=3, train/loss_step=0.528]\n",
      "Epoch 0:  36%|███▌      | 196/547 [03:33<06:22,  0.92it/s, v_num=3, train/loss_step=0.528]\n",
      "Epoch 0:  36%|███▌      | 196/547 [03:34<06:24,  0.91it/s, v_num=3, train/loss_step=0.468]\n",
      "Epoch 0:  36%|███▌      | 197/547 [03:34<06:21,  0.92it/s, v_num=3, train/loss_step=0.468]\n",
      "Epoch 0:  36%|███▌      | 197/547 [03:35<06:23,  0.91it/s, v_num=3, train/loss_step=0.280]\n",
      "Epoch 0:  36%|███▌      | 198/547 [03:35<06:20,  0.92it/s, v_num=3, train/loss_step=0.280]\n",
      "Epoch 0:  36%|███▌      | 198/547 [03:36<06:21,  0.91it/s, v_num=3, train/loss_step=0.381]\n",
      "Epoch 0:  36%|███▋      | 199/547 [03:36<06:19,  0.92it/s, v_num=3, train/loss_step=0.381]\n",
      "Epoch 0:  36%|███▋      | 199/547 [03:37<06:20,  0.91it/s, v_num=3, train/loss_step=0.406]\n",
      "Epoch 0:  37%|███▋      | 200/547 [03:38<06:18,  0.92it/s, v_num=3, train/loss_step=0.406]\n",
      "Epoch 0:  37%|███▋      | 200/547 [03:38<06:19,  0.91it/s, v_num=3, train/loss_step=0.422]\n",
      "Epoch 0:  37%|███▋      | 201/547 [03:39<06:17,  0.92it/s, v_num=3, train/loss_step=0.422]\n",
      "Epoch 0:  37%|███▋      | 201/547 [03:39<06:18,  0.91it/s, v_num=3, train/loss_step=0.404]\n",
      "Epoch 0:  37%|███▋      | 202/547 [03:40<06:16,  0.92it/s, v_num=3, train/loss_step=0.404]\n",
      "Epoch 0:  37%|███▋      | 202/547 [03:41<06:17,  0.91it/s, v_num=3, train/loss_step=0.320]\n",
      "Epoch 0:  37%|███▋      | 203/547 [03:41<06:15,  0.92it/s, v_num=3, train/loss_step=0.320]\n",
      "Epoch 0:  37%|███▋      | 203/547 [03:42<06:16,  0.91it/s, v_num=3, train/loss_step=0.465]\n",
      "Epoch 0:  37%|███▋      | 204/547 [03:42<06:13,  0.92it/s, v_num=3, train/loss_step=0.465]\n",
      "Epoch 0:  37%|███▋      | 204/547 [03:43<06:15,  0.91it/s, v_num=3, train/loss_step=0.387]\n",
      "Epoch 0:  37%|███▋      | 205/547 [03:43<06:12,  0.92it/s, v_num=3, train/loss_step=0.387]\n",
      "Epoch 0:  37%|███▋      | 205/547 [03:44<06:14,  0.91it/s, v_num=3, train/loss_step=0.327]\n",
      "Epoch 0:  38%|███▊      | 206/547 [03:44<06:11,  0.92it/s, v_num=3, train/loss_step=0.327]\n",
      "Epoch 0:  38%|███▊      | 206/547 [03:45<06:13,  0.91it/s, v_num=3, train/loss_step=0.408]\n",
      "Epoch 0:  38%|███▊      | 207/547 [03:45<06:10,  0.92it/s, v_num=3, train/loss_step=0.408]\n",
      "Epoch 0:  38%|███▊      | 207/547 [03:46<06:12,  0.91it/s, v_num=3, train/loss_step=0.258]\n",
      "Epoch 0:  38%|███▊      | 208/547 [03:46<06:09,  0.92it/s, v_num=3, train/loss_step=0.258]\n",
      "Epoch 0:  38%|███▊      | 208/547 [03:47<06:11,  0.91it/s, v_num=3, train/loss_step=0.248]\n",
      "Epoch 0:  38%|███▊      | 209/547 [03:47<06:08,  0.92it/s, v_num=3, train/loss_step=0.248]\n",
      "Epoch 0:  38%|███▊      | 209/547 [03:48<06:09,  0.91it/s, v_num=3, train/loss_step=0.527]\n",
      "Epoch 0:  38%|███▊      | 210/547 [03:48<06:07,  0.92it/s, v_num=3, train/loss_step=0.527]\n",
      "Epoch 0:  38%|███▊      | 210/547 [03:49<06:08,  0.91it/s, v_num=3, train/loss_step=0.433]\n",
      "Epoch 0:  39%|███▊      | 211/547 [03:50<06:06,  0.92it/s, v_num=3, train/loss_step=0.433]\n",
      "Epoch 0:  39%|███▊      | 211/547 [03:50<06:07,  0.91it/s, v_num=3, train/loss_step=0.360]\n",
      "Epoch 0:  39%|███▉      | 212/547 [03:51<06:05,  0.92it/s, v_num=3, train/loss_step=0.360]\n",
      "Epoch 0:  39%|███▉      | 212/547 [03:52<06:06,  0.91it/s, v_num=3, train/loss_step=0.327]\n",
      "Epoch 0:  39%|███▉      | 213/547 [03:52<06:04,  0.92it/s, v_num=3, train/loss_step=0.327]\n",
      "Epoch 0:  39%|███▉      | 213/547 [03:53<06:05,  0.91it/s, v_num=3, train/loss_step=0.501]\n",
      "Epoch 0:  39%|███▉      | 214/547 [03:53<06:03,  0.92it/s, v_num=3, train/loss_step=0.501]\n",
      "Epoch 0:  39%|███▉      | 214/547 [03:54<06:04,  0.91it/s, v_num=3, train/loss_step=0.396]\n",
      "Epoch 0:  39%|███▉      | 215/547 [03:54<06:02,  0.92it/s, v_num=3, train/loss_step=0.396]\n",
      "Epoch 0:  39%|███▉      | 215/547 [03:55<06:03,  0.91it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  39%|███▉      | 216/547 [03:55<06:00,  0.92it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  39%|███▉      | 216/547 [03:56<06:02,  0.91it/s, v_num=3, train/loss_step=0.329]\n",
      "Epoch 0:  40%|███▉      | 217/547 [03:56<05:59,  0.92it/s, v_num=3, train/loss_step=0.329]\n",
      "Epoch 0:  40%|███▉      | 217/547 [03:57<06:01,  0.91it/s, v_num=3, train/loss_step=0.280]\n",
      "Epoch 0:  40%|███▉      | 218/547 [03:57<05:58,  0.92it/s, v_num=3, train/loss_step=0.280]\n",
      "Epoch 0:  40%|███▉      | 218/547 [03:58<06:00,  0.91it/s, v_num=3, train/loss_step=0.273]\n",
      "Epoch 0:  40%|████      | 219/547 [03:58<05:57,  0.92it/s, v_num=3, train/loss_step=0.273]\n",
      "Epoch 0:  40%|████      | 219/547 [03:59<05:59,  0.91it/s, v_num=3, train/loss_step=0.265]\n",
      "Epoch 0:  40%|████      | 220/547 [03:59<05:56,  0.92it/s, v_num=3, train/loss_step=0.265]\n",
      "Epoch 0:  40%|████      | 220/547 [04:00<05:57,  0.91it/s, v_num=3, train/loss_step=0.373]\n",
      "Epoch 0:  40%|████      | 221/547 [04:01<05:55,  0.92it/s, v_num=3, train/loss_step=0.373]\n",
      "Epoch 0:  40%|████      | 221/547 [04:01<05:56,  0.91it/s, v_num=3, train/loss_step=0.219]\n",
      "Epoch 0:  41%|████      | 222/547 [04:02<05:54,  0.92it/s, v_num=3, train/loss_step=0.219]\n",
      "Epoch 0:  41%|████      | 222/547 [04:03<05:55,  0.91it/s, v_num=3, train/loss_step=0.364]\n",
      "Epoch 0:  41%|████      | 223/547 [04:03<05:53,  0.92it/s, v_num=3, train/loss_step=0.364]\n",
      "Epoch 0:  41%|████      | 223/547 [04:04<05:54,  0.91it/s, v_num=3, train/loss_step=0.288]\n",
      "Epoch 0:  41%|████      | 224/547 [04:04<05:52,  0.92it/s, v_num=3, train/loss_step=0.288]\n",
      "Epoch 0:  41%|████      | 224/547 [04:05<05:53,  0.91it/s, v_num=3, train/loss_step=0.264]\n",
      "Epoch 0:  41%|████      | 225/547 [04:05<05:51,  0.92it/s, v_num=3, train/loss_step=0.264]\n",
      "Epoch 0:  41%|████      | 225/547 [04:06<05:52,  0.91it/s, v_num=3, train/loss_step=0.325]\n",
      "Epoch 0:  41%|████▏     | 226/547 [04:06<05:50,  0.92it/s, v_num=3, train/loss_step=0.325]\n",
      "Epoch 0:  41%|████▏     | 226/547 [04:07<05:51,  0.91it/s, v_num=3, train/loss_step=0.418]\n",
      "Epoch 0:  41%|████▏     | 227/547 [04:07<05:49,  0.92it/s, v_num=3, train/loss_step=0.418]\n",
      "Epoch 0:  41%|████▏     | 227/547 [04:08<05:50,  0.91it/s, v_num=3, train/loss_step=0.279]\n",
      "Epoch 0:  42%|████▏     | 228/547 [04:08<05:47,  0.92it/s, v_num=3, train/loss_step=0.279]\n",
      "Epoch 0:  42%|████▏     | 228/547 [04:09<05:49,  0.91it/s, v_num=3, train/loss_step=0.311]\n",
      "Epoch 0:  42%|████▏     | 229/547 [04:09<05:46,  0.92it/s, v_num=3, train/loss_step=0.311]\n",
      "Epoch 0:  42%|████▏     | 229/547 [04:10<05:48,  0.91it/s, v_num=3, train/loss_step=0.317]\n",
      "Epoch 0:  42%|████▏     | 230/547 [04:10<05:45,  0.92it/s, v_num=3, train/loss_step=0.317]\n",
      "Epoch 0:  42%|████▏     | 230/547 [04:11<05:47,  0.91it/s, v_num=3, train/loss_step=0.387]\n",
      "Epoch 0:  42%|████▏     | 231/547 [04:12<05:44,  0.92it/s, v_num=3, train/loss_step=0.387]\n",
      "Epoch 0:  42%|████▏     | 231/547 [04:12<05:45,  0.91it/s, v_num=3, train/loss_step=0.227]\n",
      "Epoch 0:  42%|████▏     | 232/547 [04:13<05:43,  0.92it/s, v_num=3, train/loss_step=0.227]\n",
      "Epoch 0:  42%|████▏     | 232/547 [04:13<05:44,  0.91it/s, v_num=3, train/loss_step=0.183]\n",
      "Epoch 0:  43%|████▎     | 233/547 [04:14<05:42,  0.92it/s, v_num=3, train/loss_step=0.183]\n",
      "Epoch 0:  43%|████▎     | 233/547 [04:15<05:43,  0.91it/s, v_num=3, train/loss_step=0.182]\n",
      "Epoch 0:  43%|████▎     | 234/547 [04:15<05:41,  0.92it/s, v_num=3, train/loss_step=0.182]\n",
      "Epoch 0:  43%|████▎     | 234/547 [04:16<05:42,  0.91it/s, v_num=3, train/loss_step=0.273]\n",
      "Epoch 0:  43%|████▎     | 235/547 [04:16<05:40,  0.92it/s, v_num=3, train/loss_step=0.273]\n",
      "Epoch 0:  43%|████▎     | 235/547 [04:17<05:41,  0.91it/s, v_num=3, train/loss_step=0.186]\n",
      "Epoch 0:  43%|████▎     | 236/547 [04:17<05:39,  0.92it/s, v_num=3, train/loss_step=0.186]\n",
      "Epoch 0:  43%|████▎     | 236/547 [04:18<05:40,  0.91it/s, v_num=3, train/loss_step=0.279]\n",
      "Epoch 0:  43%|████▎     | 237/547 [04:18<05:38,  0.92it/s, v_num=3, train/loss_step=0.279]\n",
      "Epoch 0:  43%|████▎     | 237/547 [04:19<05:39,  0.91it/s, v_num=3, train/loss_step=0.456]\n",
      "Epoch 0:  44%|████▎     | 238/547 [04:19<05:37,  0.92it/s, v_num=3, train/loss_step=0.456]\n",
      "Epoch 0:  44%|████▎     | 238/547 [04:20<05:38,  0.91it/s, v_num=3, train/loss_step=0.432]\n",
      "Epoch 0:  44%|████▎     | 239/547 [04:20<05:36,  0.92it/s, v_num=3, train/loss_step=0.432]\n",
      "Epoch 0:  44%|████▎     | 239/547 [04:21<05:37,  0.91it/s, v_num=3, train/loss_step=0.463]\n",
      "Epoch 0:  44%|████▍     | 240/547 [04:21<05:35,  0.92it/s, v_num=3, train/loss_step=0.463]\n",
      "Epoch 0:  44%|████▍     | 240/547 [04:22<05:36,  0.91it/s, v_num=3, train/loss_step=0.356]\n",
      "Epoch 0:  44%|████▍     | 241/547 [04:23<05:33,  0.92it/s, v_num=3, train/loss_step=0.356]\n",
      "Epoch 0:  44%|████▍     | 241/547 [04:23<05:35,  0.91it/s, v_num=3, train/loss_step=0.266]\n",
      "Epoch 0:  44%|████▍     | 242/547 [04:24<05:32,  0.92it/s, v_num=3, train/loss_step=0.266]\n",
      "Epoch 0:  44%|████▍     | 242/547 [04:24<05:33,  0.91it/s, v_num=3, train/loss_step=0.360]\n",
      "Epoch 0:  44%|████▍     | 243/547 [04:25<05:31,  0.92it/s, v_num=3, train/loss_step=0.360]\n",
      "Epoch 0:  44%|████▍     | 243/547 [04:26<05:32,  0.91it/s, v_num=3, train/loss_step=0.200]\n",
      "Epoch 0:  45%|████▍     | 244/547 [04:26<05:30,  0.92it/s, v_num=3, train/loss_step=0.200]\n",
      "Epoch 0:  45%|████▍     | 244/547 [04:27<05:31,  0.91it/s, v_num=3, train/loss_step=0.407]\n",
      "Epoch 0:  45%|████▍     | 245/547 [04:27<05:29,  0.92it/s, v_num=3, train/loss_step=0.407]\n",
      "Epoch 0:  45%|████▍     | 245/547 [04:28<05:30,  0.91it/s, v_num=3, train/loss_step=0.298]\n",
      "Epoch 0:  45%|████▍     | 246/547 [04:28<05:28,  0.92it/s, v_num=3, train/loss_step=0.298]\n",
      "Epoch 0:  45%|████▍     | 246/547 [04:29<05:29,  0.91it/s, v_num=3, train/loss_step=0.487]\n",
      "Epoch 0:  45%|████▌     | 247/547 [04:29<05:27,  0.92it/s, v_num=3, train/loss_step=0.487]\n",
      "Epoch 0:  45%|████▌     | 247/547 [04:30<05:28,  0.91it/s, v_num=3, train/loss_step=0.139]\n",
      "Epoch 0:  45%|████▌     | 248/547 [04:30<05:26,  0.92it/s, v_num=3, train/loss_step=0.139]\n",
      "Epoch 0:  45%|████▌     | 248/547 [04:31<05:27,  0.91it/s, v_num=3, train/loss_step=0.296]\n",
      "Epoch 0:  46%|████▌     | 249/547 [04:31<05:25,  0.92it/s, v_num=3, train/loss_step=0.296]\n",
      "Epoch 0:  46%|████▌     | 249/547 [04:32<05:26,  0.91it/s, v_num=3, train/loss_step=0.407]\n",
      "Epoch 0:  46%|████▌     | 250/547 [04:32<05:24,  0.92it/s, v_num=3, train/loss_step=0.407]\n",
      "Epoch 0:  46%|████▌     | 250/547 [04:33<05:25,  0.91it/s, v_num=3, train/loss_step=0.452]\n",
      "Epoch 0:  46%|████▌     | 251/547 [04:33<05:23,  0.92it/s, v_num=3, train/loss_step=0.452]\n",
      "Epoch 0:  46%|████▌     | 251/547 [04:34<05:24,  0.91it/s, v_num=3, train/loss_step=0.418]\n",
      "Epoch 0:  46%|████▌     | 252/547 [04:35<05:22,  0.92it/s, v_num=3, train/loss_step=0.418]\n",
      "Epoch 0:  46%|████▌     | 252/547 [04:35<05:23,  0.91it/s, v_num=3, train/loss_step=0.242]\n",
      "Epoch 0:  46%|████▋     | 253/547 [04:36<05:20,  0.92it/s, v_num=3, train/loss_step=0.242]\n",
      "Epoch 0:  46%|████▋     | 253/547 [04:37<05:21,  0.91it/s, v_num=3, train/loss_step=0.354]\n",
      "Epoch 0:  46%|████▋     | 254/547 [04:37<05:19,  0.92it/s, v_num=3, train/loss_step=0.354]\n",
      "Epoch 0:  46%|████▋     | 254/547 [04:38<05:20,  0.91it/s, v_num=3, train/loss_step=0.329]\n",
      "Epoch 0:  47%|████▋     | 255/547 [04:38<05:18,  0.92it/s, v_num=3, train/loss_step=0.329]\n",
      "Epoch 0:  47%|████▋     | 255/547 [04:39<05:19,  0.91it/s, v_num=3, train/loss_step=0.151]\n",
      "Epoch 0:  47%|████▋     | 256/547 [04:39<05:17,  0.92it/s, v_num=3, train/loss_step=0.151]\n",
      "Epoch 0:  47%|████▋     | 256/547 [04:40<05:18,  0.91it/s, v_num=3, train/loss_step=0.189]\n",
      "Epoch 0:  47%|████▋     | 257/547 [04:40<05:16,  0.92it/s, v_num=3, train/loss_step=0.189]\n",
      "Epoch 0:  47%|████▋     | 257/547 [04:41<05:17,  0.91it/s, v_num=3, train/loss_step=0.155]\n",
      "Epoch 0:  47%|████▋     | 258/547 [04:41<05:15,  0.92it/s, v_num=3, train/loss_step=0.155]\n",
      "Epoch 0:  47%|████▋     | 258/547 [04:42<05:16,  0.91it/s, v_num=3, train/loss_step=0.347]\n",
      "Epoch 0:  47%|████▋     | 259/547 [04:42<05:14,  0.92it/s, v_num=3, train/loss_step=0.347]\n",
      "Epoch 0:  47%|████▋     | 259/547 [04:43<05:15,  0.91it/s, v_num=3, train/loss_step=0.293]\n",
      "Epoch 0:  48%|████▊     | 260/547 [04:43<05:13,  0.92it/s, v_num=3, train/loss_step=0.293]\n",
      "Epoch 0:  48%|████▊     | 260/547 [04:44<05:14,  0.91it/s, v_num=3, train/loss_step=0.278]\n",
      "Epoch 0:  48%|████▊     | 261/547 [04:44<05:12,  0.92it/s, v_num=3, train/loss_step=0.278]\n",
      "Epoch 0:  48%|████▊     | 261/547 [04:45<05:13,  0.91it/s, v_num=3, train/loss_step=0.272]\n",
      "Epoch 0:  48%|████▊     | 262/547 [04:46<05:11,  0.92it/s, v_num=3, train/loss_step=0.272]\n",
      "Epoch 0:  48%|████▊     | 262/547 [04:46<05:12,  0.91it/s, v_num=3, train/loss_step=0.470]\n",
      "Epoch 0:  48%|████▊     | 263/547 [04:47<05:10,  0.92it/s, v_num=3, train/loss_step=0.470]\n",
      "Epoch 0:  48%|████▊     | 263/547 [04:48<05:11,  0.91it/s, v_num=3, train/loss_step=0.145]\n",
      "Epoch 0:  48%|████▊     | 264/547 [04:48<05:09,  0.92it/s, v_num=3, train/loss_step=0.145]\n",
      "Epoch 0:  48%|████▊     | 264/547 [04:49<05:09,  0.91it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  48%|████▊     | 265/547 [04:49<05:07,  0.92it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  48%|████▊     | 265/547 [04:50<05:08,  0.91it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  49%|████▊     | 266/547 [04:50<05:06,  0.92it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  49%|████▊     | 266/547 [04:51<05:07,  0.91it/s, v_num=3, train/loss_step=0.319]\n",
      "Epoch 0:  49%|████▉     | 267/547 [04:51<05:05,  0.92it/s, v_num=3, train/loss_step=0.319]\n",
      "Epoch 0:  49%|████▉     | 267/547 [04:52<05:06,  0.91it/s, v_num=3, train/loss_step=0.449]\n",
      "Epoch 0:  49%|████▉     | 268/547 [04:52<05:04,  0.92it/s, v_num=3, train/loss_step=0.449]\n",
      "Epoch 0:  49%|████▉     | 268/547 [04:53<05:05,  0.91it/s, v_num=3, train/loss_step=0.381]\n",
      "Epoch 0:  49%|████▉     | 269/547 [04:53<05:03,  0.92it/s, v_num=3, train/loss_step=0.381]\n",
      "Epoch 0:  49%|████▉     | 269/547 [04:54<05:04,  0.91it/s, v_num=3, train/loss_step=0.245]\n",
      "Epoch 0:  49%|████▉     | 270/547 [04:54<05:02,  0.92it/s, v_num=3, train/loss_step=0.245]\n",
      "Epoch 0:  49%|████▉     | 270/547 [04:55<05:03,  0.91it/s, v_num=3, train/loss_step=0.238]\n",
      "Epoch 0:  50%|████▉     | 271/547 [04:55<05:01,  0.92it/s, v_num=3, train/loss_step=0.238]\n",
      "Epoch 0:  50%|████▉     | 271/547 [04:56<05:02,  0.91it/s, v_num=3, train/loss_step=0.231]\n",
      "Epoch 0:  50%|████▉     | 272/547 [04:57<05:00,  0.92it/s, v_num=3, train/loss_step=0.231]\n",
      "Epoch 0:  50%|████▉     | 272/547 [04:57<05:01,  0.91it/s, v_num=3, train/loss_step=0.325]\n",
      "Epoch 0:  50%|████▉     | 273/547 [04:58<04:59,  0.92it/s, v_num=3, train/loss_step=0.325]\n",
      "Epoch 0:  50%|████▉     | 273/547 [04:58<05:00,  0.91it/s, v_num=3, train/loss_step=0.411]\n",
      "Epoch 0:  50%|█████     | 274/547 [04:59<04:58,  0.92it/s, v_num=3, train/loss_step=0.411]\n",
      "Epoch 0:  50%|█████     | 274/547 [05:00<04:58,  0.91it/s, v_num=3, train/loss_step=0.293]\n",
      "Epoch 0:  50%|█████     | 275/547 [05:00<04:57,  0.92it/s, v_num=3, train/loss_step=0.293]\n",
      "Epoch 0:  50%|█████     | 275/547 [05:01<04:57,  0.91it/s, v_num=3, train/loss_step=0.360]\n",
      "Epoch 0:  50%|█████     | 276/547 [05:01<04:55,  0.92it/s, v_num=3, train/loss_step=0.360]\n",
      "Epoch 0:  50%|█████     | 276/547 [05:02<04:56,  0.91it/s, v_num=3, train/loss_step=0.328]\n",
      "Epoch 0:  51%|█████     | 277/547 [05:02<04:54,  0.92it/s, v_num=3, train/loss_step=0.328]\n",
      "Epoch 0:  51%|█████     | 277/547 [05:03<04:55,  0.91it/s, v_num=3, train/loss_step=0.380]\n",
      "Epoch 0:  51%|█████     | 278/547 [05:03<04:53,  0.92it/s, v_num=3, train/loss_step=0.380]\n",
      "Epoch 0:  51%|█████     | 278/547 [05:04<04:54,  0.91it/s, v_num=3, train/loss_step=0.0927]\n",
      "Epoch 0:  51%|█████     | 279/547 [05:04<04:52,  0.92it/s, v_num=3, train/loss_step=0.0927]\n",
      "Epoch 0:  51%|█████     | 279/547 [05:05<04:53,  0.91it/s, v_num=3, train/loss_step=0.405]\n",
      "Epoch 0:  51%|█████     | 280/547 [05:05<04:51,  0.92it/s, v_num=3, train/loss_step=0.405]\n",
      "Epoch 0:  51%|█████     | 280/547 [05:06<04:52,  0.91it/s, v_num=3, train/loss_step=0.123]\n",
      "Epoch 0:  51%|█████▏    | 281/547 [05:06<04:50,  0.92it/s, v_num=3, train/loss_step=0.123]\n",
      "Epoch 0:  51%|█████▏    | 281/547 [05:07<04:51,  0.91it/s, v_num=3, train/loss_step=0.462]\n",
      "Epoch 0:  52%|█████▏    | 282/547 [05:08<04:49,  0.92it/s, v_num=3, train/loss_step=0.462]\n",
      "Epoch 0:  52%|█████▏    | 282/547 [05:08<04:50,  0.91it/s, v_num=3, train/loss_step=0.300]\n",
      "Epoch 0:  52%|█████▏    | 283/547 [05:09<04:48,  0.92it/s, v_num=3, train/loss_step=0.300]\n",
      "Epoch 0:  52%|█████▏    | 283/547 [05:09<04:49,  0.91it/s, v_num=3, train/loss_step=0.155]\n",
      "Epoch 0:  52%|█████▏    | 284/547 [05:10<04:47,  0.92it/s, v_num=3, train/loss_step=0.155]\n",
      "Epoch 0:  52%|█████▏    | 284/547 [05:11<04:48,  0.91it/s, v_num=3, train/loss_step=0.230]\n",
      "Epoch 0:  52%|█████▏    | 285/547 [05:11<04:46,  0.92it/s, v_num=3, train/loss_step=0.230]\n",
      "Epoch 0:  52%|█████▏    | 285/547 [05:12<04:46,  0.91it/s, v_num=3, train/loss_step=0.387]\n",
      "Epoch 0:  52%|█████▏    | 286/547 [05:12<04:45,  0.92it/s, v_num=3, train/loss_step=0.387]\n",
      "Epoch 0:  52%|█████▏    | 286/547 [05:13<04:45,  0.91it/s, v_num=3, train/loss_step=0.102]\n",
      "Epoch 0:  52%|█████▏    | 287/547 [05:13<04:43,  0.92it/s, v_num=3, train/loss_step=0.102]\n",
      "Epoch 0:  52%|█████▏    | 287/547 [05:14<04:44,  0.91it/s, v_num=3, train/loss_step=0.105]\n",
      "Epoch 0:  53%|█████▎    | 288/547 [05:14<04:42,  0.92it/s, v_num=3, train/loss_step=0.105]\n",
      "Epoch 0:  53%|█████▎    | 288/547 [05:15<04:43,  0.91it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  53%|█████▎    | 289/547 [05:15<04:41,  0.92it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  53%|█████▎    | 289/547 [05:16<04:42,  0.91it/s, v_num=3, train/loss_step=0.253]\n",
      "Epoch 0:  53%|█████▎    | 290/547 [05:16<04:40,  0.92it/s, v_num=3, train/loss_step=0.253]\n",
      "Epoch 0:  53%|█████▎    | 290/547 [05:17<04:41,  0.91it/s, v_num=3, train/loss_step=0.406]\n",
      "Epoch 0:  53%|█████▎    | 291/547 [05:17<04:39,  0.92it/s, v_num=3, train/loss_step=0.406]\n",
      "Epoch 0:  53%|█████▎    | 291/547 [05:18<04:40,  0.91it/s, v_num=3, train/loss_step=0.229]\n",
      "Epoch 0:  53%|█████▎    | 292/547 [05:18<04:38,  0.92it/s, v_num=3, train/loss_step=0.229]\n",
      "Epoch 0:  53%|█████▎    | 292/547 [05:19<04:39,  0.91it/s, v_num=3, train/loss_step=0.169]\n",
      "Epoch 0:  54%|█████▎    | 293/547 [05:20<04:37,  0.92it/s, v_num=3, train/loss_step=0.169]\n",
      "Epoch 0:  54%|█████▎    | 293/547 [05:20<04:38,  0.91it/s, v_num=3, train/loss_step=0.356]\n",
      "Epoch 0:  54%|█████▎    | 294/547 [05:21<04:36,  0.92it/s, v_num=3, train/loss_step=0.356]\n",
      "Epoch 0:  54%|█████▎    | 294/547 [05:22<04:37,  0.91it/s, v_num=3, train/loss_step=0.199]\n",
      "Epoch 0:  54%|█████▍    | 295/547 [05:22<04:35,  0.92it/s, v_num=3, train/loss_step=0.199]\n",
      "Epoch 0:  54%|█████▍    | 295/547 [05:23<04:36,  0.91it/s, v_num=3, train/loss_step=0.212]\n",
      "Epoch 0:  54%|█████▍    | 296/547 [05:23<04:34,  0.92it/s, v_num=3, train/loss_step=0.212]\n",
      "Epoch 0:  54%|█████▍    | 296/547 [05:24<04:34,  0.91it/s, v_num=3, train/loss_step=0.243]\n",
      "Epoch 0:  54%|█████▍    | 297/547 [05:24<04:33,  0.92it/s, v_num=3, train/loss_step=0.243]\n",
      "Epoch 0:  54%|█████▍    | 297/547 [05:25<04:33,  0.91it/s, v_num=3, train/loss_step=0.140]\n",
      "Epoch 0:  54%|█████▍    | 298/547 [05:25<04:32,  0.92it/s, v_num=3, train/loss_step=0.140]\n",
      "Epoch 0:  54%|█████▍    | 298/547 [05:26<04:32,  0.91it/s, v_num=3, train/loss_step=0.254]\n",
      "Epoch 0:  55%|█████▍    | 299/547 [05:26<04:30,  0.92it/s, v_num=3, train/loss_step=0.254]\n",
      "Epoch 0:  55%|█████▍    | 299/547 [05:27<04:31,  0.91it/s, v_num=3, train/loss_step=0.292]\n",
      "Epoch 0:  55%|█████▍    | 300/547 [05:27<04:29,  0.92it/s, v_num=3, train/loss_step=0.292]\n",
      "Epoch 0:  55%|█████▍    | 300/547 [05:28<04:30,  0.91it/s, v_num=3, train/loss_step=0.282]\n",
      "Epoch 0:  55%|█████▌    | 301/547 [05:28<04:28,  0.92it/s, v_num=3, train/loss_step=0.282]\n",
      "Epoch 0:  55%|█████▌    | 301/547 [05:29<04:29,  0.91it/s, v_num=3, train/loss_step=0.627]\n",
      "Epoch 0:  55%|█████▌    | 302/547 [05:29<04:27,  0.92it/s, v_num=3, train/loss_step=0.627]\n",
      "Epoch 0:  55%|█████▌    | 302/547 [05:30<04:28,  0.91it/s, v_num=3, train/loss_step=0.289]\n",
      "Epoch 0:  55%|█████▌    | 303/547 [05:31<04:26,  0.92it/s, v_num=3, train/loss_step=0.289]\n",
      "Epoch 0:  55%|█████▌    | 303/547 [05:31<04:27,  0.91it/s, v_num=3, train/loss_step=0.209]\n",
      "Epoch 0:  56%|█████▌    | 304/547 [05:32<04:25,  0.92it/s, v_num=3, train/loss_step=0.209]\n",
      "Epoch 0:  56%|█████▌    | 304/547 [05:33<04:26,  0.91it/s, v_num=3, train/loss_step=0.348]\n",
      "Epoch 0:  56%|█████▌    | 305/547 [05:33<04:24,  0.92it/s, v_num=3, train/loss_step=0.348]\n",
      "Epoch 0:  56%|█████▌    | 305/547 [05:34<04:25,  0.91it/s, v_num=3, train/loss_step=0.0885]\n",
      "Epoch 0:  56%|█████▌    | 306/547 [05:34<04:23,  0.92it/s, v_num=3, train/loss_step=0.0885]\n",
      "Epoch 0:  56%|█████▌    | 306/547 [05:35<04:24,  0.91it/s, v_num=3, train/loss_step=0.203]\n",
      "Epoch 0:  56%|█████▌    | 307/547 [05:35<04:22,  0.92it/s, v_num=3, train/loss_step=0.203]\n",
      "Epoch 0:  56%|█████▌    | 307/547 [05:36<04:22,  0.91it/s, v_num=3, train/loss_step=0.325]\n",
      "Epoch 0:  56%|█████▋    | 308/547 [05:36<04:21,  0.92it/s, v_num=3, train/loss_step=0.325]\n",
      "Epoch 0:  56%|█████▋    | 308/547 [05:37<04:21,  0.91it/s, v_num=3, train/loss_step=0.198]\n",
      "Epoch 0:  56%|█████▋    | 309/547 [05:37<04:20,  0.91it/s, v_num=3, train/loss_step=0.198]\n",
      "Epoch 0:  56%|█████▋    | 309/547 [05:38<04:20,  0.91it/s, v_num=3, train/loss_step=0.394]\n",
      "Epoch 0:  57%|█████▋    | 310/547 [05:39<04:19,  0.91it/s, v_num=3, train/loss_step=0.394]\n",
      "Epoch 0:  57%|█████▋    | 310/547 [05:39<04:19,  0.91it/s, v_num=3, train/loss_step=0.0923]\n",
      "Epoch 0:  57%|█████▋    | 311/547 [05:40<04:18,  0.91it/s, v_num=3, train/loss_step=0.0923]\n",
      "Epoch 0:  57%|█████▋    | 311/547 [05:40<04:18,  0.91it/s, v_num=3, train/loss_step=0.160]\n",
      "Epoch 0:  57%|█████▋    | 312/547 [05:41<04:16,  0.91it/s, v_num=3, train/loss_step=0.160]\n",
      "Epoch 0:  57%|█████▋    | 312/547 [05:41<04:17,  0.91it/s, v_num=3, train/loss_step=0.093]\n",
      "Epoch 0:  57%|█████▋    | 313/547 [05:42<04:15,  0.91it/s, v_num=3, train/loss_step=0.093]\n",
      "Epoch 0:  57%|█████▋    | 313/547 [05:43<04:16,  0.91it/s, v_num=3, train/loss_step=0.125]\n",
      "Epoch 0:  57%|█████▋    | 314/547 [05:43<04:14,  0.91it/s, v_num=3, train/loss_step=0.125]\n",
      "Epoch 0:  57%|█████▋    | 314/547 [05:44<04:15,  0.91it/s, v_num=3, train/loss_step=0.340]\n",
      "Epoch 0:  58%|█████▊    | 315/547 [05:44<04:13,  0.91it/s, v_num=3, train/loss_step=0.340]\n",
      "Epoch 0:  58%|█████▊    | 315/547 [05:45<04:14,  0.91it/s, v_num=3, train/loss_step=0.409]\n",
      "Epoch 0:  58%|█████▊    | 316/547 [05:45<04:12,  0.91it/s, v_num=3, train/loss_step=0.409]\n",
      "Epoch 0:  58%|█████▊    | 316/547 [05:46<04:13,  0.91it/s, v_num=3, train/loss_step=0.288]\n",
      "Epoch 0:  58%|█████▊    | 317/547 [05:46<04:11,  0.91it/s, v_num=3, train/loss_step=0.288]\n",
      "Epoch 0:  58%|█████▊    | 317/547 [05:47<04:12,  0.91it/s, v_num=3, train/loss_step=0.0854]\n",
      "Epoch 0:  58%|█████▊    | 318/547 [05:47<04:10,  0.91it/s, v_num=3, train/loss_step=0.0854]\n",
      "Epoch 0:  58%|█████▊    | 318/547 [05:48<04:11,  0.91it/s, v_num=3, train/loss_step=0.0894]\n",
      "Epoch 0:  58%|█████▊    | 319/547 [05:48<04:09,  0.91it/s, v_num=3, train/loss_step=0.0894]\n",
      "Epoch 0:  58%|█████▊    | 319/547 [05:49<04:09,  0.91it/s, v_num=3, train/loss_step=0.312]\n",
      "Epoch 0:  59%|█████▊    | 320/547 [05:49<04:08,  0.91it/s, v_num=3, train/loss_step=0.312]\n",
      "Epoch 0:  59%|█████▊    | 320/547 [05:50<04:08,  0.91it/s, v_num=3, train/loss_step=0.196]\n",
      "Epoch 0:  59%|█████▊    | 321/547 [05:51<04:07,  0.91it/s, v_num=3, train/loss_step=0.196]\n",
      "Epoch 0:  59%|█████▊    | 321/547 [05:51<04:07,  0.91it/s, v_num=3, train/loss_step=0.355]\n",
      "Epoch 0:  59%|█████▉    | 322/547 [05:52<04:06,  0.91it/s, v_num=3, train/loss_step=0.355]\n",
      "Epoch 0:  59%|█████▉    | 322/547 [05:52<04:06,  0.91it/s, v_num=3, train/loss_step=0.0665]\n",
      "Epoch 0:  59%|█████▉    | 323/547 [05:53<04:04,  0.91it/s, v_num=3, train/loss_step=0.0665]\n",
      "Epoch 0:  59%|█████▉    | 323/547 [05:54<04:05,  0.91it/s, v_num=3, train/loss_step=0.299]\n",
      "Epoch 0:  59%|█████▉    | 324/547 [05:54<04:03,  0.91it/s, v_num=3, train/loss_step=0.299]\n",
      "Epoch 0:  59%|█████▉    | 324/547 [05:55<04:04,  0.91it/s, v_num=3, train/loss_step=0.0988]\n",
      "Epoch 0:  59%|█████▉    | 325/547 [05:55<04:02,  0.91it/s, v_num=3, train/loss_step=0.0988]\n",
      "Epoch 0:  59%|█████▉    | 325/547 [05:56<04:03,  0.91it/s, v_num=3, train/loss_step=0.487]\n",
      "Epoch 0:  60%|█████▉    | 326/547 [05:56<04:01,  0.91it/s, v_num=3, train/loss_step=0.487]\n",
      "Epoch 0:  60%|█████▉    | 326/547 [05:57<04:02,  0.91it/s, v_num=3, train/loss_step=0.282]\n",
      "Epoch 0:  60%|█████▉    | 327/547 [05:57<04:00,  0.91it/s, v_num=3, train/loss_step=0.282]\n",
      "Epoch 0:  60%|█████▉    | 327/547 [05:58<04:01,  0.91it/s, v_num=3, train/loss_step=0.141]\n",
      "Epoch 0:  60%|█████▉    | 328/547 [05:58<03:59,  0.91it/s, v_num=3, train/loss_step=0.141]\n",
      "Epoch 0:  60%|█████▉    | 328/547 [05:59<04:00,  0.91it/s, v_num=3, train/loss_step=0.162]\n",
      "Epoch 0:  60%|██████    | 329/547 [05:59<03:58,  0.91it/s, v_num=3, train/loss_step=0.162]\n",
      "Epoch 0:  60%|██████    | 329/547 [06:00<03:59,  0.91it/s, v_num=3, train/loss_step=0.474]\n",
      "Epoch 0:  60%|██████    | 330/547 [06:00<03:57,  0.91it/s, v_num=3, train/loss_step=0.474]\n",
      "Epoch 0:  60%|██████    | 330/547 [06:01<03:57,  0.91it/s, v_num=3, train/loss_step=0.218]\n",
      "Epoch 0:  61%|██████    | 331/547 [06:02<03:56,  0.91it/s, v_num=3, train/loss_step=0.218]\n",
      "Epoch 0:  61%|██████    | 331/547 [06:02<03:56,  0.91it/s, v_num=3, train/loss_step=0.112]\n",
      "Epoch 0:  61%|██████    | 332/547 [06:03<03:55,  0.91it/s, v_num=3, train/loss_step=0.112]\n",
      "Epoch 0:  61%|██████    | 332/547 [06:04<03:55,  0.91it/s, v_num=3, train/loss_step=0.355]\n",
      "Epoch 0:  61%|██████    | 333/547 [06:04<03:54,  0.91it/s, v_num=3, train/loss_step=0.355]\n",
      "Epoch 0:  61%|██████    | 333/547 [06:05<03:54,  0.91it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  61%|██████    | 334/547 [06:05<03:52,  0.91it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  61%|██████    | 334/547 [06:06<03:53,  0.91it/s, v_num=3, train/loss_step=0.361]\n",
      "Epoch 0:  61%|██████    | 335/547 [06:06<03:51,  0.91it/s, v_num=3, train/loss_step=0.361]\n",
      "Epoch 0:  61%|██████    | 335/547 [06:07<03:52,  0.91it/s, v_num=3, train/loss_step=0.352]\n",
      "Epoch 0:  61%|██████▏   | 336/547 [06:07<03:50,  0.91it/s, v_num=3, train/loss_step=0.352]\n",
      "Epoch 0:  61%|██████▏   | 336/547 [06:08<03:51,  0.91it/s, v_num=3, train/loss_step=0.331]\n",
      "Epoch 0:  62%|██████▏   | 337/547 [06:08<03:49,  0.91it/s, v_num=3, train/loss_step=0.331]\n",
      "Epoch 0:  62%|██████▏   | 337/547 [06:09<03:50,  0.91it/s, v_num=3, train/loss_step=0.202]\n",
      "Epoch 0:  62%|██████▏   | 338/547 [06:09<03:48,  0.91it/s, v_num=3, train/loss_step=0.202]\n",
      "Epoch 0:  62%|██████▏   | 338/547 [06:10<03:49,  0.91it/s, v_num=3, train/loss_step=0.226]\n",
      "Epoch 0:  62%|██████▏   | 339/547 [06:10<03:47,  0.91it/s, v_num=3, train/loss_step=0.226]\n",
      "Epoch 0:  62%|██████▏   | 339/547 [06:11<03:48,  0.91it/s, v_num=3, train/loss_step=0.0967]\n",
      "Epoch 0:  62%|██████▏   | 340/547 [06:11<03:46,  0.91it/s, v_num=3, train/loss_step=0.0967]\n",
      "Epoch 0:  62%|██████▏   | 340/547 [06:12<03:46,  0.91it/s, v_num=3, train/loss_step=0.238]\n",
      "Epoch 0:  62%|██████▏   | 341/547 [06:13<03:45,  0.91it/s, v_num=3, train/loss_step=0.238]\n",
      "Epoch 0:  62%|██████▏   | 341/547 [06:13<03:45,  0.91it/s, v_num=3, train/loss_step=0.311]\n",
      "Epoch 0:  63%|██████▎   | 342/547 [06:14<03:44,  0.91it/s, v_num=3, train/loss_step=0.311]\n",
      "Epoch 0:  63%|██████▎   | 342/547 [06:15<03:44,  0.91it/s, v_num=3, train/loss_step=0.149]\n",
      "Epoch 0:  63%|██████▎   | 343/547 [06:15<03:43,  0.91it/s, v_num=3, train/loss_step=0.149]\n",
      "Epoch 0:  63%|██████▎   | 343/547 [06:16<03:43,  0.91it/s, v_num=3, train/loss_step=0.216]\n",
      "Epoch 0:  63%|██████▎   | 344/547 [06:16<03:42,  0.91it/s, v_num=3, train/loss_step=0.216]\n",
      "Epoch 0:  63%|██████▎   | 344/547 [06:17<03:42,  0.91it/s, v_num=3, train/loss_step=0.295]\n",
      "Epoch 0:  63%|██████▎   | 345/547 [06:17<03:41,  0.91it/s, v_num=3, train/loss_step=0.295]\n",
      "Epoch 0:  63%|██████▎   | 345/547 [06:18<03:41,  0.91it/s, v_num=3, train/loss_step=0.117]\n",
      "Epoch 0:  63%|██████▎   | 346/547 [06:18<03:39,  0.91it/s, v_num=3, train/loss_step=0.117]\n",
      "Epoch 0:  63%|██████▎   | 346/547 [06:19<03:40,  0.91it/s, v_num=3, train/loss_step=0.222]\n",
      "Epoch 0:  63%|██████▎   | 347/547 [06:19<03:38,  0.91it/s, v_num=3, train/loss_step=0.222]\n",
      "Epoch 0:  63%|██████▎   | 347/547 [06:20<03:39,  0.91it/s, v_num=3, train/loss_step=0.163]\n",
      "Epoch 0:  64%|██████▎   | 348/547 [06:20<03:37,  0.91it/s, v_num=3, train/loss_step=0.163]\n",
      "Epoch 0:  64%|██████▎   | 348/547 [06:21<03:38,  0.91it/s, v_num=3, train/loss_step=0.265]\n",
      "Epoch 0:  64%|██████▍   | 349/547 [06:21<03:36,  0.91it/s, v_num=3, train/loss_step=0.265]\n",
      "Epoch 0:  64%|██████▍   | 349/547 [06:22<03:37,  0.91it/s, v_num=3, train/loss_step=0.245]\n",
      "Epoch 0:  64%|██████▍   | 350/547 [06:23<03:35,  0.91it/s, v_num=3, train/loss_step=0.245]\n",
      "Epoch 0:  64%|██████▍   | 350/547 [06:23<03:36,  0.91it/s, v_num=3, train/loss_step=0.342]\n",
      "Epoch 0:  64%|██████▍   | 351/547 [06:24<03:34,  0.91it/s, v_num=3, train/loss_step=0.342]\n",
      "Epoch 0:  64%|██████▍   | 351/547 [06:24<03:34,  0.91it/s, v_num=3, train/loss_step=0.511]\n",
      "Epoch 0:  64%|██████▍   | 352/547 [06:25<03:33,  0.91it/s, v_num=3, train/loss_step=0.511]\n",
      "Epoch 0:  64%|██████▍   | 352/547 [06:26<03:33,  0.91it/s, v_num=3, train/loss_step=0.229]\n",
      "Epoch 0:  65%|██████▍   | 353/547 [06:26<03:32,  0.91it/s, v_num=3, train/loss_step=0.229]\n",
      "Epoch 0:  65%|██████▍   | 353/547 [06:27<03:32,  0.91it/s, v_num=3, train/loss_step=0.347]\n",
      "Epoch 0:  65%|██████▍   | 354/547 [06:27<03:31,  0.91it/s, v_num=3, train/loss_step=0.347]\n",
      "Epoch 0:  65%|██████▍   | 354/547 [06:28<03:31,  0.91it/s, v_num=3, train/loss_step=0.397]\n",
      "Epoch 0:  65%|██████▍   | 355/547 [06:28<03:30,  0.91it/s, v_num=3, train/loss_step=0.397]\n",
      "Epoch 0:  65%|██████▍   | 355/547 [06:29<03:30,  0.91it/s, v_num=3, train/loss_step=0.153]\n",
      "Epoch 0:  65%|██████▌   | 356/547 [06:29<03:29,  0.91it/s, v_num=3, train/loss_step=0.153]\n",
      "Epoch 0:  65%|██████▌   | 356/547 [06:30<03:29,  0.91it/s, v_num=3, train/loss_step=0.119]\n",
      "Epoch 0:  65%|██████▌   | 357/547 [06:30<03:27,  0.91it/s, v_num=3, train/loss_step=0.119]\n",
      "Epoch 0:  65%|██████▌   | 357/547 [06:31<03:28,  0.91it/s, v_num=3, train/loss_step=0.527]\n",
      "Epoch 0:  65%|██████▌   | 358/547 [06:31<03:26,  0.91it/s, v_num=3, train/loss_step=0.527]\n",
      "Epoch 0:  65%|██████▌   | 358/547 [06:32<03:27,  0.91it/s, v_num=3, train/loss_step=0.277]\n",
      "Epoch 0:  66%|██████▌   | 359/547 [06:32<03:25,  0.91it/s, v_num=3, train/loss_step=0.277]\n",
      "Epoch 0:  66%|██████▌   | 359/547 [06:33<03:26,  0.91it/s, v_num=3, train/loss_step=0.199]\n",
      "Epoch 0:  66%|██████▌   | 360/547 [06:34<03:24,  0.91it/s, v_num=3, train/loss_step=0.199]\n",
      "Epoch 0:  66%|██████▌   | 360/547 [06:34<03:25,  0.91it/s, v_num=3, train/loss_step=0.298]\n",
      "Epoch 0:  66%|██████▌   | 361/547 [06:35<03:23,  0.91it/s, v_num=3, train/loss_step=0.298]\n",
      "Epoch 0:  66%|██████▌   | 361/547 [06:36<03:24,  0.91it/s, v_num=3, train/loss_step=0.0895]\n",
      "Epoch 0:  66%|██████▌   | 362/547 [06:36<03:22,  0.91it/s, v_num=3, train/loss_step=0.0895]\n",
      "Epoch 0:  66%|██████▌   | 362/547 [06:37<03:22,  0.91it/s, v_num=3, train/loss_step=0.120]\n",
      "Epoch 0:  66%|██████▋   | 363/547 [06:37<03:21,  0.91it/s, v_num=3, train/loss_step=0.120]\n",
      "Epoch 0:  66%|██████▋   | 363/547 [06:38<03:21,  0.91it/s, v_num=3, train/loss_step=0.0692]\n",
      "Epoch 0:  67%|██████▋   | 364/547 [06:38<03:20,  0.91it/s, v_num=3, train/loss_step=0.0692]\n",
      "Epoch 0:  67%|██████▋   | 364/547 [06:39<03:20,  0.91it/s, v_num=3, train/loss_step=0.127]\n",
      "Epoch 0:  67%|██████▋   | 365/547 [06:39<03:19,  0.91it/s, v_num=3, train/loss_step=0.127]\n",
      "Epoch 0:  67%|██████▋   | 365/547 [06:40<03:19,  0.91it/s, v_num=3, train/loss_step=0.167]\n",
      "Epoch 0:  67%|██████▋   | 366/547 [06:40<03:18,  0.91it/s, v_num=3, train/loss_step=0.167]\n",
      "Epoch 0:  67%|██████▋   | 366/547 [06:41<03:18,  0.91it/s, v_num=3, train/loss_step=0.200]\n",
      "Epoch 0:  67%|██████▋   | 367/547 [06:41<03:17,  0.91it/s, v_num=3, train/loss_step=0.200]\n",
      "Epoch 0:  67%|██████▋   | 367/547 [06:42<03:17,  0.91it/s, v_num=3, train/loss_step=0.261]\n",
      "Epoch 0:  67%|██████▋   | 368/547 [06:42<03:15,  0.91it/s, v_num=3, train/loss_step=0.261]\n",
      "Epoch 0:  67%|██████▋   | 368/547 [06:43<03:16,  0.91it/s, v_num=3, train/loss_step=0.324]\n",
      "Epoch 0:  67%|██████▋   | 369/547 [06:44<03:14,  0.91it/s, v_num=3, train/loss_step=0.324]\n",
      "Epoch 0:  67%|██████▋   | 369/547 [06:44<03:15,  0.91it/s, v_num=3, train/loss_step=0.250]\n",
      "Epoch 0:  68%|██████▊   | 370/547 [06:45<03:13,  0.91it/s, v_num=3, train/loss_step=0.250]\n",
      "Epoch 0:  68%|██████▊   | 370/547 [06:46<03:14,  0.91it/s, v_num=3, train/loss_step=0.415]\n",
      "Epoch 0:  68%|██████▊   | 371/547 [06:46<03:12,  0.91it/s, v_num=3, train/loss_step=0.415]\n",
      "Epoch 0:  68%|██████▊   | 371/547 [06:47<03:13,  0.91it/s, v_num=3, train/loss_step=0.309]\n",
      "Epoch 0:  68%|██████▊   | 372/547 [06:47<03:11,  0.91it/s, v_num=3, train/loss_step=0.309]\n",
      "Epoch 0:  68%|██████▊   | 372/547 [06:48<03:12,  0.91it/s, v_num=3, train/loss_step=0.296]\n",
      "Epoch 0:  68%|██████▊   | 373/547 [06:48<03:10,  0.91it/s, v_num=3, train/loss_step=0.296]\n",
      "Epoch 0:  68%|██████▊   | 373/547 [06:49<03:10,  0.91it/s, v_num=3, train/loss_step=0.0859]\n",
      "Epoch 0:  68%|██████▊   | 374/547 [06:49<03:09,  0.91it/s, v_num=3, train/loss_step=0.0859]\n",
      "Epoch 0:  68%|██████▊   | 374/547 [06:50<03:09,  0.91it/s, v_num=3, train/loss_step=0.359]\n",
      "Epoch 0:  69%|██████▊   | 375/547 [06:50<03:08,  0.91it/s, v_num=3, train/loss_step=0.359]\n",
      "Epoch 0:  69%|██████▊   | 375/547 [06:51<03:08,  0.91it/s, v_num=3, train/loss_step=0.481]\n",
      "Epoch 0:  69%|██████▊   | 376/547 [06:51<03:07,  0.91it/s, v_num=3, train/loss_step=0.481]\n",
      "Epoch 0:  69%|██████▊   | 376/547 [06:52<03:07,  0.91it/s, v_num=3, train/loss_step=0.297]\n",
      "Epoch 0:  69%|██████▉   | 377/547 [06:52<03:06,  0.91it/s, v_num=3, train/loss_step=0.297]\n",
      "Epoch 0:  69%|██████▉   | 377/547 [06:53<03:06,  0.91it/s, v_num=3, train/loss_step=0.167]\n",
      "Epoch 0:  69%|██████▉   | 378/547 [06:53<03:05,  0.91it/s, v_num=3, train/loss_step=0.167]\n",
      "Epoch 0:  69%|██████▉   | 378/547 [06:54<03:05,  0.91it/s, v_num=3, train/loss_step=0.0766]\n",
      "Epoch 0:  69%|██████▉   | 379/547 [06:55<03:04,  0.91it/s, v_num=3, train/loss_step=0.0766]\n",
      "Epoch 0:  69%|██████▉   | 379/547 [06:55<03:04,  0.91it/s, v_num=3, train/loss_step=0.143]\n",
      "Epoch 0:  69%|██████▉   | 380/547 [06:56<03:02,  0.91it/s, v_num=3, train/loss_step=0.143]\n",
      "Epoch 0:  69%|██████▉   | 380/547 [06:57<03:03,  0.91it/s, v_num=3, train/loss_step=0.274]\n",
      "Epoch 0:  70%|██████▉   | 381/547 [06:57<03:01,  0.91it/s, v_num=3, train/loss_step=0.274]\n",
      "Epoch 0:  70%|██████▉   | 381/547 [06:58<03:02,  0.91it/s, v_num=3, train/loss_step=0.228]\n",
      "Epoch 0:  70%|██████▉   | 382/547 [06:58<03:00,  0.91it/s, v_num=3, train/loss_step=0.228]\n",
      "Epoch 0:  70%|██████▉   | 382/547 [06:59<03:01,  0.91it/s, v_num=3, train/loss_step=0.425]\n",
      "Epoch 0:  70%|███████   | 383/547 [06:59<02:59,  0.91it/s, v_num=3, train/loss_step=0.425]\n",
      "Epoch 0:  70%|███████   | 383/547 [07:00<03:00,  0.91it/s, v_num=3, train/loss_step=0.0792]\n",
      "Epoch 0:  70%|███████   | 384/547 [07:00<02:58,  0.91it/s, v_num=3, train/loss_step=0.0792]\n",
      "Epoch 0:  70%|███████   | 384/547 [07:01<02:58,  0.91it/s, v_num=3, train/loss_step=0.0967]\n",
      "Epoch 0:  70%|███████   | 385/547 [07:01<02:57,  0.91it/s, v_num=3, train/loss_step=0.0967]\n",
      "Epoch 0:  70%|███████   | 385/547 [07:02<02:57,  0.91it/s, v_num=3, train/loss_step=0.146]\n",
      "Epoch 0:  71%|███████   | 386/547 [07:02<02:56,  0.91it/s, v_num=3, train/loss_step=0.146]\n",
      "Epoch 0:  71%|███████   | 386/547 [07:03<02:56,  0.91it/s, v_num=3, train/loss_step=0.228]\n",
      "Epoch 0:  71%|███████   | 387/547 [07:03<02:55,  0.91it/s, v_num=3, train/loss_step=0.228]\n",
      "Epoch 0:  71%|███████   | 387/547 [07:04<02:55,  0.91it/s, v_num=3, train/loss_step=0.467]\n",
      "Epoch 0:  71%|███████   | 388/547 [07:05<02:54,  0.91it/s, v_num=3, train/loss_step=0.467]\n",
      "Epoch 0:  71%|███████   | 388/547 [07:05<02:54,  0.91it/s, v_num=3, train/loss_step=0.089]\n",
      "Epoch 0:  71%|███████   | 389/547 [07:06<02:53,  0.91it/s, v_num=3, train/loss_step=0.089]\n",
      "Epoch 0:  71%|███████   | 389/547 [07:07<02:53,  0.91it/s, v_num=3, train/loss_step=0.241]\n",
      "Epoch 0:  71%|███████▏  | 390/547 [07:07<02:52,  0.91it/s, v_num=3, train/loss_step=0.241]\n",
      "Epoch 0:  71%|███████▏  | 390/547 [07:08<02:52,  0.91it/s, v_num=3, train/loss_step=0.305]\n",
      "Epoch 0:  71%|███████▏  | 391/547 [07:08<02:50,  0.91it/s, v_num=3, train/loss_step=0.305]\n",
      "Epoch 0:  71%|███████▏  | 391/547 [07:09<02:51,  0.91it/s, v_num=3, train/loss_step=0.100]\n",
      "Epoch 0:  72%|███████▏  | 392/547 [07:09<02:49,  0.91it/s, v_num=3, train/loss_step=0.100]\n",
      "Epoch 0:  72%|███████▏  | 392/547 [07:10<02:50,  0.91it/s, v_num=3, train/loss_step=0.146]\n",
      "Epoch 0:  72%|███████▏  | 393/547 [07:10<02:48,  0.91it/s, v_num=3, train/loss_step=0.146]\n",
      "Epoch 0:  72%|███████▏  | 393/547 [07:11<02:49,  0.91it/s, v_num=3, train/loss_step=0.0801]\n",
      "Epoch 0:  72%|███████▏  | 394/547 [07:11<02:47,  0.91it/s, v_num=3, train/loss_step=0.0801]\n",
      "Epoch 0:  72%|███████▏  | 394/547 [07:12<02:47,  0.91it/s, v_num=3, train/loss_step=0.403]\n",
      "Epoch 0:  72%|███████▏  | 395/547 [07:12<02:46,  0.91it/s, v_num=3, train/loss_step=0.403]\n",
      "Epoch 0:  72%|███████▏  | 395/547 [07:13<02:46,  0.91it/s, v_num=3, train/loss_step=0.446]\n",
      "Epoch 0:  72%|███████▏  | 396/547 [07:13<02:45,  0.91it/s, v_num=3, train/loss_step=0.446]\n",
      "Epoch 0:  72%|███████▏  | 396/547 [07:14<02:45,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  73%|███████▎  | 397/547 [07:15<02:44,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  73%|███████▎  | 397/547 [07:15<02:44,  0.91it/s, v_num=3, train/loss_step=0.576]\n",
      "Epoch 0:  73%|███████▎  | 398/547 [07:16<02:43,  0.91it/s, v_num=3, train/loss_step=0.576]\n",
      "Epoch 0:  73%|███████▎  | 398/547 [07:16<02:43,  0.91it/s, v_num=3, train/loss_step=0.143]\n",
      "Epoch 0:  73%|███████▎  | 399/547 [07:17<02:42,  0.91it/s, v_num=3, train/loss_step=0.143]\n",
      "Epoch 0:  73%|███████▎  | 399/547 [07:18<02:42,  0.91it/s, v_num=3, train/loss_step=0.327]\n",
      "Epoch 0:  73%|███████▎  | 400/547 [07:18<02:41,  0.91it/s, v_num=3, train/loss_step=0.327]\n",
      "Epoch 0:  73%|███████▎  | 400/547 [07:19<02:41,  0.91it/s, v_num=3, train/loss_step=0.127]\n",
      "Epoch 0:  73%|███████▎  | 401/547 [07:19<02:39,  0.91it/s, v_num=3, train/loss_step=0.127]\n",
      "Epoch 0:  73%|███████▎  | 401/547 [07:20<02:40,  0.91it/s, v_num=3, train/loss_step=0.0991]\n",
      "Epoch 0:  73%|███████▎  | 402/547 [07:20<02:38,  0.91it/s, v_num=3, train/loss_step=0.0991]\n",
      "Epoch 0:  73%|███████▎  | 402/547 [07:21<02:39,  0.91it/s, v_num=3, train/loss_step=0.158]\n",
      "Epoch 0:  74%|███████▎  | 403/547 [07:21<02:37,  0.91it/s, v_num=3, train/loss_step=0.158]\n",
      "Epoch 0:  74%|███████▎  | 403/547 [07:22<02:38,  0.91it/s, v_num=3, train/loss_step=0.0993]\n",
      "Epoch 0:  74%|███████▍  | 404/547 [07:22<02:36,  0.91it/s, v_num=3, train/loss_step=0.0993]\n",
      "Epoch 0:  74%|███████▍  | 404/547 [07:23<02:37,  0.91it/s, v_num=3, train/loss_step=0.227]\n",
      "Epoch 0:  74%|███████▍  | 405/547 [07:23<02:35,  0.91it/s, v_num=3, train/loss_step=0.227]\n",
      "Epoch 0:  74%|███████▍  | 405/547 [07:24<02:35,  0.91it/s, v_num=3, train/loss_step=0.0603]\n",
      "Epoch 0:  74%|███████▍  | 406/547 [07:24<02:34,  0.91it/s, v_num=3, train/loss_step=0.0603]\n",
      "Epoch 0:  74%|███████▍  | 406/547 [07:25<02:34,  0.91it/s, v_num=3, train/loss_step=0.0842]\n",
      "Epoch 0:  74%|███████▍  | 407/547 [07:26<02:33,  0.91it/s, v_num=3, train/loss_step=0.0842]\n",
      "Epoch 0:  74%|███████▍  | 407/547 [07:26<02:33,  0.91it/s, v_num=3, train/loss_step=0.181]\n",
      "Epoch 0:  75%|███████▍  | 408/547 [07:27<02:32,  0.91it/s, v_num=3, train/loss_step=0.181]\n",
      "Epoch 0:  75%|███████▍  | 408/547 [07:28<02:32,  0.91it/s, v_num=3, train/loss_step=0.0806]\n",
      "Epoch 0:  75%|███████▍  | 409/547 [07:28<02:31,  0.91it/s, v_num=3, train/loss_step=0.0806]\n",
      "Epoch 0:  75%|███████▍  | 409/547 [07:29<02:31,  0.91it/s, v_num=3, train/loss_step=0.192]\n",
      "Epoch 0:  75%|███████▍  | 410/547 [07:29<02:30,  0.91it/s, v_num=3, train/loss_step=0.192]\n",
      "Epoch 0:  75%|███████▍  | 410/547 [07:30<02:30,  0.91it/s, v_num=3, train/loss_step=0.0988]\n",
      "Epoch 0:  75%|███████▌  | 411/547 [07:30<02:29,  0.91it/s, v_num=3, train/loss_step=0.0988]\n",
      "Epoch 0:  75%|███████▌  | 411/547 [07:31<02:29,  0.91it/s, v_num=3, train/loss_step=0.319]\n",
      "Epoch 0:  75%|███████▌  | 412/547 [07:31<02:27,  0.91it/s, v_num=3, train/loss_step=0.319]\n",
      "Epoch 0:  75%|███████▌  | 412/547 [07:32<02:28,  0.91it/s, v_num=3, train/loss_step=0.175]\n",
      "Epoch 0:  76%|███████▌  | 413/547 [07:32<02:26,  0.91it/s, v_num=3, train/loss_step=0.175]\n",
      "Epoch 0:  76%|███████▌  | 413/547 [07:33<02:27,  0.91it/s, v_num=3, train/loss_step=0.125]\n",
      "Epoch 0:  76%|███████▌  | 414/547 [07:33<02:25,  0.91it/s, v_num=3, train/loss_step=0.125]\n",
      "Epoch 0:  76%|███████▌  | 414/547 [07:34<02:26,  0.91it/s, v_num=3, train/loss_step=0.186]\n",
      "Epoch 0:  76%|███████▌  | 415/547 [07:34<02:24,  0.91it/s, v_num=3, train/loss_step=0.186]\n",
      "Epoch 0:  76%|███████▌  | 415/547 [07:35<02:24,  0.91it/s, v_num=3, train/loss_step=0.275]\n",
      "Epoch 0:  76%|███████▌  | 416/547 [07:36<02:23,  0.91it/s, v_num=3, train/loss_step=0.275]\n",
      "Epoch 0:  76%|███████▌  | 416/547 [07:36<02:23,  0.91it/s, v_num=3, train/loss_step=0.117]\n",
      "Epoch 0:  76%|███████▌  | 417/547 [07:37<02:22,  0.91it/s, v_num=3, train/loss_step=0.117]\n",
      "Epoch 0:  76%|███████▌  | 417/547 [07:38<02:22,  0.91it/s, v_num=3, train/loss_step=0.340]\n",
      "Epoch 0:  76%|███████▋  | 418/547 [07:38<02:21,  0.91it/s, v_num=3, train/loss_step=0.340]\n",
      "Epoch 0:  76%|███████▋  | 418/547 [07:39<02:21,  0.91it/s, v_num=3, train/loss_step=0.239]\n",
      "Epoch 0:  77%|███████▋  | 419/547 [07:39<02:20,  0.91it/s, v_num=3, train/loss_step=0.239]\n",
      "Epoch 0:  77%|███████▋  | 419/547 [07:40<02:20,  0.91it/s, v_num=3, train/loss_step=0.147]\n",
      "Epoch 0:  77%|███████▋  | 420/547 [07:40<02:19,  0.91it/s, v_num=3, train/loss_step=0.147]\n",
      "Epoch 0:  77%|███████▋  | 420/547 [07:41<02:19,  0.91it/s, v_num=3, train/loss_step=0.136]\n",
      "Epoch 0:  77%|███████▋  | 421/547 [07:41<02:18,  0.91it/s, v_num=3, train/loss_step=0.136]\n",
      "Epoch 0:  77%|███████▋  | 421/547 [07:42<02:18,  0.91it/s, v_num=3, train/loss_step=0.489]\n",
      "Epoch 0:  77%|███████▋  | 422/547 [07:42<02:17,  0.91it/s, v_num=3, train/loss_step=0.489]\n",
      "Epoch 0:  77%|███████▋  | 422/547 [07:43<02:17,  0.91it/s, v_num=3, train/loss_step=0.219]\n",
      "Epoch 0:  77%|███████▋  | 423/547 [07:43<02:15,  0.91it/s, v_num=3, train/loss_step=0.219]\n",
      "Epoch 0:  77%|███████▋  | 423/547 [07:44<02:16,  0.91it/s, v_num=3, train/loss_step=0.324]\n",
      "Epoch 0:  78%|███████▊  | 424/547 [07:44<02:14,  0.91it/s, v_num=3, train/loss_step=0.324]\n",
      "Epoch 0:  78%|███████▊  | 424/547 [07:45<02:15,  0.91it/s, v_num=3, train/loss_step=0.332]\n",
      "Epoch 0:  78%|███████▊  | 425/547 [07:46<02:13,  0.91it/s, v_num=3, train/loss_step=0.332]\n",
      "Epoch 0:  78%|███████▊  | 425/547 [07:46<02:14,  0.91it/s, v_num=3, train/loss_step=0.320]\n",
      "Epoch 0:  78%|███████▊  | 426/547 [07:47<02:12,  0.91it/s, v_num=3, train/loss_step=0.320]\n",
      "Epoch 0:  78%|███████▊  | 426/547 [07:48<02:12,  0.91it/s, v_num=3, train/loss_step=0.126]\n",
      "Epoch 0:  78%|███████▊  | 427/547 [07:48<02:11,  0.91it/s, v_num=3, train/loss_step=0.126]\n",
      "Epoch 0:  78%|███████▊  | 427/547 [07:49<02:11,  0.91it/s, v_num=3, train/loss_step=0.129]\n",
      "Epoch 0:  78%|███████▊  | 428/547 [07:49<02:10,  0.91it/s, v_num=3, train/loss_step=0.129]\n",
      "Epoch 0:  78%|███████▊  | 428/547 [07:50<02:10,  0.91it/s, v_num=3, train/loss_step=0.410]\n",
      "Epoch 0:  78%|███████▊  | 429/547 [07:50<02:09,  0.91it/s, v_num=3, train/loss_step=0.410]\n",
      "Epoch 0:  78%|███████▊  | 429/547 [07:51<02:09,  0.91it/s, v_num=3, train/loss_step=0.0972]\n",
      "Epoch 0:  79%|███████▊  | 430/547 [07:51<02:08,  0.91it/s, v_num=3, train/loss_step=0.0972]\n",
      "Epoch 0:  79%|███████▊  | 430/547 [07:52<02:08,  0.91it/s, v_num=3, train/loss_step=0.148]\n",
      "Epoch 0:  79%|███████▉  | 431/547 [07:52<02:07,  0.91it/s, v_num=3, train/loss_step=0.148]\n",
      "Epoch 0:  79%|███████▉  | 431/547 [07:53<02:07,  0.91it/s, v_num=3, train/loss_step=0.148]\n",
      "Epoch 0:  79%|███████▉  | 432/547 [07:53<02:06,  0.91it/s, v_num=3, train/loss_step=0.148]\n",
      "Epoch 0:  79%|███████▉  | 432/547 [07:54<02:06,  0.91it/s, v_num=3, train/loss_step=0.243]\n",
      "Epoch 0:  79%|███████▉  | 433/547 [07:54<02:05,  0.91it/s, v_num=3, train/loss_step=0.243]\n",
      "Epoch 0:  79%|███████▉  | 433/547 [07:55<02:05,  0.91it/s, v_num=3, train/loss_step=0.0807]\n",
      "Epoch 0:  79%|███████▉  | 434/547 [07:56<02:03,  0.91it/s, v_num=3, train/loss_step=0.0807]\n",
      "Epoch 0:  79%|███████▉  | 434/547 [07:56<02:04,  0.91it/s, v_num=3, train/loss_step=0.403]\n",
      "Epoch 0:  80%|███████▉  | 435/547 [07:57<02:02,  0.91it/s, v_num=3, train/loss_step=0.403]\n",
      "Epoch 0:  80%|███████▉  | 435/547 [07:58<02:03,  0.91it/s, v_num=3, train/loss_step=0.316]\n",
      "Epoch 0:  80%|███████▉  | 436/547 [07:58<02:01,  0.91it/s, v_num=3, train/loss_step=0.316]\n",
      "Epoch 0:  80%|███████▉  | 436/547 [07:59<02:01,  0.91it/s, v_num=3, train/loss_step=0.449]\n",
      "Epoch 0:  80%|███████▉  | 437/547 [07:59<02:00,  0.91it/s, v_num=3, train/loss_step=0.449]\n",
      "Epoch 0:  80%|███████▉  | 437/547 [08:00<02:00,  0.91it/s, v_num=3, train/loss_step=0.206]\n",
      "Epoch 0:  80%|████████  | 438/547 [08:00<01:59,  0.91it/s, v_num=3, train/loss_step=0.206]\n",
      "Epoch 0:  80%|████████  | 438/547 [08:01<01:59,  0.91it/s, v_num=3, train/loss_step=0.197]\n",
      "Epoch 0:  80%|████████  | 439/547 [08:01<01:58,  0.91it/s, v_num=3, train/loss_step=0.197]\n",
      "Epoch 0:  80%|████████  | 439/547 [08:02<01:58,  0.91it/s, v_num=3, train/loss_step=0.0542]\n",
      "Epoch 0:  80%|████████  | 440/547 [08:02<01:57,  0.91it/s, v_num=3, train/loss_step=0.0542]\n",
      "Epoch 0:  80%|████████  | 440/547 [08:03<01:57,  0.91it/s, v_num=3, train/loss_step=0.188]\n",
      "Epoch 0:  81%|████████  | 441/547 [08:03<01:56,  0.91it/s, v_num=3, train/loss_step=0.188]\n",
      "Epoch 0:  81%|████████  | 441/547 [08:04<01:56,  0.91it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  81%|████████  | 442/547 [08:04<01:55,  0.91it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  81%|████████  | 442/547 [08:05<01:55,  0.91it/s, v_num=3, train/loss_step=0.308]\n",
      "Epoch 0:  81%|████████  | 443/547 [08:06<01:54,  0.91it/s, v_num=3, train/loss_step=0.308]\n",
      "Epoch 0:  81%|████████  | 443/547 [08:06<01:54,  0.91it/s, v_num=3, train/loss_step=0.331]\n",
      "Epoch 0:  81%|████████  | 444/547 [08:07<01:53,  0.91it/s, v_num=3, train/loss_step=0.331]\n",
      "Epoch 0:  81%|████████  | 444/547 [08:08<01:53,  0.91it/s, v_num=3, train/loss_step=0.134]\n",
      "Epoch 0:  81%|████████▏ | 445/547 [08:08<01:51,  0.91it/s, v_num=3, train/loss_step=0.134]\n",
      "Epoch 0:  81%|████████▏ | 445/547 [08:09<01:52,  0.91it/s, v_num=3, train/loss_step=0.233]\n",
      "Epoch 0:  82%|████████▏ | 446/547 [08:09<01:50,  0.91it/s, v_num=3, train/loss_step=0.233]\n",
      "Epoch 0:  82%|████████▏ | 446/547 [08:10<01:51,  0.91it/s, v_num=3, train/loss_step=0.159]\n",
      "Epoch 0:  82%|████████▏ | 447/547 [08:10<01:49,  0.91it/s, v_num=3, train/loss_step=0.159]\n",
      "Epoch 0:  82%|████████▏ | 447/547 [08:11<01:49,  0.91it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  82%|████████▏ | 448/547 [08:11<01:48,  0.91it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  82%|████████▏ | 448/547 [08:12<01:48,  0.91it/s, v_num=3, train/loss_step=0.241]\n",
      "Epoch 0:  82%|████████▏ | 449/547 [08:12<01:47,  0.91it/s, v_num=3, train/loss_step=0.241]\n",
      "Epoch 0:  82%|████████▏ | 449/547 [08:13<01:47,  0.91it/s, v_num=3, train/loss_step=0.276]\n",
      "Epoch 0:  82%|████████▏ | 450/547 [08:13<01:46,  0.91it/s, v_num=3, train/loss_step=0.276]\n",
      "Epoch 0:  82%|████████▏ | 450/547 [08:14<01:46,  0.91it/s, v_num=3, train/loss_step=0.114]\n",
      "Epoch 0:  82%|████████▏ | 451/547 [08:14<01:45,  0.91it/s, v_num=3, train/loss_step=0.114]\n",
      "Epoch 0:  82%|████████▏ | 451/547 [08:15<01:45,  0.91it/s, v_num=3, train/loss_step=0.253]\n",
      "Epoch 0:  83%|████████▎ | 452/547 [08:16<01:44,  0.91it/s, v_num=3, train/loss_step=0.253]\n",
      "Epoch 0:  83%|████████▎ | 452/547 [08:16<01:44,  0.91it/s, v_num=3, train/loss_step=0.308]\n",
      "Epoch 0:  83%|████████▎ | 453/547 [08:17<01:43,  0.91it/s, v_num=3, train/loss_step=0.308]\n",
      "Epoch 0:  83%|████████▎ | 453/547 [08:18<01:43,  0.91it/s, v_num=3, train/loss_step=0.163]\n",
      "Epoch 0:  83%|████████▎ | 454/547 [08:18<01:42,  0.91it/s, v_num=3, train/loss_step=0.163]\n",
      "Epoch 0:  83%|████████▎ | 454/547 [08:19<01:42,  0.91it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  83%|████████▎ | 455/547 [08:19<01:40,  0.91it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  83%|████████▎ | 455/547 [08:20<01:41,  0.91it/s, v_num=3, train/loss_step=0.255]\n",
      "Epoch 0:  83%|████████▎ | 456/547 [08:20<01:39,  0.91it/s, v_num=3, train/loss_step=0.255]\n",
      "Epoch 0:  83%|████████▎ | 456/547 [08:21<01:40,  0.91it/s, v_num=3, train/loss_step=0.419]\n",
      "Epoch 0:  84%|████████▎ | 457/547 [08:21<01:38,  0.91it/s, v_num=3, train/loss_step=0.419]\n",
      "Epoch 0:  84%|████████▎ | 457/547 [08:22<01:38,  0.91it/s, v_num=3, train/loss_step=0.178]\n",
      "Epoch 0:  84%|████████▎ | 458/547 [08:22<01:37,  0.91it/s, v_num=3, train/loss_step=0.178]\n",
      "Epoch 0:  84%|████████▎ | 458/547 [08:23<01:37,  0.91it/s, v_num=3, train/loss_step=0.0926]\n",
      "Epoch 0:  84%|████████▍ | 459/547 [08:23<01:36,  0.91it/s, v_num=3, train/loss_step=0.0926]\n",
      "Epoch 0:  84%|████████▍ | 459/547 [08:24<01:36,  0.91it/s, v_num=3, train/loss_step=0.184]\n",
      "Epoch 0:  84%|████████▍ | 460/547 [08:24<01:35,  0.91it/s, v_num=3, train/loss_step=0.184]\n",
      "Epoch 0:  84%|████████▍ | 460/547 [08:25<01:35,  0.91it/s, v_num=3, train/loss_step=0.450]\n",
      "Epoch 0:  84%|████████▍ | 461/547 [08:26<01:34,  0.91it/s, v_num=3, train/loss_step=0.450]\n",
      "Epoch 0:  84%|████████▍ | 461/547 [08:26<01:34,  0.91it/s, v_num=3, train/loss_step=0.202]\n",
      "Epoch 0:  84%|████████▍ | 462/547 [08:27<01:33,  0.91it/s, v_num=3, train/loss_step=0.202]\n",
      "Epoch 0:  84%|████████▍ | 462/547 [08:27<01:33,  0.91it/s, v_num=3, train/loss_step=0.484]\n",
      "Epoch 0:  85%|████████▍ | 463/547 [08:28<01:32,  0.91it/s, v_num=3, train/loss_step=0.484]\n",
      "Epoch 0:  85%|████████▍ | 463/547 [08:29<01:32,  0.91it/s, v_num=3, train/loss_step=0.330]\n",
      "Epoch 0:  85%|████████▍ | 464/547 [08:29<01:31,  0.91it/s, v_num=3, train/loss_step=0.330]\n",
      "Epoch 0:  85%|████████▍ | 464/547 [08:30<01:31,  0.91it/s, v_num=3, train/loss_step=0.0727]\n",
      "Epoch 0:  85%|████████▌ | 465/547 [08:30<01:30,  0.91it/s, v_num=3, train/loss_step=0.0727]\n",
      "Epoch 0:  85%|████████▌ | 465/547 [08:31<01:30,  0.91it/s, v_num=3, train/loss_step=0.151]\n",
      "Epoch 0:  85%|████████▌ | 466/547 [08:31<01:28,  0.91it/s, v_num=3, train/loss_step=0.151]\n",
      "Epoch 0:  85%|████████▌ | 466/547 [08:32<01:29,  0.91it/s, v_num=3, train/loss_step=0.0923]\n",
      "Epoch 0:  85%|████████▌ | 467/547 [08:32<01:27,  0.91it/s, v_num=3, train/loss_step=0.0923]\n",
      "Epoch 0:  85%|████████▌ | 467/547 [08:33<01:27,  0.91it/s, v_num=3, train/loss_step=0.0471]\n",
      "Epoch 0:  86%|████████▌ | 468/547 [08:33<01:26,  0.91it/s, v_num=3, train/loss_step=0.0471]\n",
      "Epoch 0:  86%|████████▌ | 468/547 [08:34<01:26,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  86%|████████▌ | 469/547 [08:34<01:25,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  86%|████████▌ | 469/547 [08:35<01:25,  0.91it/s, v_num=3, train/loss_step=0.0703]\n",
      "Epoch 0:  86%|████████▌ | 470/547 [08:36<01:24,  0.91it/s, v_num=3, train/loss_step=0.0703]\n",
      "Epoch 0:  86%|████████▌ | 470/547 [08:36<01:24,  0.91it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  86%|████████▌ | 471/547 [08:37<01:23,  0.91it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  86%|████████▌ | 471/547 [08:37<01:23,  0.91it/s, v_num=3, train/loss_step=0.548]\n",
      "Epoch 0:  86%|████████▋ | 472/547 [08:38<01:22,  0.91it/s, v_num=3, train/loss_step=0.548]\n",
      "Epoch 0:  86%|████████▋ | 472/547 [08:39<01:22,  0.91it/s, v_num=3, train/loss_step=0.190]\n",
      "Epoch 0:  86%|████████▋ | 473/547 [08:39<01:21,  0.91it/s, v_num=3, train/loss_step=0.190]\n",
      "Epoch 0:  86%|████████▋ | 473/547 [08:40<01:21,  0.91it/s, v_num=3, train/loss_step=0.056]\n",
      "Epoch 0:  87%|████████▋ | 474/547 [08:40<01:20,  0.91it/s, v_num=3, train/loss_step=0.056]\n",
      "Epoch 0:  87%|████████▋ | 474/547 [08:41<01:20,  0.91it/s, v_num=3, train/loss_step=0.335]\n",
      "Epoch 0:  87%|████████▋ | 475/547 [08:41<01:19,  0.91it/s, v_num=3, train/loss_step=0.335]\n",
      "Epoch 0:  87%|████████▋ | 475/547 [08:42<01:19,  0.91it/s, v_num=3, train/loss_step=0.111]\n",
      "Epoch 0:  87%|████████▋ | 476/547 [08:42<01:17,  0.91it/s, v_num=3, train/loss_step=0.111]\n",
      "Epoch 0:  87%|████████▋ | 476/547 [08:43<01:18,  0.91it/s, v_num=3, train/loss_step=0.300]\n",
      "Epoch 0:  87%|████████▋ | 477/547 [08:43<01:16,  0.91it/s, v_num=3, train/loss_step=0.300]\n",
      "Epoch 0:  87%|████████▋ | 477/547 [08:44<01:16,  0.91it/s, v_num=3, train/loss_step=0.160]\n",
      "Epoch 0:  87%|████████▋ | 478/547 [08:44<01:15,  0.91it/s, v_num=3, train/loss_step=0.160]\n",
      "Epoch 0:  87%|████████▋ | 478/547 [08:45<01:15,  0.91it/s, v_num=3, train/loss_step=0.128]\n",
      "Epoch 0:  88%|████████▊ | 479/547 [08:46<01:14,  0.91it/s, v_num=3, train/loss_step=0.128]\n",
      "Epoch 0:  88%|████████▊ | 479/547 [08:46<01:14,  0.91it/s, v_num=3, train/loss_step=0.103]\n",
      "Epoch 0:  88%|████████▊ | 480/547 [08:47<01:13,  0.91it/s, v_num=3, train/loss_step=0.103]\n",
      "Epoch 0:  88%|████████▊ | 480/547 [08:48<01:13,  0.91it/s, v_num=3, train/loss_step=0.306]\n",
      "Epoch 0:  88%|████████▊ | 481/547 [08:48<01:12,  0.91it/s, v_num=3, train/loss_step=0.306]\n",
      "Epoch 0:  88%|████████▊ | 481/547 [08:49<01:12,  0.91it/s, v_num=3, train/loss_step=0.286]\n",
      "Epoch 0:  88%|████████▊ | 482/547 [08:49<01:11,  0.91it/s, v_num=3, train/loss_step=0.286]\n",
      "Epoch 0:  88%|████████▊ | 482/547 [08:50<01:11,  0.91it/s, v_num=3, train/loss_step=0.335]\n",
      "Epoch 0:  88%|████████▊ | 483/547 [08:50<01:10,  0.91it/s, v_num=3, train/loss_step=0.335]\n",
      "Epoch 0:  88%|████████▊ | 483/547 [08:51<01:10,  0.91it/s, v_num=3, train/loss_step=0.156]\n",
      "Epoch 0:  88%|████████▊ | 484/547 [08:51<01:09,  0.91it/s, v_num=3, train/loss_step=0.156]\n",
      "Epoch 0:  88%|████████▊ | 484/547 [08:52<01:09,  0.91it/s, v_num=3, train/loss_step=0.348]\n",
      "Epoch 0:  89%|████████▊ | 485/547 [08:52<01:08,  0.91it/s, v_num=3, train/loss_step=0.348]\n",
      "Epoch 0:  89%|████████▊ | 485/547 [08:53<01:08,  0.91it/s, v_num=3, train/loss_step=0.474]\n",
      "Epoch 0:  89%|████████▉ | 486/547 [08:53<01:07,  0.91it/s, v_num=3, train/loss_step=0.474]\n",
      "Epoch 0:  89%|████████▉ | 486/547 [08:54<01:07,  0.91it/s, v_num=3, train/loss_step=0.181]\n",
      "Epoch 0:  89%|████████▉ | 487/547 [08:54<01:05,  0.91it/s, v_num=3, train/loss_step=0.181]\n",
      "Epoch 0:  89%|████████▉ | 487/547 [08:55<01:06,  0.91it/s, v_num=3, train/loss_step=0.365]\n",
      "Epoch 0:  89%|████████▉ | 488/547 [08:56<01:04,  0.91it/s, v_num=3, train/loss_step=0.365]\n",
      "Epoch 0:  89%|████████▉ | 488/547 [08:56<01:04,  0.91it/s, v_num=3, train/loss_step=0.200]\n",
      "Epoch 0:  89%|████████▉ | 489/547 [08:57<01:03,  0.91it/s, v_num=3, train/loss_step=0.200]\n",
      "Epoch 0:  89%|████████▉ | 489/547 [08:57<01:03,  0.91it/s, v_num=3, train/loss_step=0.233]\n",
      "Epoch 0:  90%|████████▉ | 490/547 [08:58<01:02,  0.91it/s, v_num=3, train/loss_step=0.233]\n",
      "Epoch 0:  90%|████████▉ | 490/547 [08:59<01:02,  0.91it/s, v_num=3, train/loss_step=0.278]\n",
      "Epoch 0:  90%|████████▉ | 491/547 [08:59<01:01,  0.91it/s, v_num=3, train/loss_step=0.278]\n",
      "Epoch 0:  90%|████████▉ | 491/547 [09:00<01:01,  0.91it/s, v_num=3, train/loss_step=0.113]\n",
      "Epoch 0:  90%|████████▉ | 492/547 [09:00<01:00,  0.91it/s, v_num=3, train/loss_step=0.113]\n",
      "Epoch 0:  90%|████████▉ | 492/547 [09:01<01:00,  0.91it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  90%|█████████ | 493/547 [09:01<00:59,  0.91it/s, v_num=3, train/loss_step=0.106]\n",
      "Epoch 0:  90%|█████████ | 493/547 [09:02<00:59,  0.91it/s, v_num=3, train/loss_step=0.142]\n",
      "Epoch 0:  90%|█████████ | 494/547 [09:02<00:58,  0.91it/s, v_num=3, train/loss_step=0.142]\n",
      "Epoch 0:  90%|█████████ | 494/547 [09:03<00:58,  0.91it/s, v_num=3, train/loss_step=0.236]\n",
      "Epoch 0:  90%|█████████ | 495/547 [09:03<00:57,  0.91it/s, v_num=3, train/loss_step=0.236]\n",
      "Epoch 0:  90%|█████████ | 495/547 [09:04<00:57,  0.91it/s, v_num=3, train/loss_step=0.133]\n",
      "Epoch 0:  91%|█████████ | 496/547 [09:04<00:56,  0.91it/s, v_num=3, train/loss_step=0.133]\n",
      "Epoch 0:  91%|█████████ | 496/547 [09:05<00:56,  0.91it/s, v_num=3, train/loss_step=0.0776]\n",
      "Epoch 0:  91%|█████████ | 497/547 [09:06<00:54,  0.91it/s, v_num=3, train/loss_step=0.0776]\n",
      "Epoch 0:  91%|█████████ | 497/547 [09:06<00:55,  0.91it/s, v_num=3, train/loss_step=0.248]\n",
      "Epoch 0:  91%|█████████ | 498/547 [09:07<00:53,  0.91it/s, v_num=3, train/loss_step=0.248]\n",
      "Epoch 0:  91%|█████████ | 498/547 [09:07<00:53,  0.91it/s, v_num=3, train/loss_step=0.0739]\n",
      "Epoch 0:  91%|█████████ | 499/547 [09:08<00:52,  0.91it/s, v_num=3, train/loss_step=0.0739]\n",
      "Epoch 0:  91%|█████████ | 499/547 [09:09<00:52,  0.91it/s, v_num=3, train/loss_step=0.0941]\n",
      "Epoch 0:  91%|█████████▏| 500/547 [09:09<00:51,  0.91it/s, v_num=3, train/loss_step=0.0941]\n",
      "Epoch 0:  91%|█████████▏| 500/547 [09:10<00:51,  0.91it/s, v_num=3, train/loss_step=0.0824]\n",
      "Epoch 0:  92%|█████████▏| 501/547 [09:10<00:50,  0.91it/s, v_num=3, train/loss_step=0.0824]\n",
      "Epoch 0:  92%|█████████▏| 501/547 [09:11<00:50,  0.91it/s, v_num=3, train/loss_step=0.0678]\n",
      "Epoch 0:  92%|█████████▏| 502/547 [09:11<00:49,  0.91it/s, v_num=3, train/loss_step=0.0678]\n",
      "Epoch 0:  92%|█████████▏| 502/547 [09:12<00:49,  0.91it/s, v_num=3, train/loss_step=0.326]\n",
      "Epoch 0:  92%|█████████▏| 503/547 [09:12<00:48,  0.91it/s, v_num=3, train/loss_step=0.326]\n",
      "Epoch 0:  92%|█████████▏| 503/547 [09:13<00:48,  0.91it/s, v_num=3, train/loss_step=0.166]\n",
      "Epoch 0:  92%|█████████▏| 504/547 [09:13<00:47,  0.91it/s, v_num=3, train/loss_step=0.166]\n",
      "Epoch 0:  92%|█████████▏| 504/547 [09:14<00:47,  0.91it/s, v_num=3, train/loss_step=0.115]\n",
      "Epoch 0:  92%|█████████▏| 505/547 [09:14<00:46,  0.91it/s, v_num=3, train/loss_step=0.115]\n",
      "Epoch 0:  92%|█████████▏| 505/547 [09:15<00:46,  0.91it/s, v_num=3, train/loss_step=0.0817]\n",
      "Epoch 0:  93%|█████████▎| 506/547 [09:16<00:45,  0.91it/s, v_num=3, train/loss_step=0.0817]\n",
      "Epoch 0:  93%|█████████▎| 506/547 [09:16<00:45,  0.91it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  93%|█████████▎| 507/547 [09:17<00:43,  0.91it/s, v_num=3, train/loss_step=0.237]\n",
      "Epoch 0:  93%|█████████▎| 507/547 [09:18<00:44,  0.91it/s, v_num=3, train/loss_step=0.242]\n",
      "Epoch 0:  93%|█████████▎| 508/547 [09:18<00:42,  0.91it/s, v_num=3, train/loss_step=0.242]\n",
      "Epoch 0:  93%|█████████▎| 508/547 [09:19<00:42,  0.91it/s, v_num=3, train/loss_step=0.465]\n",
      "Epoch 0:  93%|█████████▎| 509/547 [09:19<00:41,  0.91it/s, v_num=3, train/loss_step=0.465]\n",
      "Epoch 0:  93%|█████████▎| 509/547 [09:20<00:41,  0.91it/s, v_num=3, train/loss_step=0.208]\n",
      "Epoch 0:  93%|█████████▎| 510/547 [09:20<00:40,  0.91it/s, v_num=3, train/loss_step=0.208]\n",
      "Epoch 0:  93%|█████████▎| 510/547 [09:21<00:40,  0.91it/s, v_num=3, train/loss_step=0.0893]\n",
      "Epoch 0:  93%|█████████▎| 511/547 [09:21<00:39,  0.91it/s, v_num=3, train/loss_step=0.0893]\n",
      "Epoch 0:  93%|█████████▎| 511/547 [09:22<00:39,  0.91it/s, v_num=3, train/loss_step=0.0607]\n",
      "Epoch 0:  94%|█████████▎| 512/547 [09:22<00:38,  0.91it/s, v_num=3, train/loss_step=0.0607]\n",
      "Epoch 0:  94%|█████████▎| 512/547 [09:23<00:38,  0.91it/s, v_num=3, train/loss_step=0.194]\n",
      "Epoch 0:  94%|█████████▍| 513/547 [09:23<00:37,  0.91it/s, v_num=3, train/loss_step=0.194]\n",
      "Epoch 0:  94%|█████████▍| 513/547 [09:24<00:37,  0.91it/s, v_num=3, train/loss_step=0.122]\n",
      "Epoch 0:  94%|█████████▍| 514/547 [09:24<00:36,  0.91it/s, v_num=3, train/loss_step=0.122]\n",
      "Epoch 0:  94%|█████████▍| 514/547 [09:25<00:36,  0.91it/s, v_num=3, train/loss_step=0.145]\n",
      "Epoch 0:  94%|█████████▍| 515/547 [09:26<00:35,  0.91it/s, v_num=3, train/loss_step=0.145]\n",
      "Epoch 0:  94%|█████████▍| 515/547 [09:26<00:35,  0.91it/s, v_num=3, train/loss_step=0.121]\n",
      "Epoch 0:  94%|█████████▍| 516/547 [09:27<00:34,  0.91it/s, v_num=3, train/loss_step=0.121]\n",
      "Epoch 0:  94%|█████████▍| 516/547 [09:27<00:34,  0.91it/s, v_num=3, train/loss_step=0.0305]\n",
      "Epoch 0:  95%|█████████▍| 517/547 [09:28<00:32,  0.91it/s, v_num=3, train/loss_step=0.0305]\n",
      "Epoch 0:  95%|█████████▍| 517/547 [09:29<00:33,  0.91it/s, v_num=3, train/loss_step=0.124]\n",
      "Epoch 0:  95%|█████████▍| 518/547 [09:29<00:31,  0.91it/s, v_num=3, train/loss_step=0.124]\n",
      "Epoch 0:  95%|█████████▍| 518/547 [09:30<00:31,  0.91it/s, v_num=3, train/loss_step=0.276]\n",
      "Epoch 0:  95%|█████████▍| 519/547 [09:30<00:30,  0.91it/s, v_num=3, train/loss_step=0.276]\n",
      "Epoch 0:  95%|█████████▍| 519/547 [09:31<00:30,  0.91it/s, v_num=3, train/loss_step=0.0663]\n",
      "Epoch 0:  95%|█████████▌| 520/547 [09:31<00:29,  0.91it/s, v_num=3, train/loss_step=0.0663]\n",
      "Epoch 0:  95%|█████████▌| 520/547 [09:32<00:29,  0.91it/s, v_num=3, train/loss_step=0.263]\n",
      "Epoch 0:  95%|█████████▌| 521/547 [09:32<00:28,  0.91it/s, v_num=3, train/loss_step=0.263]\n",
      "Epoch 0:  95%|█████████▌| 521/547 [09:33<00:28,  0.91it/s, v_num=3, train/loss_step=0.217]\n",
      "Epoch 0:  95%|█████████▌| 522/547 [09:33<00:27,  0.91it/s, v_num=3, train/loss_step=0.217]\n",
      "Epoch 0:  95%|█████████▌| 522/547 [09:34<00:27,  0.91it/s, v_num=3, train/loss_step=0.182]\n",
      "Epoch 0:  96%|█████████▌| 523/547 [09:34<00:26,  0.91it/s, v_num=3, train/loss_step=0.182]\n",
      "Epoch 0:  96%|█████████▌| 523/547 [09:35<00:26,  0.91it/s, v_num=3, train/loss_step=0.0683]\n",
      "Epoch 0:  96%|█████████▌| 524/547 [09:36<00:25,  0.91it/s, v_num=3, train/loss_step=0.0683]\n",
      "Epoch 0:  96%|█████████▌| 524/547 [09:36<00:25,  0.91it/s, v_num=3, train/loss_step=0.189]\n",
      "Epoch 0:  96%|█████████▌| 525/547 [09:37<00:24,  0.91it/s, v_num=3, train/loss_step=0.189]\n",
      "Epoch 0:  96%|█████████▌| 525/547 [09:38<00:24,  0.91it/s, v_num=3, train/loss_step=0.136]\n",
      "Epoch 0:  96%|█████████▌| 526/547 [09:38<00:23,  0.91it/s, v_num=3, train/loss_step=0.136]\n",
      "Epoch 0:  96%|█████████▌| 526/547 [09:39<00:23,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  96%|█████████▋| 527/547 [09:39<00:21,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  96%|█████████▋| 527/547 [09:40<00:22,  0.91it/s, v_num=3, train/loss_step=0.194]\n",
      "Epoch 0:  97%|█████████▋| 528/547 [09:40<00:20,  0.91it/s, v_num=3, train/loss_step=0.194]\n",
      "Epoch 0:  97%|█████████▋| 528/547 [09:41<00:20,  0.91it/s, v_num=3, train/loss_step=0.115]\n",
      "Epoch 0:  97%|█████████▋| 529/547 [09:41<00:19,  0.91it/s, v_num=3, train/loss_step=0.115]\n",
      "Epoch 0:  97%|█████████▋| 529/547 [09:42<00:19,  0.91it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  97%|█████████▋| 530/547 [09:42<00:18,  0.91it/s, v_num=3, train/loss_step=0.118]\n",
      "Epoch 0:  97%|█████████▋| 530/547 [09:43<00:18,  0.91it/s, v_num=3, train/loss_step=0.258]\n",
      "Epoch 0:  97%|█████████▋| 531/547 [09:43<00:17,  0.91it/s, v_num=3, train/loss_step=0.258]\n",
      "Epoch 0:  97%|█████████▋| 531/547 [09:44<00:17,  0.91it/s, v_num=3, train/loss_step=0.205]\n",
      "Epoch 0:  97%|█████████▋| 532/547 [09:44<00:16,  0.91it/s, v_num=3, train/loss_step=0.205]\n",
      "Epoch 0:  97%|█████████▋| 532/547 [09:45<00:16,  0.91it/s, v_num=3, train/loss_step=0.0656]\n",
      "Epoch 0:  97%|█████████▋| 533/547 [09:46<00:15,  0.91it/s, v_num=3, train/loss_step=0.0656]\n",
      "Epoch 0:  97%|█████████▋| 533/547 [09:46<00:15,  0.91it/s, v_num=3, train/loss_step=0.134]\n",
      "Epoch 0:  98%|█████████▊| 534/547 [09:47<00:14,  0.91it/s, v_num=3, train/loss_step=0.134]\n",
      "Epoch 0:  98%|█████████▊| 534/547 [09:48<00:14,  0.91it/s, v_num=3, train/loss_step=0.0739]\n",
      "Epoch 0:  98%|█████████▊| 535/547 [09:48<00:13,  0.91it/s, v_num=3, train/loss_step=0.0739]\n",
      "Epoch 0:  98%|█████████▊| 535/547 [09:49<00:13,  0.91it/s, v_num=3, train/loss_step=0.100]\n",
      "Epoch 0:  98%|█████████▊| 536/547 [09:49<00:12,  0.91it/s, v_num=3, train/loss_step=0.100]\n",
      "Epoch 0:  98%|█████████▊| 536/547 [09:50<00:12,  0.91it/s, v_num=3, train/loss_step=0.386]\n",
      "Epoch 0:  98%|█████████▊| 537/547 [09:50<00:10,  0.91it/s, v_num=3, train/loss_step=0.386]\n",
      "Epoch 0:  98%|█████████▊| 537/547 [09:51<00:11,  0.91it/s, v_num=3, train/loss_step=0.089]\n",
      "Epoch 0:  98%|█████████▊| 538/547 [09:51<00:09,  0.91it/s, v_num=3, train/loss_step=0.089]\n",
      "Epoch 0:  98%|█████████▊| 538/547 [09:52<00:09,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  99%|█████████▊| 539/547 [09:52<00:08,  0.91it/s, v_num=3, train/loss_step=0.171]\n",
      "Epoch 0:  99%|█████████▊| 539/547 [09:53<00:08,  0.91it/s, v_num=3, train/loss_step=0.170]\n",
      "Epoch 0:  99%|█████████▊| 540/547 [09:53<00:07,  0.91it/s, v_num=3, train/loss_step=0.170]\n",
      "Epoch 0:  99%|█████████▊| 540/547 [09:54<00:07,  0.91it/s, v_num=3, train/loss_step=0.129]\n",
      "Epoch 0:  99%|█████████▉| 541/547 [09:54<00:06,  0.91it/s, v_num=3, train/loss_step=0.129]\n",
      "Epoch 0:  99%|█████████▉| 541/547 [09:55<00:06,  0.91it/s, v_num=3, train/loss_step=0.208]\n",
      "Epoch 0:  99%|█████████▉| 542/547 [09:56<00:05,  0.91it/s, v_num=3, train/loss_step=0.208]\n",
      "Epoch 0:  99%|█████████▉| 542/547 [09:56<00:05,  0.91it/s, v_num=3, train/loss_step=0.242]\n",
      "Epoch 0:  99%|█████████▉| 543/547 [09:57<00:04,  0.91it/s, v_num=3, train/loss_step=0.242]\n",
      "Epoch 0:  99%|█████████▉| 543/547 [09:58<00:04,  0.91it/s, v_num=3, train/loss_step=0.727]\n",
      "Epoch 0:  99%|█████████▉| 544/547 [09:58<00:03,  0.91it/s, v_num=3, train/loss_step=0.727]\n",
      "Epoch 0:  99%|█████████▉| 544/547 [09:59<00:03,  0.91it/s, v_num=3, train/loss_step=0.320]\n",
      "Epoch 0: 100%|█████████▉| 545/547 [09:59<00:02,  0.91it/s, v_num=3, train/loss_step=0.320]\n",
      "Epoch 0: 100%|█████████▉| 545/547 [10:00<00:02,  0.91it/s, v_num=3, train/loss_step=0.328]\n",
      "Epoch 0: 100%|█████████▉| 546/547 [10:00<00:01,  0.91it/s, v_num=3, train/loss_step=0.328]\n",
      "Epoch 0: 100%|█████████▉| 546/547 [10:01<00:01,  0.91it/s, v_num=3, train/loss_step=0.220]\n",
      "Epoch 0: 100%|██████████| 547/547 [10:01<00:00,  0.91it/s, v_num=3, train/loss_step=0.220]\n",
      "Epoch 0: 100%|██████████| 547/547 [10:02<00:00,  0.91it/s, v_num=3, train/loss_step=0.484]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/118 [00:00<00:16,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/118 [00:00<00:16,  6.90it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/118 [00:00<00:16,  6.90it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 4/118 [00:00<00:16,  6.91it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   4%|▍         | 5/118 [00:00<00:16,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/118 [00:00<00:16,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   6%|▌         | 7/118 [00:01<00:16,  6.93it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/118 [00:01<00:15,  6.93it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 10/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 11/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 12/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 13/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 14/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 15/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 16/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 17/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 18/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  16%|█▌        | 19/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 20/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 21/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  19%|█▊        | 22/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 23/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 24/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 25/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 26/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 27/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 28/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 29/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 30/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 31/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 32/118 [00:04<00:12,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 33/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 34/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  30%|██▉       | 35/118 [00:05<00:11,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 36/118 [00:05<00:11,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  31%|███▏      | 37/118 [00:05<00:11,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 38/118 [00:05<00:11,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 39/118 [00:05<00:11,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  34%|███▍      | 40/118 [00:05<00:11,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 41/118 [00:05<00:11,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 42/118 [00:06<00:10,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 43/118 [00:06<00:10,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 44/118 [00:06<00:10,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 45/118 [00:06<00:10,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 46/118 [00:06<00:10,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  40%|███▉      | 47/118 [00:06<00:10,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 48/118 [00:06<00:10,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 49/118 [00:07<00:09,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 50/118 [00:07<00:09,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 51/118 [00:07<00:09,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  44%|████▍     | 52/118 [00:07<00:09,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 53/118 [00:07<00:09,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  46%|████▌     | 54/118 [00:07<00:09,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 55/118 [00:07<00:09,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 56/118 [00:08<00:08,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 57/118 [00:08<00:08,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 58/118 [00:08<00:08,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 59/118 [00:08<00:08,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 60/118 [00:08<00:08,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 61/118 [00:08<00:08,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 62/118 [00:08<00:08,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 63/118 [00:09<00:07,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▍    | 64/118 [00:09<00:07,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 65/118 [00:09<00:07,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▌    | 66/118 [00:09<00:07,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 67/118 [00:09<00:07,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 68/118 [00:09<00:07,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 69/118 [00:09<00:07,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 70/118 [00:10<00:06,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 71/118 [00:10<00:06,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 72/118 [00:10<00:06,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 73/118 [00:10<00:06,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 74/118 [00:10<00:06,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 75/118 [00:10<00:06,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 76/118 [00:10<00:06,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 77/118 [00:11<00:05,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▌   | 78/118 [00:11<00:05,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 79/118 [00:11<00:05,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 80/118 [00:11<00:05,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▊   | 81/118 [00:11<00:05,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 82/118 [00:11<00:05,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 83/118 [00:11<00:05,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 84/118 [00:12<00:04,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 85/118 [00:12<00:04,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 86/118 [00:12<00:04,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 87/118 [00:12<00:04,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 88/118 [00:12<00:04,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 89/118 [00:12<00:04,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 90/118 [00:12<00:04,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 91/118 [00:13<00:03,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 92/118 [00:13<00:03,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 93/118 [00:13<00:03,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  80%|███████▉  | 94/118 [00:13<00:03,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 95/118 [00:13<00:03,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  81%|████████▏ | 96/118 [00:13<00:03,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 97/118 [00:13<00:03,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 98/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▍ | 99/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 100/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 101/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 102/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 103/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 104/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 105/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  90%|████████▉ | 106/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 107/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 108/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 109/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 110/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▍| 111/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 112/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▌| 113/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 114/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 115/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 116/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 117/118 [00:16<00:00,  6.95it/s]\u001b[A/home/eaguayo/.local/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 118/118 [00:16<00:00,  7.00it/s]\u001b[A\n",
      "\n",
      "                                                                          \u001b[A\n",
      "Epoch 0: 100%|██████████| 547/547 [10:40<00:00,  0.85it/s, v_num=3, train/loss_step=0.484, val/loss=0.246]/home/eaguayo/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\n",
      "Epoch 0: 100%|██████████| 547/547 [10:40<00:00,  0.85it/s, v_num=3, train/loss_step=0.484, val/loss=0.246, train/loss_epoch=0.380][rank: 0] Metric val/loss improved. New best score: 0.246\n",
      "[rank: 1] Metric val/loss improved. New best score: 0.220\n",
      "[rank: 2] Metric val/loss improved. New best score: 0.219\n",
      "[rank: 3] Metric val/loss improved. New best score: 0.220\n",
      "Epoch 0, global step 547: 'val/loss' reached 0.24581 (best 0.24581), saving model to '/home/eaguayo/workspace/DeepLearning/Week3/bert-classifier/sentiment_checkpoints/best-checkpoint-epoch=00-val/loss=0.25.ckpt' as top 1\n",
      "\n",
      "Epoch 0:   0%|          | 0/547 [00:00<?, ?it/s, v_num=3, train/loss_step=0.484, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   0%|          | 0/547 [00:00<?, ?it/s, v_num=3, train/loss_step=0.484, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   0%|          | 1/547 [00:00<03:47,  2.40it/s, v_num=3, train/loss_step=0.484, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   0%|          | 1/547 [00:01<11:27,  0.79it/s, v_num=3, train/loss_step=0.0904, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   0%|          | 2/547 [00:01<06:48,  1.33it/s, v_num=3, train/loss_step=0.0904, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   0%|          | 2/547 [00:02<10:40,  0.85it/s, v_num=3, train/loss_step=0.199, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 3/547 [00:02<07:50,  1.16it/s, v_num=3, train/loss_step=0.199, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 3/547 [00:03<10:25,  0.87it/s, v_num=3, train/loss_step=0.286, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 4/547 [00:03<08:21,  1.08it/s, v_num=3, train/loss_step=0.286, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 4/547 [00:04<10:16,  0.88it/s, v_num=3, train/loss_step=0.247, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 5/547 [00:04<08:38,  1.05it/s, v_num=3, train/loss_step=0.247, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 5/547 [00:05<10:10,  0.89it/s, v_num=3, train/loss_step=0.245, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 6/547 [00:05<08:49,  1.02it/s, v_num=3, train/loss_step=0.245, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|          | 6/547 [00:06<10:07,  0.89it/s, v_num=3, train/loss_step=0.491, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|▏         | 7/547 [00:06<08:58,  1.00it/s, v_num=3, train/loss_step=0.491, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|▏         | 7/547 [00:07<10:04,  0.89it/s, v_num=3, train/loss_step=0.250, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|▏         | 8/547 [00:08<09:03,  0.99it/s, v_num=3, train/loss_step=0.250, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   1%|▏         | 8/547 [00:08<10:01,  0.90it/s, v_num=3, train/loss_step=0.0651, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 9/547 [00:09<09:08,  0.98it/s, v_num=3, train/loss_step=0.0651, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 9/547 [00:10<09:58,  0.90it/s, v_num=3, train/loss_step=0.0875, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 10/547 [00:10<09:10,  0.97it/s, v_num=3, train/loss_step=0.0875, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 10/547 [00:11<09:56,  0.90it/s, v_num=3, train/loss_step=0.0507, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 11/547 [00:11<09:13,  0.97it/s, v_num=3, train/loss_step=0.0507, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 11/547 [00:12<09:54,  0.90it/s, v_num=3, train/loss_step=0.228, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 12/547 [00:12<09:15,  0.96it/s, v_num=3, train/loss_step=0.228, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 12/547 [00:13<09:53,  0.90it/s, v_num=3, train/loss_step=0.0879, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 13/547 [00:13<09:16,  0.96it/s, v_num=3, train/loss_step=0.0879, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   2%|▏         | 13/547 [00:14<09:51,  0.90it/s, v_num=3, train/loss_step=0.235, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 14/547 [00:14<09:17,  0.96it/s, v_num=3, train/loss_step=0.235, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 14/547 [00:15<09:50,  0.90it/s, v_num=3, train/loss_step=0.0486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 15/547 [00:15<09:18,  0.95it/s, v_num=3, train/loss_step=0.0486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 15/547 [00:16<09:48,  0.90it/s, v_num=3, train/loss_step=0.430, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 16/547 [00:16<09:19,  0.95it/s, v_num=3, train/loss_step=0.430, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 16/547 [00:17<09:47,  0.90it/s, v_num=3, train/loss_step=0.342, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 17/547 [00:17<09:19,  0.95it/s, v_num=3, train/loss_step=0.342, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 17/547 [00:18<09:45,  0.90it/s, v_num=3, train/loss_step=0.0488, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 18/547 [00:19<09:19,  0.95it/s, v_num=3, train/loss_step=0.0488, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 18/547 [00:19<09:44,  0.91it/s, v_num=3, train/loss_step=0.185, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 19/547 [00:20<09:19,  0.94it/s, v_num=3, train/loss_step=0.185, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   3%|▎         | 19/547 [00:20<09:42,  0.91it/s, v_num=3, train/loss_step=0.120, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▎         | 20/547 [00:21<09:19,  0.94it/s, v_num=3, train/loss_step=0.120, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▎         | 20/547 [00:22<09:41,  0.91it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 21/547 [00:22<09:18,  0.94it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 21/547 [00:23<09:40,  0.91it/s, v_num=3, train/loss_step=0.180, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 22/547 [00:23<09:18,  0.94it/s, v_num=3, train/loss_step=0.180, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 22/547 [00:24<09:39,  0.91it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 23/547 [00:24<09:18,  0.94it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 23/547 [00:25<09:38,  0.91it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 24/547 [00:25<09:18,  0.94it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   4%|▍         | 24/547 [00:26<09:36,  0.91it/s, v_num=3, train/loss_step=0.125, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▍         | 25/547 [00:26<09:17,  0.94it/s, v_num=3, train/loss_step=0.125, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▍         | 25/547 [00:27<09:35,  0.91it/s, v_num=3, train/loss_step=0.0866, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▍         | 26/547 [00:27<09:17,  0.93it/s, v_num=3, train/loss_step=0.0866, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▍         | 26/547 [00:28<09:34,  0.91it/s, v_num=3, train/loss_step=0.289, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▍         | 27/547 [00:28<09:16,  0.93it/s, v_num=3, train/loss_step=0.289, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▍         | 27/547 [00:29<09:33,  0.91it/s, v_num=3, train/loss_step=0.0925, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▌         | 28/547 [00:30<09:16,  0.93it/s, v_num=3, train/loss_step=0.0925, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▌         | 28/547 [00:30<09:31,  0.91it/s, v_num=3, train/loss_step=0.224, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▌         | 29/547 [00:31<09:15,  0.93it/s, v_num=3, train/loss_step=0.224, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▌         | 29/547 [00:31<09:30,  0.91it/s, v_num=3, train/loss_step=0.605, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▌         | 30/547 [00:32<09:14,  0.93it/s, v_num=3, train/loss_step=0.605, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   5%|▌         | 30/547 [00:33<09:29,  0.91it/s, v_num=3, train/loss_step=0.265, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 31/547 [00:33<09:14,  0.93it/s, v_num=3, train/loss_step=0.265, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 31/547 [00:34<09:28,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 32/547 [00:34<09:13,  0.93it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 32/547 [00:35<09:27,  0.91it/s, v_num=3, train/loss_step=0.0882, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 33/547 [00:35<09:12,  0.93it/s, v_num=3, train/loss_step=0.0882, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 33/547 [00:36<09:25,  0.91it/s, v_num=3, train/loss_step=0.087, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 34/547 [00:36<09:11,  0.93it/s, v_num=3, train/loss_step=0.087, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▌         | 34/547 [00:37<09:24,  0.91it/s, v_num=3, train/loss_step=0.172, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▋         | 35/547 [00:37<09:11,  0.93it/s, v_num=3, train/loss_step=0.172, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   6%|▋         | 35/547 [00:38<09:23,  0.91it/s, v_num=3, train/loss_step=0.0655, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 36/547 [00:38<09:10,  0.93it/s, v_num=3, train/loss_step=0.0655, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 36/547 [00:39<09:22,  0.91it/s, v_num=3, train/loss_step=0.0549, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 37/547 [00:39<09:09,  0.93it/s, v_num=3, train/loss_step=0.0549, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 37/547 [00:40<09:21,  0.91it/s, v_num=3, train/loss_step=0.0529, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 38/547 [00:40<09:08,  0.93it/s, v_num=3, train/loss_step=0.0529, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 38/547 [00:41<09:20,  0.91it/s, v_num=3, train/loss_step=0.364, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 39/547 [00:42<09:07,  0.93it/s, v_num=3, train/loss_step=0.364, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 39/547 [00:42<09:19,  0.91it/s, v_num=3, train/loss_step=0.0789, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 40/547 [00:43<09:07,  0.93it/s, v_num=3, train/loss_step=0.0789, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 40/547 [00:44<09:17,  0.91it/s, v_num=3, train/loss_step=0.279, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 41/547 [00:44<09:06,  0.93it/s, v_num=3, train/loss_step=0.279, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   7%|▋         | 41/547 [00:45<09:16,  0.91it/s, v_num=3, train/loss_step=0.0602, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 42/547 [00:45<09:05,  0.93it/s, v_num=3, train/loss_step=0.0602, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 42/547 [00:46<09:15,  0.91it/s, v_num=3, train/loss_step=0.201, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 43/547 [00:46<09:04,  0.93it/s, v_num=3, train/loss_step=0.201, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 43/547 [00:47<09:14,  0.91it/s, v_num=3, train/loss_step=0.0159, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 44/547 [00:47<09:03,  0.93it/s, v_num=3, train/loss_step=0.0159, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 44/547 [00:48<09:13,  0.91it/s, v_num=3, train/loss_step=0.0866, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 45/547 [00:48<09:02,  0.92it/s, v_num=3, train/loss_step=0.0866, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 45/547 [00:49<09:12,  0.91it/s, v_num=3, train/loss_step=0.163, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 46/547 [00:49<09:01,  0.92it/s, v_num=3, train/loss_step=0.163, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   8%|▊         | 46/547 [00:50<09:11,  0.91it/s, v_num=3, train/loss_step=0.486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▊         | 47/547 [00:50<09:00,  0.92it/s, v_num=3, train/loss_step=0.486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▊         | 47/547 [00:51<09:10,  0.91it/s, v_num=3, train/loss_step=0.069, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 48/547 [00:51<08:59,  0.92it/s, v_num=3, train/loss_step=0.069, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 48/547 [00:52<09:08,  0.91it/s, v_num=3, train/loss_step=0.364, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 49/547 [00:53<08:59,  0.92it/s, v_num=3, train/loss_step=0.364, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 49/547 [00:53<09:07,  0.91it/s, v_num=3, train/loss_step=0.204, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 50/547 [00:54<08:58,  0.92it/s, v_num=3, train/loss_step=0.204, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 50/547 [00:54<09:06,  0.91it/s, v_num=3, train/loss_step=0.104, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 51/547 [00:55<08:57,  0.92it/s, v_num=3, train/loss_step=0.104, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:   9%|▉         | 51/547 [00:56<09:05,  0.91it/s, v_num=3, train/loss_step=0.029, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|▉         | 52/547 [00:56<08:56,  0.92it/s, v_num=3, train/loss_step=0.029, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|▉         | 52/547 [00:57<09:04,  0.91it/s, v_num=3, train/loss_step=0.0439, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|▉         | 53/547 [00:57<08:55,  0.92it/s, v_num=3, train/loss_step=0.0439, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|▉         | 53/547 [00:58<09:03,  0.91it/s, v_num=3, train/loss_step=0.288, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|▉         | 54/547 [00:58<08:54,  0.92it/s, v_num=3, train/loss_step=0.288, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|▉         | 54/547 [00:59<09:02,  0.91it/s, v_num=3, train/loss_step=0.465, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|█         | 55/547 [00:59<08:53,  0.92it/s, v_num=3, train/loss_step=0.465, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|█         | 55/547 [01:00<09:01,  0.91it/s, v_num=3, train/loss_step=0.0852, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|█         | 56/547 [01:00<08:52,  0.92it/s, v_num=3, train/loss_step=0.0852, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|█         | 56/547 [01:01<08:59,  0.91it/s, v_num=3, train/loss_step=0.532, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|█         | 57/547 [01:01<08:51,  0.92it/s, v_num=3, train/loss_step=0.532, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  10%|█         | 57/547 [01:02<08:58,  0.91it/s, v_num=3, train/loss_step=0.0618, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 58/547 [01:02<08:50,  0.92it/s, v_num=3, train/loss_step=0.0618, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 58/547 [01:03<08:57,  0.91it/s, v_num=3, train/loss_step=0.0978, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 59/547 [01:04<08:49,  0.92it/s, v_num=3, train/loss_step=0.0978, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 59/547 [01:04<08:56,  0.91it/s, v_num=3, train/loss_step=0.478, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 60/547 [01:05<08:48,  0.92it/s, v_num=3, train/loss_step=0.478, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 60/547 [01:05<08:55,  0.91it/s, v_num=3, train/loss_step=0.0723, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 61/547 [01:06<08:47,  0.92it/s, v_num=3, train/loss_step=0.0723, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█         | 61/547 [01:07<08:54,  0.91it/s, v_num=3, train/loss_step=0.344, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█▏        | 62/547 [01:07<08:46,  0.92it/s, v_num=3, train/loss_step=0.344, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  11%|█▏        | 62/547 [01:08<08:53,  0.91it/s, v_num=3, train/loss_step=0.071, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 63/547 [01:08<08:45,  0.92it/s, v_num=3, train/loss_step=0.071, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 63/547 [01:09<08:52,  0.91it/s, v_num=3, train/loss_step=0.922, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 64/547 [01:09<08:44,  0.92it/s, v_num=3, train/loss_step=0.922, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 64/547 [01:10<08:51,  0.91it/s, v_num=3, train/loss_step=0.0486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 65/547 [01:10<08:43,  0.92it/s, v_num=3, train/loss_step=0.0486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 65/547 [01:11<08:49,  0.91it/s, v_num=3, train/loss_step=0.361, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 66/547 [01:11<08:42,  0.92it/s, v_num=3, train/loss_step=0.361, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 66/547 [01:12<08:48,  0.91it/s, v_num=3, train/loss_step=0.331, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 67/547 [01:12<08:41,  0.92it/s, v_num=3, train/loss_step=0.331, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 67/547 [01:13<08:47,  0.91it/s, v_num=3, train/loss_step=0.0741, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 68/547 [01:13<08:40,  0.92it/s, v_num=3, train/loss_step=0.0741, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  12%|█▏        | 68/547 [01:14<08:46,  0.91it/s, v_num=3, train/loss_step=0.0729, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 69/547 [01:14<08:39,  0.92it/s, v_num=3, train/loss_step=0.0729, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 69/547 [01:15<08:45,  0.91it/s, v_num=3, train/loss_step=0.172, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 70/547 [01:16<08:38,  0.92it/s, v_num=3, train/loss_step=0.172, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 70/547 [01:16<08:44,  0.91it/s, v_num=3, train/loss_step=0.307, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 71/547 [01:17<08:37,  0.92it/s, v_num=3, train/loss_step=0.307, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 71/547 [01:18<08:43,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 72/547 [01:18<08:36,  0.92it/s, v_num=3, train/loss_step=0.110, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 72/547 [01:19<08:42,  0.91it/s, v_num=3, train/loss_step=0.0727, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 73/547 [01:19<08:35,  0.92it/s, v_num=3, train/loss_step=0.0727, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  13%|█▎        | 73/547 [01:20<08:40,  0.91it/s, v_num=3, train/loss_step=0.0503, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▎        | 74/547 [01:20<08:34,  0.92it/s, v_num=3, train/loss_step=0.0503, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▎        | 74/547 [01:21<08:39,  0.91it/s, v_num=3, train/loss_step=0.732, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▎        | 75/547 [01:21<08:33,  0.92it/s, v_num=3, train/loss_step=0.732, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▎        | 75/547 [01:22<08:38,  0.91it/s, v_num=3, train/loss_step=0.252, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 76/547 [01:22<08:32,  0.92it/s, v_num=3, train/loss_step=0.252, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 76/547 [01:23<08:37,  0.91it/s, v_num=3, train/loss_step=0.0307, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 77/547 [01:23<08:31,  0.92it/s, v_num=3, train/loss_step=0.0307, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 77/547 [01:24<08:36,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 78/547 [01:24<08:30,  0.92it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 78/547 [01:25<08:35,  0.91it/s, v_num=3, train/loss_step=0.0348, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 79/547 [01:25<08:29,  0.92it/s, v_num=3, train/loss_step=0.0348, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  14%|█▍        | 79/547 [01:26<08:34,  0.91it/s, v_num=3, train/loss_step=0.267, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▍        | 80/547 [01:27<08:28,  0.92it/s, v_num=3, train/loss_step=0.267, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▍        | 80/547 [01:27<08:33,  0.91it/s, v_num=3, train/loss_step=0.0403, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▍        | 81/547 [01:28<08:27,  0.92it/s, v_num=3, train/loss_step=0.0403, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▍        | 81/547 [01:29<08:32,  0.91it/s, v_num=3, train/loss_step=0.291, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▍        | 82/547 [01:29<08:26,  0.92it/s, v_num=3, train/loss_step=0.291, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▍        | 82/547 [01:30<08:31,  0.91it/s, v_num=3, train/loss_step=0.297, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▌        | 83/547 [01:30<08:25,  0.92it/s, v_num=3, train/loss_step=0.297, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▌        | 83/547 [01:31<08:29,  0.91it/s, v_num=3, train/loss_step=0.321, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▌        | 84/547 [01:31<08:24,  0.92it/s, v_num=3, train/loss_step=0.321, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  15%|█▌        | 84/547 [01:32<08:28,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 85/547 [01:32<08:23,  0.92it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 85/547 [01:33<08:27,  0.91it/s, v_num=3, train/loss_step=0.346, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 86/547 [01:33<08:22,  0.92it/s, v_num=3, train/loss_step=0.346, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 86/547 [01:34<08:26,  0.91it/s, v_num=3, train/loss_step=0.152, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 87/547 [01:34<08:20,  0.92it/s, v_num=3, train/loss_step=0.152, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 87/547 [01:35<08:25,  0.91it/s, v_num=3, train/loss_step=0.287, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 88/547 [01:35<08:19,  0.92it/s, v_num=3, train/loss_step=0.287, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▌        | 88/547 [01:36<08:24,  0.91it/s, v_num=3, train/loss_step=0.368, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▋        | 89/547 [01:36<08:18,  0.92it/s, v_num=3, train/loss_step=0.368, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▋        | 89/547 [01:37<08:23,  0.91it/s, v_num=3, train/loss_step=0.0494, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▋        | 90/547 [01:38<08:17,  0.92it/s, v_num=3, train/loss_step=0.0494, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  16%|█▋        | 90/547 [01:38<08:22,  0.91it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 91/547 [01:39<08:16,  0.92it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 91/547 [01:40<08:21,  0.91it/s, v_num=3, train/loss_step=0.0379, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 92/547 [01:40<08:15,  0.92it/s, v_num=3, train/loss_step=0.0379, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 92/547 [01:41<08:20,  0.91it/s, v_num=3, train/loss_step=0.0195, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 93/547 [01:41<08:14,  0.92it/s, v_num=3, train/loss_step=0.0195, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 93/547 [01:42<08:18,  0.91it/s, v_num=3, train/loss_step=0.088, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 94/547 [01:42<08:13,  0.92it/s, v_num=3, train/loss_step=0.088, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 94/547 [01:43<08:17,  0.91it/s, v_num=3, train/loss_step=0.416, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 95/547 [01:43<08:12,  0.92it/s, v_num=3, train/loss_step=0.416, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  17%|█▋        | 95/547 [01:44<08:16,  0.91it/s, v_num=3, train/loss_step=0.195, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 96/547 [01:44<08:11,  0.92it/s, v_num=3, train/loss_step=0.195, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 96/547 [01:45<08:15,  0.91it/s, v_num=3, train/loss_step=0.0501, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 97/547 [01:45<08:10,  0.92it/s, v_num=3, train/loss_step=0.0501, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 97/547 [01:46<08:14,  0.91it/s, v_num=3, train/loss_step=0.219, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 98/547 [01:46<08:09,  0.92it/s, v_num=3, train/loss_step=0.219, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 98/547 [01:47<08:13,  0.91it/s, v_num=3, train/loss_step=0.0521, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 99/547 [01:47<08:08,  0.92it/s, v_num=3, train/loss_step=0.0521, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 99/547 [01:48<08:12,  0.91it/s, v_num=3, train/loss_step=0.154, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 100/547 [01:49<08:07,  0.92it/s, v_num=3, train/loss_step=0.154, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 100/547 [01:49<08:11,  0.91it/s, v_num=3, train/loss_step=0.313, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 101/547 [01:50<08:06,  0.92it/s, v_num=3, train/loss_step=0.313, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  18%|█▊        | 101/547 [01:50<08:10,  0.91it/s, v_num=3, train/loss_step=0.102, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▊        | 102/547 [01:51<08:05,  0.92it/s, v_num=3, train/loss_step=0.102, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▊        | 102/547 [01:52<08:09,  0.91it/s, v_num=3, train/loss_step=0.258, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 103/547 [01:52<08:04,  0.92it/s, v_num=3, train/loss_step=0.258, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 103/547 [01:53<08:07,  0.91it/s, v_num=3, train/loss_step=0.205, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 104/547 [01:53<08:03,  0.92it/s, v_num=3, train/loss_step=0.205, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 104/547 [01:54<08:06,  0.91it/s, v_num=3, train/loss_step=0.324, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 105/547 [01:54<08:02,  0.92it/s, v_num=3, train/loss_step=0.324, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 105/547 [01:55<08:05,  0.91it/s, v_num=3, train/loss_step=0.058, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 106/547 [01:55<08:01,  0.92it/s, v_num=3, train/loss_step=0.058, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  19%|█▉        | 106/547 [01:56<08:04,  0.91it/s, v_num=3, train/loss_step=0.0403, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|█▉        | 107/547 [01:56<08:00,  0.92it/s, v_num=3, train/loss_step=0.0403, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|█▉        | 107/547 [01:57<08:03,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|█▉        | 108/547 [01:57<07:58,  0.92it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|█▉        | 108/547 [01:58<08:02,  0.91it/s, v_num=3, train/loss_step=0.0901, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|█▉        | 109/547 [01:58<07:57,  0.92it/s, v_num=3, train/loss_step=0.0901, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|█▉        | 109/547 [01:59<08:01,  0.91it/s, v_num=3, train/loss_step=0.121, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|██        | 110/547 [02:00<07:56,  0.92it/s, v_num=3, train/loss_step=0.121, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|██        | 110/547 [02:00<08:00,  0.91it/s, v_num=3, train/loss_step=0.270, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|██        | 111/547 [02:01<07:55,  0.92it/s, v_num=3, train/loss_step=0.270, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|██        | 111/547 [02:01<07:59,  0.91it/s, v_num=3, train/loss_step=0.189, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|██        | 112/547 [02:02<07:54,  0.92it/s, v_num=3, train/loss_step=0.189, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  20%|██        | 112/547 [02:03<07:58,  0.91it/s, v_num=3, train/loss_step=0.205, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 113/547 [02:03<07:53,  0.92it/s, v_num=3, train/loss_step=0.205, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 113/547 [02:04<07:56,  0.91it/s, v_num=3, train/loss_step=0.0572, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 114/547 [02:04<07:52,  0.92it/s, v_num=3, train/loss_step=0.0572, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 114/547 [02:05<07:55,  0.91it/s, v_num=3, train/loss_step=0.0391, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 115/547 [02:05<07:51,  0.92it/s, v_num=3, train/loss_step=0.0391, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 115/547 [02:06<07:54,  0.91it/s, v_num=3, train/loss_step=0.375, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 116/547 [02:06<07:50,  0.92it/s, v_num=3, train/loss_step=0.375, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██        | 116/547 [02:07<07:53,  0.91it/s, v_num=3, train/loss_step=0.0989, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██▏       | 117/547 [02:07<07:49,  0.92it/s, v_num=3, train/loss_step=0.0989, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  21%|██▏       | 117/547 [02:08<07:52,  0.91it/s, v_num=3, train/loss_step=0.0952, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 118/547 [02:08<07:48,  0.92it/s, v_num=3, train/loss_step=0.0952, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 118/547 [02:09<07:51,  0.91it/s, v_num=3, train/loss_step=0.0252, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 119/547 [02:09<07:47,  0.92it/s, v_num=3, train/loss_step=0.0252, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 119/547 [02:10<07:50,  0.91it/s, v_num=3, train/loss_step=0.162, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 120/547 [02:11<07:46,  0.92it/s, v_num=3, train/loss_step=0.162, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 120/547 [02:11<07:49,  0.91it/s, v_num=3, train/loss_step=0.137, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 121/547 [02:12<07:45,  0.92it/s, v_num=3, train/loss_step=0.137, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 121/547 [02:12<07:48,  0.91it/s, v_num=3, train/loss_step=0.300, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 122/547 [02:13<07:44,  0.92it/s, v_num=3, train/loss_step=0.300, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 122/547 [02:14<07:46,  0.91it/s, v_num=3, train/loss_step=0.321, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 123/547 [02:14<07:42,  0.92it/s, v_num=3, train/loss_step=0.321, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  22%|██▏       | 123/547 [02:15<07:45,  0.91it/s, v_num=3, train/loss_step=0.305, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 124/547 [02:15<07:41,  0.92it/s, v_num=3, train/loss_step=0.305, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 124/547 [02:16<07:44,  0.91it/s, v_num=3, train/loss_step=0.0836, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 125/547 [02:16<07:40,  0.92it/s, v_num=3, train/loss_step=0.0836, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 125/547 [02:17<07:43,  0.91it/s, v_num=3, train/loss_step=0.0919, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 126/547 [02:17<07:39,  0.92it/s, v_num=3, train/loss_step=0.0919, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 126/547 [02:18<07:42,  0.91it/s, v_num=3, train/loss_step=0.266, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 127/547 [02:18<07:38,  0.92it/s, v_num=3, train/loss_step=0.266, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 127/547 [02:19<07:41,  0.91it/s, v_num=3, train/loss_step=0.0582, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 128/547 [02:19<07:37,  0.92it/s, v_num=3, train/loss_step=0.0582, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  23%|██▎       | 128/547 [02:20<07:40,  0.91it/s, v_num=3, train/loss_step=0.310, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▎       | 129/547 [02:20<07:36,  0.92it/s, v_num=3, train/loss_step=0.310, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▎       | 129/547 [02:21<07:39,  0.91it/s, v_num=3, train/loss_step=0.231, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 130/547 [02:22<07:35,  0.92it/s, v_num=3, train/loss_step=0.231, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 130/547 [02:22<07:38,  0.91it/s, v_num=3, train/loss_step=0.0906, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 131/547 [02:23<07:34,  0.92it/s, v_num=3, train/loss_step=0.0906, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 131/547 [02:23<07:37,  0.91it/s, v_num=3, train/loss_step=0.0839, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 132/547 [02:24<07:33,  0.92it/s, v_num=3, train/loss_step=0.0839, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 132/547 [02:25<07:36,  0.91it/s, v_num=3, train/loss_step=0.132, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 133/547 [02:25<07:32,  0.92it/s, v_num=3, train/loss_step=0.132, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 133/547 [02:26<07:34,  0.91it/s, v_num=3, train/loss_step=0.123, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 134/547 [02:26<07:31,  0.92it/s, v_num=3, train/loss_step=0.123, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  24%|██▍       | 134/547 [02:27<07:33,  0.91it/s, v_num=3, train/loss_step=0.102, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▍       | 135/547 [02:27<07:30,  0.92it/s, v_num=3, train/loss_step=0.102, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▍       | 135/547 [02:28<07:32,  0.91it/s, v_num=3, train/loss_step=0.185, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▍       | 136/547 [02:28<07:29,  0.92it/s, v_num=3, train/loss_step=0.185, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▍       | 136/547 [02:29<07:31,  0.91it/s, v_num=3, train/loss_step=0.0732, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▌       | 137/547 [02:29<07:28,  0.92it/s, v_num=3, train/loss_step=0.0732, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▌       | 137/547 [02:30<07:30,  0.91it/s, v_num=3, train/loss_step=0.0552, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▌       | 138/547 [02:30<07:26,  0.92it/s, v_num=3, train/loss_step=0.0552, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▌       | 138/547 [02:31<07:29,  0.91it/s, v_num=3, train/loss_step=0.242, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▌       | 139/547 [02:31<07:25,  0.92it/s, v_num=3, train/loss_step=0.242, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  25%|██▌       | 139/547 [02:32<07:28,  0.91it/s, v_num=3, train/loss_step=0.442, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 140/547 [02:33<07:24,  0.92it/s, v_num=3, train/loss_step=0.442, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 140/547 [02:33<07:27,  0.91it/s, v_num=3, train/loss_step=0.201, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 141/547 [02:34<07:23,  0.91it/s, v_num=3, train/loss_step=0.201, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 141/547 [02:34<07:26,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 142/547 [02:35<07:22,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 142/547 [02:36<07:25,  0.91it/s, v_num=3, train/loss_step=0.0229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 143/547 [02:36<07:21,  0.91it/s, v_num=3, train/loss_step=0.0229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▌       | 143/547 [02:37<07:24,  0.91it/s, v_num=3, train/loss_step=0.153, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▋       | 144/547 [02:37<07:20,  0.91it/s, v_num=3, train/loss_step=0.153, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  26%|██▋       | 144/547 [02:38<07:22,  0.91it/s, v_num=3, train/loss_step=0.165, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 145/547 [02:38<07:19,  0.91it/s, v_num=3, train/loss_step=0.165, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 145/547 [02:39<07:21,  0.91it/s, v_num=3, train/loss_step=0.340, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 146/547 [02:39<07:18,  0.91it/s, v_num=3, train/loss_step=0.340, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 146/547 [02:40<07:20,  0.91it/s, v_num=3, train/loss_step=0.0377, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 147/547 [02:40<07:17,  0.91it/s, v_num=3, train/loss_step=0.0377, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 147/547 [02:41<07:19,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 148/547 [02:41<07:16,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 148/547 [02:42<07:18,  0.91it/s, v_num=3, train/loss_step=0.0444, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 149/547 [02:42<07:15,  0.91it/s, v_num=3, train/loss_step=0.0444, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 149/547 [02:43<07:17,  0.91it/s, v_num=3, train/loss_step=0.229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 150/547 [02:43<07:14,  0.91it/s, v_num=3, train/loss_step=0.229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  27%|██▋       | 150/547 [02:44<07:16,  0.91it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 151/547 [02:45<07:12,  0.91it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 151/547 [02:45<07:15,  0.91it/s, v_num=3, train/loss_step=0.0857, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 152/547 [02:46<07:11,  0.91it/s, v_num=3, train/loss_step=0.0857, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 152/547 [02:47<07:14,  0.91it/s, v_num=3, train/loss_step=0.135, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 153/547 [02:47<07:10,  0.91it/s, v_num=3, train/loss_step=0.135, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 153/547 [02:48<07:13,  0.91it/s, v_num=3, train/loss_step=0.147, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 154/547 [02:48<07:09,  0.91it/s, v_num=3, train/loss_step=0.147, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 154/547 [02:49<07:11,  0.91it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 155/547 [02:49<07:08,  0.91it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  28%|██▊       | 155/547 [02:50<07:10,  0.91it/s, v_num=3, train/loss_step=0.293, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▊       | 156/547 [02:50<07:07,  0.91it/s, v_num=3, train/loss_step=0.293, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▊       | 156/547 [02:51<07:09,  0.91it/s, v_num=3, train/loss_step=0.482, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▊       | 157/547 [02:51<07:06,  0.91it/s, v_num=3, train/loss_step=0.482, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▊       | 157/547 [02:52<07:08,  0.91it/s, v_num=3, train/loss_step=0.208, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 158/547 [02:52<07:05,  0.91it/s, v_num=3, train/loss_step=0.208, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 158/547 [02:53<07:07,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 159/547 [02:53<07:04,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 159/547 [02:54<07:06,  0.91it/s, v_num=3, train/loss_step=0.0864, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 160/547 [02:54<07:03,  0.91it/s, v_num=3, train/loss_step=0.0864, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 160/547 [02:55<07:05,  0.91it/s, v_num=3, train/loss_step=0.149, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 161/547 [02:56<07:02,  0.91it/s, v_num=3, train/loss_step=0.149, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  29%|██▉       | 161/547 [02:56<07:04,  0.91it/s, v_num=3, train/loss_step=0.0349, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|██▉       | 162/547 [02:57<07:01,  0.91it/s, v_num=3, train/loss_step=0.0349, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|██▉       | 162/547 [02:58<07:03,  0.91it/s, v_num=3, train/loss_step=0.285, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|██▉       | 163/547 [02:58<07:00,  0.91it/s, v_num=3, train/loss_step=0.285, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|██▉       | 163/547 [02:59<07:02,  0.91it/s, v_num=3, train/loss_step=0.0864, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|██▉       | 164/547 [02:59<06:58,  0.91it/s, v_num=3, train/loss_step=0.0864, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|██▉       | 164/547 [03:00<07:00,  0.91it/s, v_num=3, train/loss_step=0.0747, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|███       | 165/547 [03:00<06:57,  0.91it/s, v_num=3, train/loss_step=0.0747, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|███       | 165/547 [03:01<06:59,  0.91it/s, v_num=3, train/loss_step=0.170, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|███       | 166/547 [03:01<06:56,  0.91it/s, v_num=3, train/loss_step=0.170, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  30%|███       | 166/547 [03:02<06:58,  0.91it/s, v_num=3, train/loss_step=0.451, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 167/547 [03:02<06:55,  0.91it/s, v_num=3, train/loss_step=0.451, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 167/547 [03:03<06:57,  0.91it/s, v_num=3, train/loss_step=0.263, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 168/547 [03:03<06:54,  0.91it/s, v_num=3, train/loss_step=0.263, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 168/547 [03:04<06:56,  0.91it/s, v_num=3, train/loss_step=0.0791, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 169/547 [03:04<06:53,  0.91it/s, v_num=3, train/loss_step=0.0791, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 169/547 [03:05<06:55,  0.91it/s, v_num=3, train/loss_step=0.247, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 170/547 [03:05<06:52,  0.91it/s, v_num=3, train/loss_step=0.247, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███       | 170/547 [03:06<06:54,  0.91it/s, v_num=3, train/loss_step=0.0524, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███▏      | 171/547 [03:07<06:51,  0.91it/s, v_num=3, train/loss_step=0.0524, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███▏      | 171/547 [03:07<06:53,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███▏      | 172/547 [03:08<06:50,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  31%|███▏      | 172/547 [03:09<06:52,  0.91it/s, v_num=3, train/loss_step=0.305, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 173/547 [03:09<06:49,  0.91it/s, v_num=3, train/loss_step=0.305, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 173/547 [03:10<06:51,  0.91it/s, v_num=3, train/loss_step=0.478, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 174/547 [03:10<06:48,  0.91it/s, v_num=3, train/loss_step=0.478, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 174/547 [03:11<06:49,  0.91it/s, v_num=3, train/loss_step=0.218, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 175/547 [03:11<06:47,  0.91it/s, v_num=3, train/loss_step=0.218, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 175/547 [03:12<06:48,  0.91it/s, v_num=3, train/loss_step=0.199, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 176/547 [03:12<06:45,  0.91it/s, v_num=3, train/loss_step=0.199, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 176/547 [03:13<06:47,  0.91it/s, v_num=3, train/loss_step=0.0223, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 177/547 [03:13<06:44,  0.91it/s, v_num=3, train/loss_step=0.0223, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  32%|███▏      | 177/547 [03:14<06:46,  0.91it/s, v_num=3, train/loss_step=0.270, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 178/547 [03:14<06:43,  0.91it/s, v_num=3, train/loss_step=0.270, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 178/547 [03:15<06:45,  0.91it/s, v_num=3, train/loss_step=0.0687, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 179/547 [03:15<06:42,  0.91it/s, v_num=3, train/loss_step=0.0687, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 179/547 [03:16<06:44,  0.91it/s, v_num=3, train/loss_step=0.0828, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 180/547 [03:16<06:41,  0.91it/s, v_num=3, train/loss_step=0.0828, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 180/547 [03:17<06:43,  0.91it/s, v_num=3, train/loss_step=0.246, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 181/547 [03:18<06:40,  0.91it/s, v_num=3, train/loss_step=0.246, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 181/547 [03:18<06:42,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 182/547 [03:19<06:39,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 182/547 [03:20<06:41,  0.91it/s, v_num=3, train/loss_step=0.038, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 183/547 [03:20<06:38,  0.91it/s, v_num=3, train/loss_step=0.038, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  33%|███▎      | 183/547 [03:21<06:40,  0.91it/s, v_num=3, train/loss_step=0.253, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▎      | 184/547 [03:21<06:37,  0.91it/s, v_num=3, train/loss_step=0.253, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▎      | 184/547 [03:22<06:39,  0.91it/s, v_num=3, train/loss_step=0.0412, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 185/547 [03:22<06:36,  0.91it/s, v_num=3, train/loss_step=0.0412, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 185/547 [03:23<06:37,  0.91it/s, v_num=3, train/loss_step=0.0445, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 186/547 [03:23<06:35,  0.91it/s, v_num=3, train/loss_step=0.0445, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 186/547 [03:24<06:36,  0.91it/s, v_num=3, train/loss_step=0.356, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 187/547 [03:24<06:34,  0.91it/s, v_num=3, train/loss_step=0.356, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 187/547 [03:25<06:35,  0.91it/s, v_num=3, train/loss_step=0.0317, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 188/547 [03:25<06:32,  0.91it/s, v_num=3, train/loss_step=0.0317, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  34%|███▍      | 188/547 [03:26<06:34,  0.91it/s, v_num=3, train/loss_step=0.0385, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▍      | 189/547 [03:26<06:31,  0.91it/s, v_num=3, train/loss_step=0.0385, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▍      | 189/547 [03:27<06:33,  0.91it/s, v_num=3, train/loss_step=0.0339, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▍      | 190/547 [03:27<06:30,  0.91it/s, v_num=3, train/loss_step=0.0339, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▍      | 190/547 [03:28<06:32,  0.91it/s, v_num=3, train/loss_step=0.0203, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▍      | 191/547 [03:29<06:29,  0.91it/s, v_num=3, train/loss_step=0.0203, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▍      | 191/547 [03:29<06:31,  0.91it/s, v_num=3, train/loss_step=0.409, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▌      | 192/547 [03:30<06:28,  0.91it/s, v_num=3, train/loss_step=0.409, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▌      | 192/547 [03:31<06:30,  0.91it/s, v_num=3, train/loss_step=0.237, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▌      | 193/547 [03:31<06:27,  0.91it/s, v_num=3, train/loss_step=0.237, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▌      | 193/547 [03:32<06:29,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▌      | 194/547 [03:32<06:26,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  35%|███▌      | 194/547 [03:33<06:28,  0.91it/s, v_num=3, train/loss_step=0.0516, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 195/547 [03:33<06:25,  0.91it/s, v_num=3, train/loss_step=0.0516, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 195/547 [03:34<06:26,  0.91it/s, v_num=3, train/loss_step=0.0872, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 196/547 [03:34<06:24,  0.91it/s, v_num=3, train/loss_step=0.0872, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 196/547 [03:35<06:25,  0.91it/s, v_num=3, train/loss_step=0.223, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 197/547 [03:35<06:23,  0.91it/s, v_num=3, train/loss_step=0.223, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 197/547 [03:36<06:24,  0.91it/s, v_num=3, train/loss_step=0.0815, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 198/547 [03:36<06:22,  0.91it/s, v_num=3, train/loss_step=0.0815, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▌      | 198/547 [03:37<06:23,  0.91it/s, v_num=3, train/loss_step=0.119, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▋      | 199/547 [03:37<06:21,  0.91it/s, v_num=3, train/loss_step=0.119, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  36%|███▋      | 199/547 [03:38<06:22,  0.91it/s, v_num=3, train/loss_step=0.312, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 200/547 [03:38<06:19,  0.91it/s, v_num=3, train/loss_step=0.312, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 200/547 [03:39<06:21,  0.91it/s, v_num=3, train/loss_step=0.336, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 201/547 [03:40<06:18,  0.91it/s, v_num=3, train/loss_step=0.336, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 201/547 [03:40<06:20,  0.91it/s, v_num=3, train/loss_step=0.171, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 202/547 [03:41<06:17,  0.91it/s, v_num=3, train/loss_step=0.171, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 202/547 [03:42<06:19,  0.91it/s, v_num=3, train/loss_step=0.242, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 203/547 [03:42<06:16,  0.91it/s, v_num=3, train/loss_step=0.242, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 203/547 [03:43<06:18,  0.91it/s, v_num=3, train/loss_step=0.0785, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 204/547 [03:43<06:15,  0.91it/s, v_num=3, train/loss_step=0.0785, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 204/547 [03:44<06:17,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 205/547 [03:44<06:14,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  37%|███▋      | 205/547 [03:45<06:15,  0.91it/s, v_num=3, train/loss_step=0.156, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 206/547 [03:45<06:13,  0.91it/s, v_num=3, train/loss_step=0.156, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 206/547 [03:46<06:14,  0.91it/s, v_num=3, train/loss_step=0.671, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 207/547 [03:46<06:12,  0.91it/s, v_num=3, train/loss_step=0.671, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 207/547 [03:47<06:13,  0.91it/s, v_num=3, train/loss_step=0.0892, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 208/547 [03:47<06:11,  0.91it/s, v_num=3, train/loss_step=0.0892, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 208/547 [03:48<06:12,  0.91it/s, v_num=3, train/loss_step=0.119, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 209/547 [03:48<06:10,  0.91it/s, v_num=3, train/loss_step=0.119, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 209/547 [03:49<06:11,  0.91it/s, v_num=3, train/loss_step=0.0831, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 210/547 [03:50<06:09,  0.91it/s, v_num=3, train/loss_step=0.0831, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  38%|███▊      | 210/547 [03:50<06:10,  0.91it/s, v_num=3, train/loss_step=0.083, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▊      | 211/547 [03:51<06:08,  0.91it/s, v_num=3, train/loss_step=0.083, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▊      | 211/547 [03:51<06:09,  0.91it/s, v_num=3, train/loss_step=0.105, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 212/547 [03:52<06:06,  0.91it/s, v_num=3, train/loss_step=0.105, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 212/547 [03:53<06:08,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 213/547 [03:53<06:05,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 213/547 [03:54<06:07,  0.91it/s, v_num=3, train/loss_step=0.281, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 214/547 [03:54<06:04,  0.91it/s, v_num=3, train/loss_step=0.281, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 214/547 [03:55<06:06,  0.91it/s, v_num=3, train/loss_step=0.547, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 215/547 [03:55<06:03,  0.91it/s, v_num=3, train/loss_step=0.547, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 215/547 [03:56<06:04,  0.91it/s, v_num=3, train/loss_step=0.152, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 216/547 [03:56<06:02,  0.91it/s, v_num=3, train/loss_step=0.152, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  39%|███▉      | 216/547 [03:57<06:03,  0.91it/s, v_num=3, train/loss_step=0.175, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|███▉      | 217/547 [03:57<06:01,  0.91it/s, v_num=3, train/loss_step=0.175, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|███▉      | 217/547 [03:58<06:02,  0.91it/s, v_num=3, train/loss_step=0.231, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|███▉      | 218/547 [03:58<06:00,  0.91it/s, v_num=3, train/loss_step=0.231, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|███▉      | 218/547 [03:59<06:01,  0.91it/s, v_num=3, train/loss_step=0.226, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|████      | 219/547 [03:59<05:59,  0.91it/s, v_num=3, train/loss_step=0.226, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|████      | 219/547 [04:00<06:00,  0.91it/s, v_num=3, train/loss_step=0.217, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|████      | 220/547 [04:01<05:58,  0.91it/s, v_num=3, train/loss_step=0.217, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|████      | 220/547 [04:01<05:59,  0.91it/s, v_num=3, train/loss_step=0.142, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|████      | 221/547 [04:02<05:57,  0.91it/s, v_num=3, train/loss_step=0.142, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  40%|████      | 221/547 [04:02<05:58,  0.91it/s, v_num=3, train/loss_step=0.0486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 222/547 [04:03<05:56,  0.91it/s, v_num=3, train/loss_step=0.0486, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 222/547 [04:04<05:57,  0.91it/s, v_num=3, train/loss_step=0.141, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 223/547 [04:04<05:54,  0.91it/s, v_num=3, train/loss_step=0.141, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 223/547 [04:05<05:56,  0.91it/s, v_num=3, train/loss_step=0.0399, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 224/547 [04:05<05:53,  0.91it/s, v_num=3, train/loss_step=0.0399, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 224/547 [04:06<05:55,  0.91it/s, v_num=3, train/loss_step=0.253, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 225/547 [04:06<05:52,  0.91it/s, v_num=3, train/loss_step=0.253, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████      | 225/547 [04:07<05:54,  0.91it/s, v_num=3, train/loss_step=0.0224, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████▏     | 226/547 [04:07<05:51,  0.91it/s, v_num=3, train/loss_step=0.0224, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████▏     | 226/547 [04:08<05:52,  0.91it/s, v_num=3, train/loss_step=0.182, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████▏     | 227/547 [04:08<05:50,  0.91it/s, v_num=3, train/loss_step=0.182, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  41%|████▏     | 227/547 [04:09<05:51,  0.91it/s, v_num=3, train/loss_step=0.491, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 228/547 [04:09<05:49,  0.91it/s, v_num=3, train/loss_step=0.491, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 228/547 [04:10<05:50,  0.91it/s, v_num=3, train/loss_step=0.289, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 229/547 [04:10<05:48,  0.91it/s, v_num=3, train/loss_step=0.289, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 229/547 [04:11<05:49,  0.91it/s, v_num=3, train/loss_step=0.278, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 230/547 [04:12<05:47,  0.91it/s, v_num=3, train/loss_step=0.278, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 230/547 [04:12<05:48,  0.91it/s, v_num=3, train/loss_step=0.0695, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 231/547 [04:13<05:46,  0.91it/s, v_num=3, train/loss_step=0.0695, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 231/547 [04:13<05:47,  0.91it/s, v_num=3, train/loss_step=0.0912, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 232/547 [04:14<05:45,  0.91it/s, v_num=3, train/loss_step=0.0912, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  42%|████▏     | 232/547 [04:15<05:46,  0.91it/s, v_num=3, train/loss_step=0.499, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 233/547 [04:15<05:44,  0.91it/s, v_num=3, train/loss_step=0.499, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 233/547 [04:16<05:45,  0.91it/s, v_num=3, train/loss_step=0.216, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 234/547 [04:16<05:42,  0.91it/s, v_num=3, train/loss_step=0.216, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 234/547 [04:17<05:44,  0.91it/s, v_num=3, train/loss_step=0.0544, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 235/547 [04:17<05:41,  0.91it/s, v_num=3, train/loss_step=0.0544, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 235/547 [04:18<05:43,  0.91it/s, v_num=3, train/loss_step=0.153, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 236/547 [04:18<05:40,  0.91it/s, v_num=3, train/loss_step=0.153, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 236/547 [04:19<05:41,  0.91it/s, v_num=3, train/loss_step=0.303, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 237/547 [04:19<05:39,  0.91it/s, v_num=3, train/loss_step=0.303, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  43%|████▎     | 237/547 [04:20<05:40,  0.91it/s, v_num=3, train/loss_step=0.0425, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▎     | 238/547 [04:20<05:38,  0.91it/s, v_num=3, train/loss_step=0.0425, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▎     | 238/547 [04:21<05:39,  0.91it/s, v_num=3, train/loss_step=0.0131, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▎     | 239/547 [04:21<05:37,  0.91it/s, v_num=3, train/loss_step=0.0131, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▎     | 239/547 [04:22<05:38,  0.91it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 240/547 [04:23<05:36,  0.91it/s, v_num=3, train/loss_step=0.292, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 240/547 [04:23<05:37,  0.91it/s, v_num=3, train/loss_step=0.381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 241/547 [04:24<05:35,  0.91it/s, v_num=3, train/loss_step=0.381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 241/547 [04:24<05:36,  0.91it/s, v_num=3, train/loss_step=0.441, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 242/547 [04:25<05:34,  0.91it/s, v_num=3, train/loss_step=0.441, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 242/547 [04:26<05:35,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 243/547 [04:26<05:33,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  44%|████▍     | 243/547 [04:27<05:34,  0.91it/s, v_num=3, train/loss_step=0.318, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▍     | 244/547 [04:27<05:32,  0.91it/s, v_num=3, train/loss_step=0.318, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▍     | 244/547 [04:28<05:33,  0.91it/s, v_num=3, train/loss_step=0.0466, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▍     | 245/547 [04:28<05:30,  0.91it/s, v_num=3, train/loss_step=0.0466, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▍     | 245/547 [04:29<05:32,  0.91it/s, v_num=3, train/loss_step=0.0936, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▍     | 246/547 [04:29<05:29,  0.91it/s, v_num=3, train/loss_step=0.0936, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▍     | 246/547 [04:30<05:30,  0.91it/s, v_num=3, train/loss_step=0.0861, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▌     | 247/547 [04:30<05:28,  0.91it/s, v_num=3, train/loss_step=0.0861, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▌     | 247/547 [04:31<05:29,  0.91it/s, v_num=3, train/loss_step=0.0833, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▌     | 248/547 [04:31<05:27,  0.91it/s, v_num=3, train/loss_step=0.0833, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  45%|████▌     | 248/547 [04:32<05:28,  0.91it/s, v_num=3, train/loss_step=0.368, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 249/547 [04:32<05:26,  0.91it/s, v_num=3, train/loss_step=0.368, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 249/547 [04:33<05:27,  0.91it/s, v_num=3, train/loss_step=0.0366, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 250/547 [04:34<05:25,  0.91it/s, v_num=3, train/loss_step=0.0366, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 250/547 [04:34<05:26,  0.91it/s, v_num=3, train/loss_step=0.371, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 251/547 [04:35<05:24,  0.91it/s, v_num=3, train/loss_step=0.371, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 251/547 [04:35<05:25,  0.91it/s, v_num=3, train/loss_step=0.0415, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 252/547 [04:36<05:23,  0.91it/s, v_num=3, train/loss_step=0.0415, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▌     | 252/547 [04:37<05:24,  0.91it/s, v_num=3, train/loss_step=0.444, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▋     | 253/547 [04:37<05:22,  0.91it/s, v_num=3, train/loss_step=0.444, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▋     | 253/547 [04:38<05:23,  0.91it/s, v_num=3, train/loss_step=0.502, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▋     | 254/547 [04:38<05:21,  0.91it/s, v_num=3, train/loss_step=0.502, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  46%|████▋     | 254/547 [04:39<05:22,  0.91it/s, v_num=3, train/loss_step=0.0717, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 255/547 [04:39<05:20,  0.91it/s, v_num=3, train/loss_step=0.0717, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 255/547 [04:40<05:21,  0.91it/s, v_num=3, train/loss_step=0.260, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 256/547 [04:40<05:18,  0.91it/s, v_num=3, train/loss_step=0.260, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 256/547 [04:41<05:19,  0.91it/s, v_num=3, train/loss_step=0.200, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 257/547 [04:41<05:17,  0.91it/s, v_num=3, train/loss_step=0.200, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 257/547 [04:42<05:18,  0.91it/s, v_num=3, train/loss_step=0.0356, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 258/547 [04:42<05:16,  0.91it/s, v_num=3, train/loss_step=0.0356, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 258/547 [04:43<05:17,  0.91it/s, v_num=3, train/loss_step=0.175, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 259/547 [04:43<05:15,  0.91it/s, v_num=3, train/loss_step=0.175, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  47%|████▋     | 259/547 [04:44<05:16,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 260/547 [04:45<05:14,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 260/547 [04:45<05:15,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 261/547 [04:46<05:13,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 261/547 [04:47<05:14,  0.91it/s, v_num=3, train/loss_step=0.116, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 262/547 [04:47<05:12,  0.91it/s, v_num=3, train/loss_step=0.116, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 262/547 [04:48<05:13,  0.91it/s, v_num=3, train/loss_step=0.0271, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 263/547 [04:48<05:11,  0.91it/s, v_num=3, train/loss_step=0.0271, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 263/547 [04:49<05:12,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 264/547 [04:49<05:10,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 264/547 [04:50<05:11,  0.91it/s, v_num=3, train/loss_step=0.0288, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 265/547 [04:50<05:09,  0.91it/s, v_num=3, train/loss_step=0.0288, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  48%|████▊     | 265/547 [04:51<05:10,  0.91it/s, v_num=3, train/loss_step=0.0443, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▊     | 266/547 [04:51<05:08,  0.91it/s, v_num=3, train/loss_step=0.0443, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▊     | 266/547 [04:52<05:09,  0.91it/s, v_num=3, train/loss_step=0.674, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 267/547 [04:52<05:07,  0.91it/s, v_num=3, train/loss_step=0.674, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 267/547 [04:53<05:07,  0.91it/s, v_num=3, train/loss_step=0.214, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 268/547 [04:53<05:05,  0.91it/s, v_num=3, train/loss_step=0.214, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 268/547 [04:54<05:06,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 269/547 [04:54<05:04,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 269/547 [04:55<05:05,  0.91it/s, v_num=3, train/loss_step=0.279, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 270/547 [04:56<05:03,  0.91it/s, v_num=3, train/loss_step=0.279, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  49%|████▉     | 270/547 [04:56<05:04,  0.91it/s, v_num=3, train/loss_step=0.0466, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|████▉     | 271/547 [04:57<05:02,  0.91it/s, v_num=3, train/loss_step=0.0466, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|████▉     | 271/547 [04:58<05:03,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|████▉     | 272/547 [04:58<05:01,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|████▉     | 272/547 [04:59<05:02,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|████▉     | 273/547 [04:59<05:00,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|████▉     | 273/547 [05:00<05:01,  0.91it/s, v_num=3, train/loss_step=0.351, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|█████     | 274/547 [05:00<04:59,  0.91it/s, v_num=3, train/loss_step=0.351, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|█████     | 274/547 [05:01<05:00,  0.91it/s, v_num=3, train/loss_step=0.167, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|█████     | 275/547 [05:01<04:58,  0.91it/s, v_num=3, train/loss_step=0.167, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|█████     | 275/547 [05:02<04:59,  0.91it/s, v_num=3, train/loss_step=0.0388, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|█████     | 276/547 [05:02<04:57,  0.91it/s, v_num=3, train/loss_step=0.0388, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  50%|█████     | 276/547 [05:03<04:58,  0.91it/s, v_num=3, train/loss_step=0.0906, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 277/547 [05:03<04:56,  0.91it/s, v_num=3, train/loss_step=0.0906, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 277/547 [05:04<04:56,  0.91it/s, v_num=3, train/loss_step=0.400, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 278/547 [05:04<04:55,  0.91it/s, v_num=3, train/loss_step=0.400, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 278/547 [05:05<04:55,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 279/547 [05:05<04:53,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 279/547 [05:06<04:54,  0.91it/s, v_num=3, train/loss_step=0.0305, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 280/547 [05:07<04:52,  0.91it/s, v_num=3, train/loss_step=0.0305, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████     | 280/547 [05:07<04:53,  0.91it/s, v_num=3, train/loss_step=0.238, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████▏    | 281/547 [05:08<04:51,  0.91it/s, v_num=3, train/loss_step=0.238, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  51%|█████▏    | 281/547 [05:09<04:52,  0.91it/s, v_num=3, train/loss_step=0.0277, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 282/547 [05:09<04:50,  0.91it/s, v_num=3, train/loss_step=0.0277, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 282/547 [05:10<04:51,  0.91it/s, v_num=3, train/loss_step=0.351, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 283/547 [05:10<04:49,  0.91it/s, v_num=3, train/loss_step=0.351, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 283/547 [05:11<04:50,  0.91it/s, v_num=3, train/loss_step=0.146, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 284/547 [05:11<04:48,  0.91it/s, v_num=3, train/loss_step=0.146, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 284/547 [05:12<04:49,  0.91it/s, v_num=3, train/loss_step=0.122, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 285/547 [05:12<04:47,  0.91it/s, v_num=3, train/loss_step=0.122, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 285/547 [05:13<04:48,  0.91it/s, v_num=3, train/loss_step=0.188, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 286/547 [05:13<04:46,  0.91it/s, v_num=3, train/loss_step=0.188, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 286/547 [05:14<04:47,  0.91it/s, v_num=3, train/loss_step=0.249, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 287/547 [05:14<04:45,  0.91it/s, v_num=3, train/loss_step=0.249, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  52%|█████▏    | 287/547 [05:15<04:45,  0.91it/s, v_num=3, train/loss_step=0.147, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 288/547 [05:15<04:44,  0.91it/s, v_num=3, train/loss_step=0.147, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 288/547 [05:16<04:44,  0.91it/s, v_num=3, train/loss_step=0.205, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 289/547 [05:17<04:43,  0.91it/s, v_num=3, train/loss_step=0.205, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 289/547 [05:17<04:43,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 290/547 [05:18<04:41,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 290/547 [05:18<04:42,  0.91it/s, v_num=3, train/loss_step=0.248, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 291/547 [05:19<04:40,  0.91it/s, v_num=3, train/loss_step=0.248, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 291/547 [05:20<04:41,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 292/547 [05:20<04:39,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  53%|█████▎    | 292/547 [05:21<04:40,  0.91it/s, v_num=3, train/loss_step=0.301, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▎    | 293/547 [05:21<04:38,  0.91it/s, v_num=3, train/loss_step=0.301, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▎    | 293/547 [05:22<04:39,  0.91it/s, v_num=3, train/loss_step=0.0488, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▎    | 294/547 [05:22<04:37,  0.91it/s, v_num=3, train/loss_step=0.0488, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▎    | 294/547 [05:23<04:38,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 295/547 [05:23<04:36,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 295/547 [05:24<04:37,  0.91it/s, v_num=3, train/loss_step=0.197, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 296/547 [05:24<04:35,  0.91it/s, v_num=3, train/loss_step=0.197, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 296/547 [05:25<04:36,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 297/547 [05:25<04:34,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 297/547 [05:26<04:34,  0.91it/s, v_num=3, train/loss_step=0.174, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 298/547 [05:26<04:33,  0.91it/s, v_num=3, train/loss_step=0.174, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  54%|█████▍    | 298/547 [05:27<04:33,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▍    | 299/547 [05:28<04:32,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▍    | 299/547 [05:28<04:32,  0.91it/s, v_num=3, train/loss_step=0.0442, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▍    | 300/547 [05:29<04:30,  0.91it/s, v_num=3, train/loss_step=0.0442, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▍    | 300/547 [05:30<04:31,  0.91it/s, v_num=3, train/loss_step=0.0679, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▌    | 301/547 [05:30<04:29,  0.91it/s, v_num=3, train/loss_step=0.0679, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▌    | 301/547 [05:31<04:30,  0.91it/s, v_num=3, train/loss_step=0.025, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▌    | 302/547 [05:31<04:28,  0.91it/s, v_num=3, train/loss_step=0.025, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▌    | 302/547 [05:32<04:29,  0.91it/s, v_num=3, train/loss_step=0.0676, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▌    | 303/547 [05:32<04:27,  0.91it/s, v_num=3, train/loss_step=0.0676, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  55%|█████▌    | 303/547 [05:33<04:28,  0.91it/s, v_num=3, train/loss_step=0.0849, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 304/547 [05:33<04:26,  0.91it/s, v_num=3, train/loss_step=0.0849, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 304/547 [05:34<04:27,  0.91it/s, v_num=3, train/loss_step=0.220, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 305/547 [05:34<04:25,  0.91it/s, v_num=3, train/loss_step=0.220, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 305/547 [05:35<04:26,  0.91it/s, v_num=3, train/loss_step=0.0217, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 306/547 [05:35<04:24,  0.91it/s, v_num=3, train/loss_step=0.0217, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 306/547 [05:36<04:25,  0.91it/s, v_num=3, train/loss_step=0.221, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 307/547 [05:36<04:23,  0.91it/s, v_num=3, train/loss_step=0.221, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▌    | 307/547 [05:37<04:24,  0.91it/s, v_num=3, train/loss_step=0.439, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▋    | 308/547 [05:37<04:22,  0.91it/s, v_num=3, train/loss_step=0.439, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▋    | 308/547 [05:38<04:22,  0.91it/s, v_num=3, train/loss_step=0.203, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▋    | 309/547 [05:39<04:21,  0.91it/s, v_num=3, train/loss_step=0.203, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  56%|█████▋    | 309/547 [05:39<04:21,  0.91it/s, v_num=3, train/loss_step=0.470, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 310/547 [05:40<04:20,  0.91it/s, v_num=3, train/loss_step=0.470, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 310/547 [05:41<04:20,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 311/547 [05:41<04:18,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 311/547 [05:42<04:19,  0.91it/s, v_num=3, train/loss_step=0.0594, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 312/547 [05:42<04:17,  0.91it/s, v_num=3, train/loss_step=0.0594, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 312/547 [05:43<04:18,  0.91it/s, v_num=3, train/loss_step=0.141, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 313/547 [05:43<04:16,  0.91it/s, v_num=3, train/loss_step=0.141, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 313/547 [05:44<04:17,  0.91it/s, v_num=3, train/loss_step=0.0458, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 314/547 [05:44<04:15,  0.91it/s, v_num=3, train/loss_step=0.0458, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  57%|█████▋    | 314/547 [05:45<04:16,  0.91it/s, v_num=3, train/loss_step=0.0596, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 315/547 [05:45<04:14,  0.91it/s, v_num=3, train/loss_step=0.0596, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 315/547 [05:46<04:15,  0.91it/s, v_num=3, train/loss_step=0.210, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 316/547 [05:46<04:13,  0.91it/s, v_num=3, train/loss_step=0.210, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 316/547 [05:47<04:14,  0.91it/s, v_num=3, train/loss_step=0.0966, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 317/547 [05:47<04:12,  0.91it/s, v_num=3, train/loss_step=0.0966, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 317/547 [05:48<04:13,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 318/547 [05:48<04:11,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 318/547 [05:49<04:11,  0.91it/s, v_num=3, train/loss_step=0.192, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 319/547 [05:50<04:10,  0.91it/s, v_num=3, train/loss_step=0.192, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  58%|█████▊    | 319/547 [05:50<04:10,  0.91it/s, v_num=3, train/loss_step=0.381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▊    | 320/547 [05:51<04:09,  0.91it/s, v_num=3, train/loss_step=0.381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▊    | 320/547 [05:52<04:09,  0.91it/s, v_num=3, train/loss_step=0.373, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▊    | 321/547 [05:52<04:08,  0.91it/s, v_num=3, train/loss_step=0.373, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▊    | 321/547 [05:53<04:08,  0.91it/s, v_num=3, train/loss_step=0.231, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 322/547 [05:53<04:06,  0.91it/s, v_num=3, train/loss_step=0.231, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 322/547 [05:54<04:07,  0.91it/s, v_num=3, train/loss_step=0.0272, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 323/547 [05:54<04:05,  0.91it/s, v_num=3, train/loss_step=0.0272, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 323/547 [05:55<04:06,  0.91it/s, v_num=3, train/loss_step=0.0711, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 324/547 [05:55<04:04,  0.91it/s, v_num=3, train/loss_step=0.0711, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 324/547 [05:56<04:05,  0.91it/s, v_num=3, train/loss_step=0.0508, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 325/547 [05:56<04:03,  0.91it/s, v_num=3, train/loss_step=0.0508, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  59%|█████▉    | 325/547 [05:57<04:04,  0.91it/s, v_num=3, train/loss_step=0.411, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|█████▉    | 326/547 [05:57<04:02,  0.91it/s, v_num=3, train/loss_step=0.411, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|█████▉    | 326/547 [05:58<04:03,  0.91it/s, v_num=3, train/loss_step=0.229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|█████▉    | 327/547 [05:58<04:01,  0.91it/s, v_num=3, train/loss_step=0.229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|█████▉    | 327/547 [05:59<04:02,  0.91it/s, v_num=3, train/loss_step=0.064, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|█████▉    | 328/547 [06:00<04:00,  0.91it/s, v_num=3, train/loss_step=0.064, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|█████▉    | 328/547 [06:00<04:00,  0.91it/s, v_num=3, train/loss_step=0.0976, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|██████    | 329/547 [06:01<03:59,  0.91it/s, v_num=3, train/loss_step=0.0976, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|██████    | 329/547 [06:02<03:59,  0.91it/s, v_num=3, train/loss_step=0.0262, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|██████    | 330/547 [06:02<03:58,  0.91it/s, v_num=3, train/loss_step=0.0262, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  60%|██████    | 330/547 [06:03<03:58,  0.91it/s, v_num=3, train/loss_step=0.078, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 331/547 [06:03<03:57,  0.91it/s, v_num=3, train/loss_step=0.078, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 331/547 [06:04<03:57,  0.91it/s, v_num=3, train/loss_step=0.0696, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 332/547 [06:04<03:56,  0.91it/s, v_num=3, train/loss_step=0.0696, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 332/547 [06:05<03:56,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 333/547 [06:05<03:54,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 333/547 [06:06<03:55,  0.91it/s, v_num=3, train/loss_step=0.0355, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 334/547 [06:06<03:53,  0.91it/s, v_num=3, train/loss_step=0.0355, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 334/547 [06:07<03:54,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 335/547 [06:07<03:52,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████    | 335/547 [06:08<03:53,  0.91it/s, v_num=3, train/loss_step=0.0345, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████▏   | 336/547 [06:08<03:51,  0.91it/s, v_num=3, train/loss_step=0.0345, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  61%|██████▏   | 336/547 [06:09<03:52,  0.91it/s, v_num=3, train/loss_step=0.107, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 337/547 [06:09<03:50,  0.91it/s, v_num=3, train/loss_step=0.107, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 337/547 [06:10<03:51,  0.91it/s, v_num=3, train/loss_step=0.060, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 338/547 [06:11<03:49,  0.91it/s, v_num=3, train/loss_step=0.060, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 338/547 [06:11<03:49,  0.91it/s, v_num=3, train/loss_step=0.203, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 339/547 [06:12<03:48,  0.91it/s, v_num=3, train/loss_step=0.203, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 339/547 [06:13<03:48,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 340/547 [06:13<03:47,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 340/547 [06:14<03:47,  0.91it/s, v_num=3, train/loss_step=0.150, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 341/547 [06:14<03:46,  0.91it/s, v_num=3, train/loss_step=0.150, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  62%|██████▏   | 341/547 [06:15<03:46,  0.91it/s, v_num=3, train/loss_step=0.192, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 342/547 [06:15<03:45,  0.91it/s, v_num=3, train/loss_step=0.192, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 342/547 [06:16<03:45,  0.91it/s, v_num=3, train/loss_step=0.234, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 343/547 [06:16<03:43,  0.91it/s, v_num=3, train/loss_step=0.234, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 343/547 [06:17<03:44,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 344/547 [06:17<03:42,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 344/547 [06:18<03:43,  0.91it/s, v_num=3, train/loss_step=0.0802, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 345/547 [06:18<03:41,  0.91it/s, v_num=3, train/loss_step=0.0802, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 345/547 [06:19<03:42,  0.91it/s, v_num=3, train/loss_step=0.117, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 346/547 [06:19<03:40,  0.91it/s, v_num=3, train/loss_step=0.117, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 346/547 [06:20<03:41,  0.91it/s, v_num=3, train/loss_step=0.0347, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 347/547 [06:21<03:39,  0.91it/s, v_num=3, train/loss_step=0.0347, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  63%|██████▎   | 347/547 [06:21<03:40,  0.91it/s, v_num=3, train/loss_step=0.0367, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▎   | 348/547 [06:22<03:38,  0.91it/s, v_num=3, train/loss_step=0.0367, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▎   | 348/547 [06:23<03:39,  0.91it/s, v_num=3, train/loss_step=0.364, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 349/547 [06:23<03:37,  0.91it/s, v_num=3, train/loss_step=0.364, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 349/547 [06:24<03:37,  0.91it/s, v_num=3, train/loss_step=0.189, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 350/547 [06:24<03:36,  0.91it/s, v_num=3, train/loss_step=0.189, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 350/547 [06:25<03:36,  0.91it/s, v_num=3, train/loss_step=0.109, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 351/547 [06:25<03:35,  0.91it/s, v_num=3, train/loss_step=0.109, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 351/547 [06:26<03:35,  0.91it/s, v_num=3, train/loss_step=0.296, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 352/547 [06:26<03:34,  0.91it/s, v_num=3, train/loss_step=0.296, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  64%|██████▍   | 352/547 [06:27<03:34,  0.91it/s, v_num=3, train/loss_step=0.288, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▍   | 353/547 [06:27<03:33,  0.91it/s, v_num=3, train/loss_step=0.288, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▍   | 353/547 [06:28<03:33,  0.91it/s, v_num=3, train/loss_step=0.0454, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▍   | 354/547 [06:28<03:31,  0.91it/s, v_num=3, train/loss_step=0.0454, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▍   | 354/547 [06:29<03:32,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▍   | 355/547 [06:29<03:30,  0.91it/s, v_num=3, train/loss_step=0.128, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▍   | 355/547 [06:30<03:31,  0.91it/s, v_num=3, train/loss_step=0.120, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▌   | 356/547 [06:30<03:29,  0.91it/s, v_num=3, train/loss_step=0.120, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▌   | 356/547 [06:31<03:30,  0.91it/s, v_num=3, train/loss_step=0.160, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▌   | 357/547 [06:32<03:28,  0.91it/s, v_num=3, train/loss_step=0.160, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▌   | 357/547 [06:32<03:29,  0.91it/s, v_num=3, train/loss_step=0.135, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▌   | 358/547 [06:33<03:27,  0.91it/s, v_num=3, train/loss_step=0.135, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  65%|██████▌   | 358/547 [06:34<03:28,  0.91it/s, v_num=3, train/loss_step=0.0883, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 359/547 [06:34<03:26,  0.91it/s, v_num=3, train/loss_step=0.0883, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 359/547 [06:35<03:26,  0.91it/s, v_num=3, train/loss_step=0.343, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 360/547 [06:35<03:25,  0.91it/s, v_num=3, train/loss_step=0.343, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 360/547 [06:36<03:25,  0.91it/s, v_num=3, train/loss_step=0.0535, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 361/547 [06:36<03:24,  0.91it/s, v_num=3, train/loss_step=0.0535, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 361/547 [06:37<03:24,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 362/547 [06:37<03:23,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▌   | 362/547 [06:38<03:23,  0.91it/s, v_num=3, train/loss_step=0.0376, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▋   | 363/547 [06:38<03:22,  0.91it/s, v_num=3, train/loss_step=0.0376, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  66%|██████▋   | 363/547 [06:39<03:22,  0.91it/s, v_num=3, train/loss_step=0.0813, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 364/547 [06:39<03:21,  0.91it/s, v_num=3, train/loss_step=0.0813, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 364/547 [06:40<03:21,  0.91it/s, v_num=3, train/loss_step=0.0365, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 365/547 [06:40<03:19,  0.91it/s, v_num=3, train/loss_step=0.0365, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 365/547 [06:41<03:20,  0.91it/s, v_num=3, train/loss_step=0.0238, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 366/547 [06:42<03:18,  0.91it/s, v_num=3, train/loss_step=0.0238, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 366/547 [06:42<03:19,  0.91it/s, v_num=3, train/loss_step=0.291, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 367/547 [06:43<03:17,  0.91it/s, v_num=3, train/loss_step=0.291, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 367/547 [06:44<03:18,  0.91it/s, v_num=3, train/loss_step=0.0327, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 368/547 [06:44<03:16,  0.91it/s, v_num=3, train/loss_step=0.0327, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 368/547 [06:45<03:17,  0.91it/s, v_num=3, train/loss_step=0.171, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 369/547 [06:45<03:15,  0.91it/s, v_num=3, train/loss_step=0.171, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  67%|██████▋   | 369/547 [06:46<03:15,  0.91it/s, v_num=3, train/loss_step=0.207, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 370/547 [06:46<03:14,  0.91it/s, v_num=3, train/loss_step=0.207, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 370/547 [06:47<03:14,  0.91it/s, v_num=3, train/loss_step=0.0757, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 371/547 [06:47<03:13,  0.91it/s, v_num=3, train/loss_step=0.0757, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 371/547 [06:48<03:13,  0.91it/s, v_num=3, train/loss_step=0.242, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 372/547 [06:48<03:12,  0.91it/s, v_num=3, train/loss_step=0.242, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 372/547 [06:49<03:12,  0.91it/s, v_num=3, train/loss_step=0.432, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 373/547 [06:49<03:11,  0.91it/s, v_num=3, train/loss_step=0.432, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 373/547 [06:50<03:11,  0.91it/s, v_num=3, train/loss_step=0.178, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 374/547 [06:50<03:10,  0.91it/s, v_num=3, train/loss_step=0.178, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  68%|██████▊   | 374/547 [06:51<03:10,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▊   | 375/547 [06:52<03:08,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▊   | 375/547 [06:52<03:09,  0.91it/s, v_num=3, train/loss_step=0.484, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▊   | 376/547 [06:53<03:07,  0.91it/s, v_num=3, train/loss_step=0.484, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▊   | 376/547 [06:53<03:08,  0.91it/s, v_num=3, train/loss_step=0.0204, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 377/547 [06:54<03:06,  0.91it/s, v_num=3, train/loss_step=0.0204, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 377/547 [06:55<03:07,  0.91it/s, v_num=3, train/loss_step=0.103, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 378/547 [06:55<03:05,  0.91it/s, v_num=3, train/loss_step=0.103, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 378/547 [06:56<03:06,  0.91it/s, v_num=3, train/loss_step=0.410, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 379/547 [06:56<03:04,  0.91it/s, v_num=3, train/loss_step=0.410, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 379/547 [06:57<03:04,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 380/547 [06:57<03:03,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  69%|██████▉   | 380/547 [06:58<03:03,  0.91it/s, v_num=3, train/loss_step=0.277, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|██████▉   | 381/547 [06:58<03:02,  0.91it/s, v_num=3, train/loss_step=0.277, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|██████▉   | 381/547 [06:59<03:02,  0.91it/s, v_num=3, train/loss_step=0.162, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|██████▉   | 382/547 [06:59<03:01,  0.91it/s, v_num=3, train/loss_step=0.162, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|██████▉   | 382/547 [07:00<03:01,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|███████   | 383/547 [07:00<03:00,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|███████   | 383/547 [07:01<03:00,  0.91it/s, v_num=3, train/loss_step=0.147, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|███████   | 384/547 [07:01<02:59,  0.91it/s, v_num=3, train/loss_step=0.147, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|███████   | 384/547 [07:02<02:59,  0.91it/s, v_num=3, train/loss_step=0.155, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|███████   | 385/547 [07:03<02:58,  0.91it/s, v_num=3, train/loss_step=0.155, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  70%|███████   | 385/547 [07:03<02:58,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 386/547 [07:04<02:56,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 386/547 [07:05<02:57,  0.91it/s, v_num=3, train/loss_step=0.213, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 387/547 [07:05<02:55,  0.91it/s, v_num=3, train/loss_step=0.213, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 387/547 [07:06<02:56,  0.91it/s, v_num=3, train/loss_step=0.251, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 388/547 [07:06<02:54,  0.91it/s, v_num=3, train/loss_step=0.251, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 388/547 [07:07<02:55,  0.91it/s, v_num=3, train/loss_step=0.0753, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 389/547 [07:07<02:53,  0.91it/s, v_num=3, train/loss_step=0.0753, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████   | 389/547 [07:08<02:53,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████▏  | 390/547 [07:08<02:52,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████▏  | 390/547 [07:09<02:52,  0.91it/s, v_num=3, train/loss_step=0.0476, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████▏  | 391/547 [07:09<02:51,  0.91it/s, v_num=3, train/loss_step=0.0476, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  71%|███████▏  | 391/547 [07:10<02:51,  0.91it/s, v_num=3, train/loss_step=0.142, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 392/547 [07:10<02:50,  0.91it/s, v_num=3, train/loss_step=0.142, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 392/547 [07:11<02:50,  0.91it/s, v_num=3, train/loss_step=0.0831, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 393/547 [07:11<02:49,  0.91it/s, v_num=3, train/loss_step=0.0831, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 393/547 [07:12<02:49,  0.91it/s, v_num=3, train/loss_step=0.241, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 394/547 [07:13<02:48,  0.91it/s, v_num=3, train/loss_step=0.241, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 394/547 [07:13<02:48,  0.91it/s, v_num=3, train/loss_step=0.0931, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 395/547 [07:14<02:47,  0.91it/s, v_num=3, train/loss_step=0.0931, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 395/547 [07:15<02:47,  0.91it/s, v_num=3, train/loss_step=0.381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 396/547 [07:15<02:45,  0.91it/s, v_num=3, train/loss_step=0.381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  72%|███████▏  | 396/547 [07:16<02:46,  0.91it/s, v_num=3, train/loss_step=0.265, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 397/547 [07:16<02:44,  0.91it/s, v_num=3, train/loss_step=0.265, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 397/547 [07:17<02:45,  0.91it/s, v_num=3, train/loss_step=0.00767, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 398/547 [07:17<02:43,  0.91it/s, v_num=3, train/loss_step=0.00767, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 398/547 [07:18<02:44,  0.91it/s, v_num=3, train/loss_step=0.329, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 399/547 [07:18<02:42,  0.91it/s, v_num=3, train/loss_step=0.329, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 399/547 [07:19<02:42,  0.91it/s, v_num=3, train/loss_step=0.0715, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 400/547 [07:19<02:41,  0.91it/s, v_num=3, train/loss_step=0.0715, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 400/547 [07:20<02:41,  0.91it/s, v_num=3, train/loss_step=0.0909, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 401/547 [07:20<02:40,  0.91it/s, v_num=3, train/loss_step=0.0909, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 401/547 [07:21<02:40,  0.91it/s, v_num=3, train/loss_step=0.0487, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 402/547 [07:21<02:39,  0.91it/s, v_num=3, train/loss_step=0.0487, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  73%|███████▎  | 402/547 [07:22<02:39,  0.91it/s, v_num=3, train/loss_step=0.0516, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▎  | 403/547 [07:23<02:38,  0.91it/s, v_num=3, train/loss_step=0.0516, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▎  | 403/547 [07:23<02:38,  0.91it/s, v_num=3, train/loss_step=0.0596, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 404/547 [07:24<02:37,  0.91it/s, v_num=3, train/loss_step=0.0596, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 404/547 [07:24<02:37,  0.91it/s, v_num=3, train/loss_step=0.173, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 405/547 [07:25<02:36,  0.91it/s, v_num=3, train/loss_step=0.173, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 405/547 [07:26<02:36,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 406/547 [07:26<02:35,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 406/547 [07:27<02:35,  0.91it/s, v_num=3, train/loss_step=0.191, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 407/547 [07:27<02:33,  0.91it/s, v_num=3, train/loss_step=0.191, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  74%|███████▍  | 407/547 [07:28<02:34,  0.91it/s, v_num=3, train/loss_step=0.0441, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▍  | 408/547 [07:28<02:32,  0.91it/s, v_num=3, train/loss_step=0.0441, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▍  | 408/547 [07:29<02:33,  0.91it/s, v_num=3, train/loss_step=0.132, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▍  | 409/547 [07:29<02:31,  0.91it/s, v_num=3, train/loss_step=0.132, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▍  | 409/547 [07:30<02:32,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▍  | 410/547 [07:30<02:30,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▍  | 410/547 [07:31<02:30,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▌  | 411/547 [07:31<02:29,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▌  | 411/547 [07:32<02:29,  0.91it/s, v_num=3, train/loss_step=0.453, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▌  | 412/547 [07:32<02:28,  0.91it/s, v_num=3, train/loss_step=0.453, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  75%|███████▌  | 412/547 [07:33<02:28,  0.91it/s, v_num=3, train/loss_step=0.0736, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 413/547 [07:34<02:27,  0.91it/s, v_num=3, train/loss_step=0.0736, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 413/547 [07:34<02:27,  0.91it/s, v_num=3, train/loss_step=0.0481, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 414/547 [07:35<02:26,  0.91it/s, v_num=3, train/loss_step=0.0481, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 414/547 [07:36<02:26,  0.91it/s, v_num=3, train/loss_step=0.0606, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 415/547 [07:36<02:25,  0.91it/s, v_num=3, train/loss_step=0.0606, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 415/547 [07:37<02:25,  0.91it/s, v_num=3, train/loss_step=0.183, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 416/547 [07:37<02:24,  0.91it/s, v_num=3, train/loss_step=0.183, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 416/547 [07:38<02:24,  0.91it/s, v_num=3, train/loss_step=0.0961, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 417/547 [07:38<02:22,  0.91it/s, v_num=3, train/loss_step=0.0961, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▌  | 417/547 [07:39<02:23,  0.91it/s, v_num=3, train/loss_step=0.0819, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▋  | 418/547 [07:39<02:21,  0.91it/s, v_num=3, train/loss_step=0.0819, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  76%|███████▋  | 418/547 [07:40<02:22,  0.91it/s, v_num=3, train/loss_step=0.0868, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 419/547 [07:40<02:20,  0.91it/s, v_num=3, train/loss_step=0.0868, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 419/547 [07:41<02:21,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 420/547 [07:41<02:19,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 420/547 [07:42<02:19,  0.91it/s, v_num=3, train/loss_step=0.0791, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 421/547 [07:42<02:18,  0.91it/s, v_num=3, train/loss_step=0.0791, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 421/547 [07:43<02:18,  0.91it/s, v_num=3, train/loss_step=0.0609, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 422/547 [07:44<02:17,  0.91it/s, v_num=3, train/loss_step=0.0609, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 422/547 [07:44<02:17,  0.91it/s, v_num=3, train/loss_step=0.0973, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 423/547 [07:45<02:16,  0.91it/s, v_num=3, train/loss_step=0.0973, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  77%|███████▋  | 423/547 [07:46<02:16,  0.91it/s, v_num=3, train/loss_step=0.279, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 424/547 [07:46<02:15,  0.91it/s, v_num=3, train/loss_step=0.279, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 424/547 [07:47<02:15,  0.91it/s, v_num=3, train/loss_step=0.135, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 425/547 [07:47<02:14,  0.91it/s, v_num=3, train/loss_step=0.135, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 425/547 [07:48<02:14,  0.91it/s, v_num=3, train/loss_step=0.408, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 426/547 [07:48<02:13,  0.91it/s, v_num=3, train/loss_step=0.408, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 426/547 [07:49<02:13,  0.91it/s, v_num=3, train/loss_step=0.271, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 427/547 [07:49<02:11,  0.91it/s, v_num=3, train/loss_step=0.271, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 427/547 [07:50<02:12,  0.91it/s, v_num=3, train/loss_step=0.0946, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 428/547 [07:50<02:10,  0.91it/s, v_num=3, train/loss_step=0.0946, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 428/547 [07:51<02:11,  0.91it/s, v_num=3, train/loss_step=0.679, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 429/547 [07:51<02:09,  0.91it/s, v_num=3, train/loss_step=0.679, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  78%|███████▊  | 429/547 [07:52<02:10,  0.91it/s, v_num=3, train/loss_step=0.0977, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▊  | 430/547 [07:52<02:08,  0.91it/s, v_num=3, train/loss_step=0.0977, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▊  | 430/547 [07:53<02:08,  0.91it/s, v_num=3, train/loss_step=0.102, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 431/547 [07:54<02:07,  0.91it/s, v_num=3, train/loss_step=0.102, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 431/547 [07:54<02:07,  0.91it/s, v_num=3, train/loss_step=0.458, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 432/547 [07:55<02:06,  0.91it/s, v_num=3, train/loss_step=0.458, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 432/547 [07:56<02:06,  0.91it/s, v_num=3, train/loss_step=0.0744, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 433/547 [07:56<02:05,  0.91it/s, v_num=3, train/loss_step=0.0744, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 433/547 [07:57<02:05,  0.91it/s, v_num=3, train/loss_step=0.0483, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 434/547 [07:57<02:04,  0.91it/s, v_num=3, train/loss_step=0.0483, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  79%|███████▉  | 434/547 [07:58<02:04,  0.91it/s, v_num=3, train/loss_step=0.196, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|███████▉  | 435/547 [07:58<02:03,  0.91it/s, v_num=3, train/loss_step=0.196, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|███████▉  | 435/547 [07:59<02:03,  0.91it/s, v_num=3, train/loss_step=0.0266, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|███████▉  | 436/547 [07:59<02:02,  0.91it/s, v_num=3, train/loss_step=0.0266, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|███████▉  | 436/547 [08:00<02:02,  0.91it/s, v_num=3, train/loss_step=0.179, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|███████▉  | 437/547 [08:00<02:01,  0.91it/s, v_num=3, train/loss_step=0.179, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|███████▉  | 437/547 [08:01<02:01,  0.91it/s, v_num=3, train/loss_step=0.0196, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|████████  | 438/547 [08:01<01:59,  0.91it/s, v_num=3, train/loss_step=0.0196, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|████████  | 438/547 [08:02<02:00,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|████████  | 439/547 [08:02<01:58,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|████████  | 439/547 [08:03<01:59,  0.91it/s, v_num=3, train/loss_step=0.0986, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|████████  | 440/547 [08:04<01:57,  0.91it/s, v_num=3, train/loss_step=0.0986, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  80%|████████  | 440/547 [08:04<01:57,  0.91it/s, v_num=3, train/loss_step=0.290, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 441/547 [08:05<01:56,  0.91it/s, v_num=3, train/loss_step=0.290, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 441/547 [08:06<01:56,  0.91it/s, v_num=3, train/loss_step=0.328, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 442/547 [08:06<01:55,  0.91it/s, v_num=3, train/loss_step=0.328, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 442/547 [08:07<01:55,  0.91it/s, v_num=3, train/loss_step=0.160, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 443/547 [08:07<01:54,  0.91it/s, v_num=3, train/loss_step=0.160, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 443/547 [08:08<01:54,  0.91it/s, v_num=3, train/loss_step=0.159, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 444/547 [08:08<01:53,  0.91it/s, v_num=3, train/loss_step=0.159, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████  | 444/547 [08:09<01:53,  0.91it/s, v_num=3, train/loss_step=0.0827, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████▏ | 445/547 [08:09<01:52,  0.91it/s, v_num=3, train/loss_step=0.0827, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  81%|████████▏ | 445/547 [08:10<01:52,  0.91it/s, v_num=3, train/loss_step=0.124, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 446/547 [08:10<01:51,  0.91it/s, v_num=3, train/loss_step=0.124, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 446/547 [08:11<01:51,  0.91it/s, v_num=3, train/loss_step=0.0405, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 447/547 [08:11<01:50,  0.91it/s, v_num=3, train/loss_step=0.0405, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 447/547 [08:12<01:50,  0.91it/s, v_num=3, train/loss_step=0.269, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 448/547 [08:12<01:48,  0.91it/s, v_num=3, train/loss_step=0.269, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 448/547 [08:13<01:49,  0.91it/s, v_num=3, train/loss_step=0.0922, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 449/547 [08:14<01:47,  0.91it/s, v_num=3, train/loss_step=0.0922, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 449/547 [08:14<01:48,  0.91it/s, v_num=3, train/loss_step=0.0577, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 450/547 [08:15<01:46,  0.91it/s, v_num=3, train/loss_step=0.0577, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 450/547 [08:16<01:46,  0.91it/s, v_num=3, train/loss_step=0.329, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 451/547 [08:16<01:45,  0.91it/s, v_num=3, train/loss_step=0.329, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  82%|████████▏ | 451/547 [08:17<01:45,  0.91it/s, v_num=3, train/loss_step=0.0712, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 452/547 [08:17<01:44,  0.91it/s, v_num=3, train/loss_step=0.0712, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 452/547 [08:18<01:44,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 453/547 [08:18<01:43,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 453/547 [08:19<01:43,  0.91it/s, v_num=3, train/loss_step=0.0155, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 454/547 [08:19<01:42,  0.91it/s, v_num=3, train/loss_step=0.0155, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 454/547 [08:20<01:42,  0.91it/s, v_num=3, train/loss_step=0.131, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 455/547 [08:20<01:41,  0.91it/s, v_num=3, train/loss_step=0.131, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 455/547 [08:21<01:41,  0.91it/s, v_num=3, train/loss_step=0.0228, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 456/547 [08:21<01:40,  0.91it/s, v_num=3, train/loss_step=0.0228, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  83%|████████▎ | 456/547 [08:22<01:40,  0.91it/s, v_num=3, train/loss_step=0.103, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▎ | 457/547 [08:22<01:39,  0.91it/s, v_num=3, train/loss_step=0.103, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▎ | 457/547 [08:23<01:39,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▎ | 458/547 [08:24<01:37,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▎ | 458/547 [08:24<01:38,  0.91it/s, v_num=3, train/loss_step=0.0381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 459/547 [08:25<01:36,  0.91it/s, v_num=3, train/loss_step=0.0381, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 459/547 [08:26<01:37,  0.91it/s, v_num=3, train/loss_step=0.0245, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 460/547 [08:26<01:35,  0.91it/s, v_num=3, train/loss_step=0.0245, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 460/547 [08:27<01:35,  0.91it/s, v_num=3, train/loss_step=0.527, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 461/547 [08:27<01:34,  0.91it/s, v_num=3, train/loss_step=0.527, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 461/547 [08:28<01:34,  0.91it/s, v_num=3, train/loss_step=0.0643, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 462/547 [08:28<01:33,  0.91it/s, v_num=3, train/loss_step=0.0643, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  84%|████████▍ | 462/547 [08:29<01:33,  0.91it/s, v_num=3, train/loss_step=0.144, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▍ | 463/547 [08:29<01:32,  0.91it/s, v_num=3, train/loss_step=0.144, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▍ | 463/547 [08:30<01:32,  0.91it/s, v_num=3, train/loss_step=0.0496, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▍ | 464/547 [08:30<01:31,  0.91it/s, v_num=3, train/loss_step=0.0496, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▍ | 464/547 [08:31<01:31,  0.91it/s, v_num=3, train/loss_step=0.216, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▌ | 465/547 [08:31<01:30,  0.91it/s, v_num=3, train/loss_step=0.216, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▌ | 465/547 [08:32<01:30,  0.91it/s, v_num=3, train/loss_step=0.0582, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▌ | 466/547 [08:32<01:29,  0.91it/s, v_num=3, train/loss_step=0.0582, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▌ | 466/547 [08:33<01:29,  0.91it/s, v_num=3, train/loss_step=0.0669, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▌ | 467/547 [08:34<01:28,  0.91it/s, v_num=3, train/loss_step=0.0669, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  85%|████████▌ | 467/547 [08:34<01:28,  0.91it/s, v_num=3, train/loss_step=0.266, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 468/547 [08:35<01:26,  0.91it/s, v_num=3, train/loss_step=0.266, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 468/547 [08:36<01:27,  0.91it/s, v_num=3, train/loss_step=0.0309, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 469/547 [08:36<01:25,  0.91it/s, v_num=3, train/loss_step=0.0309, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 469/547 [08:37<01:26,  0.91it/s, v_num=3, train/loss_step=0.062, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 470/547 [08:37<01:24,  0.91it/s, v_num=3, train/loss_step=0.062, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 470/547 [08:38<01:24,  0.91it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 471/547 [08:38<01:23,  0.91it/s, v_num=3, train/loss_step=0.118, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▌ | 471/547 [08:39<01:23,  0.91it/s, v_num=3, train/loss_step=0.0548, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▋ | 472/547 [08:39<01:22,  0.91it/s, v_num=3, train/loss_step=0.0548, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▋ | 472/547 [08:40<01:22,  0.91it/s, v_num=3, train/loss_step=0.0379, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▋ | 473/547 [08:40<01:21,  0.91it/s, v_num=3, train/loss_step=0.0379, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  86%|████████▋ | 473/547 [08:41<01:21,  0.91it/s, v_num=3, train/loss_step=0.301, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 474/547 [08:41<01:20,  0.91it/s, v_num=3, train/loss_step=0.301, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 474/547 [08:42<01:20,  0.91it/s, v_num=3, train/loss_step=0.0617, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 475/547 [08:42<01:19,  0.91it/s, v_num=3, train/loss_step=0.0617, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 475/547 [08:43<01:19,  0.91it/s, v_num=3, train/loss_step=0.224, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 476/547 [08:44<01:18,  0.91it/s, v_num=3, train/loss_step=0.224, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 476/547 [08:44<01:18,  0.91it/s, v_num=3, train/loss_step=0.113, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 477/547 [08:45<01:17,  0.91it/s, v_num=3, train/loss_step=0.113, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 477/547 [08:46<01:17,  0.91it/s, v_num=3, train/loss_step=0.107, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 478/547 [08:46<01:15,  0.91it/s, v_num=3, train/loss_step=0.107, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  87%|████████▋ | 478/547 [08:47<01:16,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 479/547 [08:47<01:14,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 479/547 [08:48<01:14,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 480/547 [08:48<01:13,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 480/547 [08:49<01:13,  0.91it/s, v_num=3, train/loss_step=0.0524, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 481/547 [08:49<01:12,  0.91it/s, v_num=3, train/loss_step=0.0524, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 481/547 [08:50<01:12,  0.91it/s, v_num=3, train/loss_step=0.417, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 482/547 [08:50<01:11,  0.91it/s, v_num=3, train/loss_step=0.417, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 482/547 [08:51<01:11,  0.91it/s, v_num=3, train/loss_step=0.218, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 483/547 [08:51<01:10,  0.91it/s, v_num=3, train/loss_step=0.218, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 483/547 [08:52<01:10,  0.91it/s, v_num=3, train/loss_step=0.0315, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 484/547 [08:52<01:09,  0.91it/s, v_num=3, train/loss_step=0.0315, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  88%|████████▊ | 484/547 [08:53<01:09,  0.91it/s, v_num=3, train/loss_step=0.049, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▊ | 485/547 [08:54<01:08,  0.91it/s, v_num=3, train/loss_step=0.049, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▊ | 485/547 [08:54<01:08,  0.91it/s, v_num=3, train/loss_step=0.229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 486/547 [08:55<01:07,  0.91it/s, v_num=3, train/loss_step=0.229, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 486/547 [08:56<01:07,  0.91it/s, v_num=3, train/loss_step=0.120, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 487/547 [08:56<01:06,  0.91it/s, v_num=3, train/loss_step=0.120, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 487/547 [08:57<01:06,  0.91it/s, v_num=3, train/loss_step=0.241, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 488/547 [08:57<01:04,  0.91it/s, v_num=3, train/loss_step=0.241, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 488/547 [08:58<01:05,  0.91it/s, v_num=3, train/loss_step=0.227, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 489/547 [08:58<01:03,  0.91it/s, v_num=3, train/loss_step=0.227, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  89%|████████▉ | 489/547 [08:59<01:03,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|████████▉ | 490/547 [08:59<01:02,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|████████▉ | 490/547 [09:00<01:02,  0.91it/s, v_num=3, train/loss_step=0.0866, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|████████▉ | 491/547 [09:00<01:01,  0.91it/s, v_num=3, train/loss_step=0.0866, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|████████▉ | 491/547 [09:01<01:01,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|████████▉ | 492/547 [09:01<01:00,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|████████▉ | 492/547 [09:02<01:00,  0.91it/s, v_num=3, train/loss_step=0.198, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|█████████ | 493/547 [09:03<00:59,  0.91it/s, v_num=3, train/loss_step=0.198, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|█████████ | 493/547 [09:03<00:59,  0.91it/s, v_num=3, train/loss_step=0.114, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|█████████ | 494/547 [09:04<00:58,  0.91it/s, v_num=3, train/loss_step=0.114, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|█████████ | 494/547 [09:04<00:58,  0.91it/s, v_num=3, train/loss_step=0.119, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|█████████ | 495/547 [09:05<00:57,  0.91it/s, v_num=3, train/loss_step=0.119, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  90%|█████████ | 495/547 [09:06<00:57,  0.91it/s, v_num=3, train/loss_step=0.444, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 496/547 [09:06<00:56,  0.91it/s, v_num=3, train/loss_step=0.444, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 496/547 [09:07<00:56,  0.91it/s, v_num=3, train/loss_step=0.0716, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 497/547 [09:07<00:55,  0.91it/s, v_num=3, train/loss_step=0.0716, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 497/547 [09:08<00:55,  0.91it/s, v_num=3, train/loss_step=0.389, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 498/547 [09:08<00:53,  0.91it/s, v_num=3, train/loss_step=0.389, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 498/547 [09:09<00:54,  0.91it/s, v_num=3, train/loss_step=0.155, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 499/547 [09:09<00:52,  0.91it/s, v_num=3, train/loss_step=0.155, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████ | 499/547 [09:10<00:52,  0.91it/s, v_num=3, train/loss_step=0.0829, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████▏| 500/547 [09:10<00:51,  0.91it/s, v_num=3, train/loss_step=0.0829, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  91%|█████████▏| 500/547 [09:11<00:51,  0.91it/s, v_num=3, train/loss_step=0.306, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 501/547 [09:11<00:50,  0.91it/s, v_num=3, train/loss_step=0.306, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 501/547 [09:12<00:50,  0.91it/s, v_num=3, train/loss_step=0.202, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 502/547 [09:13<00:49,  0.91it/s, v_num=3, train/loss_step=0.202, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 502/547 [09:13<00:49,  0.91it/s, v_num=3, train/loss_step=0.239, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 503/547 [09:14<00:48,  0.91it/s, v_num=3, train/loss_step=0.239, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 503/547 [09:14<00:48,  0.91it/s, v_num=3, train/loss_step=0.162, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 504/547 [09:15<00:47,  0.91it/s, v_num=3, train/loss_step=0.162, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 504/547 [09:16<00:47,  0.91it/s, v_num=3, train/loss_step=0.209, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 505/547 [09:16<00:46,  0.91it/s, v_num=3, train/loss_step=0.209, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  92%|█████████▏| 505/547 [09:17<00:46,  0.91it/s, v_num=3, train/loss_step=0.239, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 506/547 [09:17<00:45,  0.91it/s, v_num=3, train/loss_step=0.239, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 506/547 [09:18<00:45,  0.91it/s, v_num=3, train/loss_step=0.302, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 507/547 [09:18<00:44,  0.91it/s, v_num=3, train/loss_step=0.302, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 507/547 [09:19<00:44,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 508/547 [09:19<00:42,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 508/547 [09:20<00:43,  0.91it/s, v_num=3, train/loss_step=0.0495, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 509/547 [09:20<00:41,  0.91it/s, v_num=3, train/loss_step=0.0495, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 509/547 [09:21<00:41,  0.91it/s, v_num=3, train/loss_step=0.293, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 510/547 [09:21<00:40,  0.91it/s, v_num=3, train/loss_step=0.293, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 510/547 [09:22<00:40,  0.91it/s, v_num=3, train/loss_step=0.207, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 511/547 [09:23<00:39,  0.91it/s, v_num=3, train/loss_step=0.207, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  93%|█████████▎| 511/547 [09:23<00:39,  0.91it/s, v_num=3, train/loss_step=0.0858, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▎| 512/547 [09:24<00:38,  0.91it/s, v_num=3, train/loss_step=0.0858, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▎| 512/547 [09:25<00:38,  0.91it/s, v_num=3, train/loss_step=0.172, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 513/547 [09:25<00:37,  0.91it/s, v_num=3, train/loss_step=0.172, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 513/547 [09:26<00:37,  0.91it/s, v_num=3, train/loss_step=0.0603, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 514/547 [09:26<00:36,  0.91it/s, v_num=3, train/loss_step=0.0603, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 514/547 [09:27<00:36,  0.91it/s, v_num=3, train/loss_step=0.121, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 515/547 [09:27<00:35,  0.91it/s, v_num=3, train/loss_step=0.121, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 515/547 [09:28<00:35,  0.91it/s, v_num=3, train/loss_step=0.122, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 516/547 [09:28<00:34,  0.91it/s, v_num=3, train/loss_step=0.122, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  94%|█████████▍| 516/547 [09:29<00:34,  0.91it/s, v_num=3, train/loss_step=0.349, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▍| 517/547 [09:29<00:33,  0.91it/s, v_num=3, train/loss_step=0.349, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▍| 517/547 [09:30<00:33,  0.91it/s, v_num=3, train/loss_step=0.153, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▍| 518/547 [09:30<00:31,  0.91it/s, v_num=3, train/loss_step=0.153, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▍| 518/547 [09:31<00:32,  0.91it/s, v_num=3, train/loss_step=0.103, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▍| 519/547 [09:31<00:30,  0.91it/s, v_num=3, train/loss_step=0.103, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▍| 519/547 [09:32<00:30,  0.91it/s, v_num=3, train/loss_step=0.0714, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▌| 520/547 [09:33<00:29,  0.91it/s, v_num=3, train/loss_step=0.0714, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▌| 520/547 [09:33<00:29,  0.91it/s, v_num=3, train/loss_step=0.0541, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▌| 521/547 [09:34<00:28,  0.91it/s, v_num=3, train/loss_step=0.0541, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▌| 521/547 [09:35<00:28,  0.91it/s, v_num=3, train/loss_step=0.075, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▌| 522/547 [09:35<00:27,  0.91it/s, v_num=3, train/loss_step=0.075, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  95%|█████████▌| 522/547 [09:36<00:27,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 523/547 [09:36<00:26,  0.91it/s, v_num=3, train/loss_step=0.181, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 523/547 [09:37<00:26,  0.91it/s, v_num=3, train/loss_step=0.368, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 524/547 [09:37<00:25,  0.91it/s, v_num=3, train/loss_step=0.368, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 524/547 [09:38<00:25,  0.91it/s, v_num=3, train/loss_step=0.0775, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 525/547 [09:38<00:24,  0.91it/s, v_num=3, train/loss_step=0.0775, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 525/547 [09:39<00:24,  0.91it/s, v_num=3, train/loss_step=0.0487, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 526/547 [09:39<00:23,  0.91it/s, v_num=3, train/loss_step=0.0487, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▌| 526/547 [09:40<00:23,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▋| 527/547 [09:40<00:22,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  96%|█████████▋| 527/547 [09:41<00:22,  0.91it/s, v_num=3, train/loss_step=0.241, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 528/547 [09:42<00:20,  0.91it/s, v_num=3, train/loss_step=0.241, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 528/547 [09:42<00:20,  0.91it/s, v_num=3, train/loss_step=0.0566, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 529/547 [09:43<00:19,  0.91it/s, v_num=3, train/loss_step=0.0566, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 529/547 [09:43<00:19,  0.91it/s, v_num=3, train/loss_step=0.0105, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 530/547 [09:44<00:18,  0.91it/s, v_num=3, train/loss_step=0.0105, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 530/547 [09:45<00:18,  0.91it/s, v_num=3, train/loss_step=0.350, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 531/547 [09:45<00:17,  0.91it/s, v_num=3, train/loss_step=0.350, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 531/547 [09:46<00:17,  0.91it/s, v_num=3, train/loss_step=0.0929, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 532/547 [09:46<00:16,  0.91it/s, v_num=3, train/loss_step=0.0929, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 532/547 [09:47<00:16,  0.91it/s, v_num=3, train/loss_step=0.331, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 533/547 [09:47<00:15,  0.91it/s, v_num=3, train/loss_step=0.331, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  97%|█████████▋| 533/547 [09:48<00:15,  0.91it/s, v_num=3, train/loss_step=0.456, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 534/547 [09:48<00:14,  0.91it/s, v_num=3, train/loss_step=0.456, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 534/547 [09:49<00:14,  0.91it/s, v_num=3, train/loss_step=0.0217, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 535/547 [09:49<00:13,  0.91it/s, v_num=3, train/loss_step=0.0217, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 535/547 [09:50<00:13,  0.91it/s, v_num=3, train/loss_step=0.191, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 536/547 [09:50<00:12,  0.91it/s, v_num=3, train/loss_step=0.191, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 536/547 [09:51<00:12,  0.91it/s, v_num=3, train/loss_step=0.375, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 537/547 [09:52<00:11,  0.91it/s, v_num=3, train/loss_step=0.375, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 537/547 [09:52<00:11,  0.91it/s, v_num=3, train/loss_step=0.0201, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 538/547 [09:53<00:09,  0.91it/s, v_num=3, train/loss_step=0.0201, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  98%|█████████▊| 538/547 [09:54<00:09,  0.91it/s, v_num=3, train/loss_step=0.0275, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▊| 539/547 [09:54<00:08,  0.91it/s, v_num=3, train/loss_step=0.0275, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▊| 539/547 [09:55<00:08,  0.91it/s, v_num=3, train/loss_step=0.290, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▊| 540/547 [09:55<00:07,  0.91it/s, v_num=3, train/loss_step=0.290, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▊| 540/547 [09:56<00:07,  0.91it/s, v_num=3, train/loss_step=0.0277, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 541/547 [09:56<00:06,  0.91it/s, v_num=3, train/loss_step=0.0277, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 541/547 [09:57<00:06,  0.91it/s, v_num=3, train/loss_step=0.309, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 542/547 [09:57<00:05,  0.91it/s, v_num=3, train/loss_step=0.309, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 542/547 [09:58<00:05,  0.91it/s, v_num=3, train/loss_step=0.509, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 543/547 [09:58<00:04,  0.91it/s, v_num=3, train/loss_step=0.509, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 543/547 [09:59<00:04,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 544/547 [09:59<00:03,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1:  99%|█████████▉| 544/547 [10:00<00:03,  0.91it/s, v_num=3, train/loss_step=0.0724, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1: 100%|█████████▉| 545/547 [10:00<00:02,  0.91it/s, v_num=3, train/loss_step=0.0724, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1: 100%|█████████▉| 545/547 [10:01<00:02,  0.91it/s, v_num=3, train/loss_step=0.0876, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1: 100%|█████████▉| 546/547 [10:02<00:01,  0.91it/s, v_num=3, train/loss_step=0.0876, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1: 100%|█████████▉| 546/547 [10:02<00:01,  0.91it/s, v_num=3, train/loss_step=0.511, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1: 100%|██████████| 547/547 [10:03<00:00,  0.91it/s, v_num=3, train/loss_step=0.511, val/loss=0.246, train/loss_epoch=0.380]\n",
      "Epoch 1: 100%|██████████| 547/547 [10:03<00:00,  0.91it/s, v_num=3, train/loss_step=0.144, val/loss=0.246, train/loss_epoch=0.380]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/118 [00:00<00:16,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/118 [00:00<00:16,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/118 [00:00<00:16,  6.90it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 4/118 [00:00<00:16,  6.91it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   4%|▍         | 5/118 [00:00<00:16,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/118 [00:00<00:16,  6.93it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   6%|▌         | 7/118 [00:01<00:16,  6.93it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 10/118 [00:01<00:15,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 11/118 [00:01<00:15,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 12/118 [00:01<00:15,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 13/118 [00:01<00:15,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 14/118 [00:02<00:14,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 15/118 [00:02<00:14,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 16/118 [00:02<00:14,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 17/118 [00:02<00:14,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 18/118 [00:02<00:14,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  16%|█▌        | 19/118 [00:02<00:14,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 20/118 [00:02<00:14,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 21/118 [00:03<00:13,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  19%|█▊        | 22/118 [00:03<00:13,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 23/118 [00:03<00:13,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 24/118 [00:03<00:13,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 25/118 [00:03<00:13,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 26/118 [00:03<00:13,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 27/118 [00:03<00:13,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 28/118 [00:04<00:12,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 29/118 [00:04<00:12,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 30/118 [00:04<00:12,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 31/118 [00:04<00:12,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 32/118 [00:04<00:12,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 33/118 [00:04<00:12,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 34/118 [00:04<00:12,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  30%|██▉       | 35/118 [00:05<00:11,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 36/118 [00:05<00:11,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  31%|███▏      | 37/118 [00:05<00:11,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 38/118 [00:05<00:11,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 39/118 [00:05<00:11,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  34%|███▍      | 40/118 [00:05<00:11,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 41/118 [00:05<00:11,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 42/118 [00:06<00:10,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 43/118 [00:06<00:10,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 44/118 [00:06<00:10,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 45/118 [00:06<00:10,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 46/118 [00:06<00:10,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  40%|███▉      | 47/118 [00:06<00:10,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 48/118 [00:06<00:10,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 49/118 [00:07<00:09,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 50/118 [00:07<00:09,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 51/118 [00:07<00:09,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  44%|████▍     | 52/118 [00:07<00:09,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 53/118 [00:07<00:09,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  46%|████▌     | 54/118 [00:07<00:09,  6.98it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 55/118 [00:07<00:09,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 56/118 [00:08<00:08,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 57/118 [00:08<00:08,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 58/118 [00:08<00:08,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 59/118 [00:08<00:08,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 60/118 [00:08<00:08,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 61/118 [00:08<00:08,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 62/118 [00:08<00:08,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 63/118 [00:09<00:07,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▍    | 64/118 [00:09<00:07,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 65/118 [00:09<00:07,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▌    | 66/118 [00:09<00:07,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 67/118 [00:09<00:07,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 68/118 [00:09<00:07,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 69/118 [00:09<00:07,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 70/118 [00:10<00:06,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 71/118 [00:10<00:06,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 72/118 [00:10<00:06,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 73/118 [00:10<00:06,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 74/118 [00:10<00:06,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 75/118 [00:10<00:06,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 76/118 [00:10<00:06,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 77/118 [00:11<00:05,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▌   | 78/118 [00:11<00:05,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 79/118 [00:11<00:05,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 80/118 [00:11<00:05,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▊   | 81/118 [00:11<00:05,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 82/118 [00:11<00:05,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 83/118 [00:11<00:05,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 84/118 [00:12<00:04,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 85/118 [00:12<00:04,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 86/118 [00:12<00:04,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 87/118 [00:12<00:04,  6.97it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 88/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 89/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 90/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 91/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 92/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 93/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  80%|███████▉  | 94/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 95/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  81%|████████▏ | 96/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 97/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 98/118 [00:14<00:02,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▍ | 99/118 [00:14<00:02,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 100/118 [00:14<00:02,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 101/118 [00:14<00:02,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 102/118 [00:14<00:02,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 103/118 [00:14<00:02,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 104/118 [00:14<00:02,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 105/118 [00:15<00:01,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  90%|████████▉ | 106/118 [00:15<00:01,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 107/118 [00:15<00:01,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 108/118 [00:15<00:01,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 109/118 [00:15<00:01,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 110/118 [00:15<00:01,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▍| 111/118 [00:15<00:01,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 112/118 [00:16<00:00,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▌| 113/118 [00:16<00:00,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 114/118 [00:16<00:00,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 115/118 [00:16<00:00,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 116/118 [00:16<00:00,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 117/118 [00:16<00:00,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 118/118 [00:16<00:00,  7.00it/s]\u001b[A\n",
      "\n",
      "                                                                          \u001b[A\n",
      "Epoch 1: 100%|██████████| 547/547 [10:42<00:00,  0.85it/s, v_num=3, train/loss_step=0.144, val/loss=0.206, train/loss_epoch=0.380]\n",
      "Epoch 1: 100%|██████████| 547/547 [10:42<00:00,  0.85it/s, v_num=3, train/loss_step=0.144, val/loss=0.206, train/loss_epoch=0.174][rank: 0] Metric val/loss improved by 0.040 >= min_delta = 0.0. New best score: 0.206\n",
      "[rank: 1] Metric val/loss improved by 0.022 >= min_delta = 0.0. New best score: 0.198\n",
      "[rank: 2] Metric val/loss improved by 0.031 >= min_delta = 0.0. New best score: 0.188\n",
      "[rank: 3] Metric val/loss improved by 0.021 >= min_delta = 0.0. New best score: 0.199\n",
      "Epoch 1, global step 1094: 'val/loss' reached 0.20605 (best 0.20605), saving model to '/home/eaguayo/workspace/DeepLearning/Week3/bert-classifier/sentiment_checkpoints/best-checkpoint-epoch=01-val/loss=0.21.ckpt' as top 1\n",
      "\n",
      "Epoch 1:   0%|          | 0/547 [00:00<?, ?it/s, v_num=3, train/loss_step=0.144, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   0%|          | 0/547 [00:00<?, ?it/s, v_num=3, train/loss_step=0.144, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   0%|          | 1/547 [00:00<03:37,  2.51it/s, v_num=3, train/loss_step=0.144, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   0%|          | 1/547 [00:01<11:02,  0.82it/s, v_num=3, train/loss_step=0.0445, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   0%|          | 2/547 [00:01<06:35,  1.38it/s, v_num=3, train/loss_step=0.0445, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   0%|          | 2/547 [00:02<10:29,  0.87it/s, v_num=3, train/loss_step=0.156, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 3/547 [00:02<07:42,  1.18it/s, v_num=3, train/loss_step=0.156, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 3/547 [00:03<10:17,  0.88it/s, v_num=3, train/loss_step=0.0413, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 4/547 [00:03<08:15,  1.10it/s, v_num=3, train/loss_step=0.0413, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 4/547 [00:04<10:11,  0.89it/s, v_num=3, train/loss_step=0.0272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 5/547 [00:04<08:34,  1.05it/s, v_num=3, train/loss_step=0.0272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 5/547 [00:05<10:08,  0.89it/s, v_num=3, train/loss_step=0.207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 6/547 [00:05<08:47,  1.03it/s, v_num=3, train/loss_step=0.207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|          | 6/547 [00:06<10:04,  0.90it/s, v_num=3, train/loss_step=0.0622, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|▏         | 7/547 [00:06<08:55,  1.01it/s, v_num=3, train/loss_step=0.0622, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|▏         | 7/547 [00:07<10:01,  0.90it/s, v_num=3, train/loss_step=0.0742, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|▏         | 8/547 [00:08<09:01,  0.99it/s, v_num=3, train/loss_step=0.0742, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   1%|▏         | 8/547 [00:08<09:59,  0.90it/s, v_num=3, train/loss_step=0.0302, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 9/547 [00:09<09:06,  0.98it/s, v_num=3, train/loss_step=0.0302, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 9/547 [00:10<09:58,  0.90it/s, v_num=3, train/loss_step=0.282, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 10/547 [00:10<09:10,  0.98it/s, v_num=3, train/loss_step=0.282, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 10/547 [00:11<09:56,  0.90it/s, v_num=3, train/loss_step=0.0498, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 11/547 [00:11<09:12,  0.97it/s, v_num=3, train/loss_step=0.0498, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 11/547 [00:12<09:54,  0.90it/s, v_num=3, train/loss_step=0.0177, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 12/547 [00:12<09:14,  0.97it/s, v_num=3, train/loss_step=0.0177, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 12/547 [00:13<09:52,  0.90it/s, v_num=3, train/loss_step=0.0148, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 13/547 [00:13<09:16,  0.96it/s, v_num=3, train/loss_step=0.0148, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   2%|▏         | 13/547 [00:14<09:51,  0.90it/s, v_num=3, train/loss_step=0.464, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 14/547 [00:14<09:17,  0.96it/s, v_num=3, train/loss_step=0.464, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 14/547 [00:15<09:49,  0.90it/s, v_num=3, train/loss_step=0.131, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 15/547 [00:15<09:18,  0.95it/s, v_num=3, train/loss_step=0.131, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 15/547 [00:16<09:48,  0.90it/s, v_num=3, train/loss_step=0.0187, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 16/547 [00:16<09:18,  0.95it/s, v_num=3, train/loss_step=0.0187, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 16/547 [00:17<09:47,  0.90it/s, v_num=3, train/loss_step=0.366, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 17/547 [00:17<09:19,  0.95it/s, v_num=3, train/loss_step=0.366, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 17/547 [00:18<09:45,  0.91it/s, v_num=3, train/loss_step=0.0463, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 18/547 [00:19<09:19,  0.95it/s, v_num=3, train/loss_step=0.0463, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 18/547 [00:19<09:44,  0.91it/s, v_num=3, train/loss_step=0.0418, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 19/547 [00:20<09:19,  0.94it/s, v_num=3, train/loss_step=0.0418, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   3%|▎         | 19/547 [00:20<09:42,  0.91it/s, v_num=3, train/loss_step=0.337, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▎         | 20/547 [00:21<09:19,  0.94it/s, v_num=3, train/loss_step=0.337, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▎         | 20/547 [00:22<09:41,  0.91it/s, v_num=3, train/loss_step=0.00614, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 21/547 [00:22<09:18,  0.94it/s, v_num=3, train/loss_step=0.00614, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 21/547 [00:23<09:40,  0.91it/s, v_num=3, train/loss_step=0.0412, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 22/547 [00:23<09:18,  0.94it/s, v_num=3, train/loss_step=0.0412, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 22/547 [00:24<09:39,  0.91it/s, v_num=3, train/loss_step=0.0195, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 23/547 [00:24<09:18,  0.94it/s, v_num=3, train/loss_step=0.0195, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 23/547 [00:25<09:38,  0.91it/s, v_num=3, train/loss_step=0.00782, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 24/547 [00:25<09:18,  0.94it/s, v_num=3, train/loss_step=0.00782, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   4%|▍         | 24/547 [00:26<09:36,  0.91it/s, v_num=3, train/loss_step=0.023, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▍         | 25/547 [00:26<09:17,  0.94it/s, v_num=3, train/loss_step=0.023, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▍         | 25/547 [00:27<09:35,  0.91it/s, v_num=3, train/loss_step=0.127, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▍         | 26/547 [00:27<09:17,  0.94it/s, v_num=3, train/loss_step=0.127, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▍         | 26/547 [00:28<09:34,  0.91it/s, v_num=3, train/loss_step=0.026, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▍         | 27/547 [00:28<09:16,  0.93it/s, v_num=3, train/loss_step=0.026, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▍         | 27/547 [00:29<09:32,  0.91it/s, v_num=3, train/loss_step=0.123, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▌         | 28/547 [00:29<09:15,  0.93it/s, v_num=3, train/loss_step=0.123, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▌         | 28/547 [00:30<09:31,  0.91it/s, v_num=3, train/loss_step=0.0353, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▌         | 29/547 [00:31<09:15,  0.93it/s, v_num=3, train/loss_step=0.0353, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▌         | 29/547 [00:31<09:30,  0.91it/s, v_num=3, train/loss_step=0.0112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▌         | 30/547 [00:32<09:14,  0.93it/s, v_num=3, train/loss_step=0.0112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   5%|▌         | 30/547 [00:33<09:29,  0.91it/s, v_num=3, train/loss_step=0.0129, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 31/547 [00:33<09:13,  0.93it/s, v_num=3, train/loss_step=0.0129, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 31/547 [00:34<09:28,  0.91it/s, v_num=3, train/loss_step=0.0624, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 32/547 [00:34<09:13,  0.93it/s, v_num=3, train/loss_step=0.0624, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 32/547 [00:35<09:27,  0.91it/s, v_num=3, train/loss_step=0.124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 33/547 [00:35<09:12,  0.93it/s, v_num=3, train/loss_step=0.124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 33/547 [00:36<09:26,  0.91it/s, v_num=3, train/loss_step=0.018, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 34/547 [00:36<09:12,  0.93it/s, v_num=3, train/loss_step=0.018, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▌         | 34/547 [00:37<09:24,  0.91it/s, v_num=3, train/loss_step=0.149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▋         | 35/547 [00:37<09:11,  0.93it/s, v_num=3, train/loss_step=0.149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   6%|▋         | 35/547 [00:38<09:23,  0.91it/s, v_num=3, train/loss_step=0.0127, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 36/547 [00:38<09:10,  0.93it/s, v_num=3, train/loss_step=0.0127, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 36/547 [00:39<09:22,  0.91it/s, v_num=3, train/loss_step=0.135, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 37/547 [00:39<09:09,  0.93it/s, v_num=3, train/loss_step=0.135, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 37/547 [00:40<09:21,  0.91it/s, v_num=3, train/loss_step=0.0137, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 38/547 [00:40<09:09,  0.93it/s, v_num=3, train/loss_step=0.0137, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 38/547 [00:41<09:20,  0.91it/s, v_num=3, train/loss_step=0.0106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 39/547 [00:42<09:08,  0.93it/s, v_num=3, train/loss_step=0.0106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 39/547 [00:42<09:19,  0.91it/s, v_num=3, train/loss_step=0.0494, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 40/547 [00:43<09:07,  0.93it/s, v_num=3, train/loss_step=0.0494, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 40/547 [00:44<09:18,  0.91it/s, v_num=3, train/loss_step=0.00675, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 41/547 [00:44<09:06,  0.93it/s, v_num=3, train/loss_step=0.00675, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   7%|▋         | 41/547 [00:45<09:17,  0.91it/s, v_num=3, train/loss_step=0.211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 42/547 [00:45<09:05,  0.93it/s, v_num=3, train/loss_step=0.211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 42/547 [00:46<09:16,  0.91it/s, v_num=3, train/loss_step=0.0218, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 43/547 [00:46<09:04,  0.92it/s, v_num=3, train/loss_step=0.0218, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 43/547 [00:47<09:14,  0.91it/s, v_num=3, train/loss_step=0.198, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 44/547 [00:47<09:04,  0.92it/s, v_num=3, train/loss_step=0.198, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 44/547 [00:48<09:13,  0.91it/s, v_num=3, train/loss_step=0.109, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 45/547 [00:48<09:03,  0.92it/s, v_num=3, train/loss_step=0.109, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 45/547 [00:49<09:12,  0.91it/s, v_num=3, train/loss_step=0.0593, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 46/547 [00:49<09:02,  0.92it/s, v_num=3, train/loss_step=0.0593, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   8%|▊         | 46/547 [00:50<09:11,  0.91it/s, v_num=3, train/loss_step=0.0352, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▊         | 47/547 [00:50<09:01,  0.92it/s, v_num=3, train/loss_step=0.0352, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▊         | 47/547 [00:51<09:10,  0.91it/s, v_num=3, train/loss_step=0.0276, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 48/547 [00:51<09:00,  0.92it/s, v_num=3, train/loss_step=0.0276, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 48/547 [00:52<09:09,  0.91it/s, v_num=3, train/loss_step=0.150, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 49/547 [00:53<08:59,  0.92it/s, v_num=3, train/loss_step=0.150, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 49/547 [00:53<09:08,  0.91it/s, v_num=3, train/loss_step=0.0382, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 50/547 [00:54<08:58,  0.92it/s, v_num=3, train/loss_step=0.0382, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 50/547 [00:55<09:07,  0.91it/s, v_num=3, train/loss_step=0.0419, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 51/547 [00:55<08:57,  0.92it/s, v_num=3, train/loss_step=0.0419, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:   9%|▉         | 51/547 [00:56<09:06,  0.91it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|▉         | 52/547 [00:56<08:56,  0.92it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|▉         | 52/547 [00:57<09:05,  0.91it/s, v_num=3, train/loss_step=0.0586, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|▉         | 53/547 [00:57<08:56,  0.92it/s, v_num=3, train/loss_step=0.0586, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|▉         | 53/547 [00:58<09:03,  0.91it/s, v_num=3, train/loss_step=0.0609, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|▉         | 54/547 [00:58<08:55,  0.92it/s, v_num=3, train/loss_step=0.0609, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|▉         | 54/547 [00:59<09:02,  0.91it/s, v_num=3, train/loss_step=0.269, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|█         | 55/547 [00:59<08:53,  0.92it/s, v_num=3, train/loss_step=0.269, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|█         | 55/547 [01:00<09:01,  0.91it/s, v_num=3, train/loss_step=0.176, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|█         | 56/547 [01:00<08:53,  0.92it/s, v_num=3, train/loss_step=0.176, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|█         | 56/547 [01:01<09:00,  0.91it/s, v_num=3, train/loss_step=0.119, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|█         | 57/547 [01:01<08:52,  0.92it/s, v_num=3, train/loss_step=0.119, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  10%|█         | 57/547 [01:02<08:59,  0.91it/s, v_num=3, train/loss_step=0.643, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 58/547 [01:03<08:51,  0.92it/s, v_num=3, train/loss_step=0.643, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 58/547 [01:03<08:58,  0.91it/s, v_num=3, train/loss_step=0.00951, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 59/547 [01:04<08:50,  0.92it/s, v_num=3, train/loss_step=0.00951, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 59/547 [01:04<08:57,  0.91it/s, v_num=3, train/loss_step=0.0477, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 60/547 [01:05<08:49,  0.92it/s, v_num=3, train/loss_step=0.0477, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 60/547 [01:06<08:56,  0.91it/s, v_num=3, train/loss_step=0.00728, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 61/547 [01:06<08:48,  0.92it/s, v_num=3, train/loss_step=0.00728, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█         | 61/547 [01:07<08:55,  0.91it/s, v_num=3, train/loss_step=0.00519, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█▏        | 62/547 [01:07<08:47,  0.92it/s, v_num=3, train/loss_step=0.00519, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  11%|█▏        | 62/547 [01:08<08:53,  0.91it/s, v_num=3, train/loss_step=0.171, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 63/547 [01:08<08:46,  0.92it/s, v_num=3, train/loss_step=0.171, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 63/547 [01:09<08:52,  0.91it/s, v_num=3, train/loss_step=0.0292, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 64/547 [01:09<08:45,  0.92it/s, v_num=3, train/loss_step=0.0292, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 64/547 [01:10<08:51,  0.91it/s, v_num=3, train/loss_step=0.0539, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 65/547 [01:10<08:44,  0.92it/s, v_num=3, train/loss_step=0.0539, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 65/547 [01:11<08:50,  0.91it/s, v_num=3, train/loss_step=0.234, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 66/547 [01:11<08:43,  0.92it/s, v_num=3, train/loss_step=0.234, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 66/547 [01:12<08:49,  0.91it/s, v_num=3, train/loss_step=0.00599, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 67/547 [01:12<08:42,  0.92it/s, v_num=3, train/loss_step=0.00599, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 67/547 [01:13<08:48,  0.91it/s, v_num=3, train/loss_step=0.0116, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 68/547 [01:13<08:41,  0.92it/s, v_num=3, train/loss_step=0.0116, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  12%|█▏        | 68/547 [01:14<08:47,  0.91it/s, v_num=3, train/loss_step=0.0782, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 69/547 [01:15<08:40,  0.92it/s, v_num=3, train/loss_step=0.0782, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 69/547 [01:15<08:46,  0.91it/s, v_num=3, train/loss_step=0.0261, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 70/547 [01:16<08:39,  0.92it/s, v_num=3, train/loss_step=0.0261, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 70/547 [01:17<08:45,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 71/547 [01:17<08:38,  0.92it/s, v_num=3, train/loss_step=0.112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 71/547 [01:18<08:43,  0.91it/s, v_num=3, train/loss_step=0.0818, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 72/547 [01:18<08:37,  0.92it/s, v_num=3, train/loss_step=0.0818, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 72/547 [01:19<08:42,  0.91it/s, v_num=3, train/loss_step=0.00675, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 73/547 [01:19<08:36,  0.92it/s, v_num=3, train/loss_step=0.00675, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  13%|█▎        | 73/547 [01:20<08:41,  0.91it/s, v_num=3, train/loss_step=0.0304, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▎        | 74/547 [01:20<08:35,  0.92it/s, v_num=3, train/loss_step=0.0304, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▎        | 74/547 [01:21<08:40,  0.91it/s, v_num=3, train/loss_step=0.0181, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▎        | 75/547 [01:21<08:34,  0.92it/s, v_num=3, train/loss_step=0.0181, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▎        | 75/547 [01:22<08:39,  0.91it/s, v_num=3, train/loss_step=0.0698, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 76/547 [01:22<08:33,  0.92it/s, v_num=3, train/loss_step=0.0698, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 76/547 [01:23<08:38,  0.91it/s, v_num=3, train/loss_step=0.0164, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 77/547 [01:23<08:32,  0.92it/s, v_num=3, train/loss_step=0.0164, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 77/547 [01:24<08:37,  0.91it/s, v_num=3, train/loss_step=0.0649, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 78/547 [01:24<08:31,  0.92it/s, v_num=3, train/loss_step=0.0649, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 78/547 [01:25<08:36,  0.91it/s, v_num=3, train/loss_step=0.00448, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 79/547 [01:26<08:30,  0.92it/s, v_num=3, train/loss_step=0.00448, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  14%|█▍        | 79/547 [01:26<08:35,  0.91it/s, v_num=3, train/loss_step=0.280, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▍        | 80/547 [01:27<08:28,  0.92it/s, v_num=3, train/loss_step=0.280, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▍        | 80/547 [01:28<08:33,  0.91it/s, v_num=3, train/loss_step=0.0168, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▍        | 81/547 [01:28<08:27,  0.92it/s, v_num=3, train/loss_step=0.0168, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▍        | 81/547 [01:29<08:32,  0.91it/s, v_num=3, train/loss_step=0.286, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▍        | 82/547 [01:29<08:26,  0.92it/s, v_num=3, train/loss_step=0.286, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▍        | 82/547 [01:30<08:31,  0.91it/s, v_num=3, train/loss_step=0.00996, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▌        | 83/547 [01:30<08:25,  0.92it/s, v_num=3, train/loss_step=0.00996, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▌        | 83/547 [01:31<08:30,  0.91it/s, v_num=3, train/loss_step=0.147, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▌        | 84/547 [01:31<08:24,  0.92it/s, v_num=3, train/loss_step=0.147, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  15%|█▌        | 84/547 [01:32<08:29,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 85/547 [01:32<08:23,  0.92it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 85/547 [01:33<08:28,  0.91it/s, v_num=3, train/loss_step=0.00639, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 86/547 [01:33<08:22,  0.92it/s, v_num=3, train/loss_step=0.00639, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 86/547 [01:34<08:27,  0.91it/s, v_num=3, train/loss_step=0.00896, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 87/547 [01:34<08:21,  0.92it/s, v_num=3, train/loss_step=0.00896, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 87/547 [01:35<08:26,  0.91it/s, v_num=3, train/loss_step=0.0275, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 88/547 [01:35<08:20,  0.92it/s, v_num=3, train/loss_step=0.0275, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▌        | 88/547 [01:36<08:25,  0.91it/s, v_num=3, train/loss_step=0.146, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▋        | 89/547 [01:37<08:19,  0.92it/s, v_num=3, train/loss_step=0.146, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▋        | 89/547 [01:37<08:23,  0.91it/s, v_num=3, train/loss_step=0.0315, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▋        | 90/547 [01:38<08:18,  0.92it/s, v_num=3, train/loss_step=0.0315, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  16%|█▋        | 90/547 [01:39<08:22,  0.91it/s, v_num=3, train/loss_step=0.0112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 91/547 [01:39<08:17,  0.92it/s, v_num=3, train/loss_step=0.0112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 91/547 [01:40<08:21,  0.91it/s, v_num=3, train/loss_step=0.00443, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 92/547 [01:40<08:16,  0.92it/s, v_num=3, train/loss_step=0.00443, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 92/547 [01:41<08:20,  0.91it/s, v_num=3, train/loss_step=0.283, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 93/547 [01:41<08:15,  0.92it/s, v_num=3, train/loss_step=0.283, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 93/547 [01:42<08:19,  0.91it/s, v_num=3, train/loss_step=0.0121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 94/547 [01:42<08:14,  0.92it/s, v_num=3, train/loss_step=0.0121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 94/547 [01:43<08:18,  0.91it/s, v_num=3, train/loss_step=0.207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 95/547 [01:43<08:13,  0.92it/s, v_num=3, train/loss_step=0.207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  17%|█▋        | 95/547 [01:44<08:17,  0.91it/s, v_num=3, train/loss_step=0.0713, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 96/547 [01:44<08:12,  0.92it/s, v_num=3, train/loss_step=0.0713, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 96/547 [01:45<08:16,  0.91it/s, v_num=3, train/loss_step=0.0845, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 97/547 [01:45<08:11,  0.92it/s, v_num=3, train/loss_step=0.0845, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 97/547 [01:46<08:15,  0.91it/s, v_num=3, train/loss_step=0.183, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 98/547 [01:46<08:10,  0.92it/s, v_num=3, train/loss_step=0.183, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 98/547 [01:47<08:14,  0.91it/s, v_num=3, train/loss_step=0.203, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 99/547 [01:48<08:09,  0.92it/s, v_num=3, train/loss_step=0.203, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 99/547 [01:48<08:12,  0.91it/s, v_num=3, train/loss_step=0.0105, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 100/547 [01:49<08:08,  0.92it/s, v_num=3, train/loss_step=0.0105, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 100/547 [01:50<08:11,  0.91it/s, v_num=3, train/loss_step=0.217, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 101/547 [01:50<08:06,  0.92it/s, v_num=3, train/loss_step=0.217, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  18%|█▊        | 101/547 [01:51<08:10,  0.91it/s, v_num=3, train/loss_step=0.258, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▊        | 102/547 [01:51<08:05,  0.92it/s, v_num=3, train/loss_step=0.258, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▊        | 102/547 [01:52<08:09,  0.91it/s, v_num=3, train/loss_step=0.0923, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 103/547 [01:52<08:04,  0.92it/s, v_num=3, train/loss_step=0.0923, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 103/547 [01:53<08:08,  0.91it/s, v_num=3, train/loss_step=0.289, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 104/547 [01:53<08:03,  0.92it/s, v_num=3, train/loss_step=0.289, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 104/547 [01:54<08:07,  0.91it/s, v_num=3, train/loss_step=0.229, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 105/547 [01:54<08:02,  0.92it/s, v_num=3, train/loss_step=0.229, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 105/547 [01:55<08:06,  0.91it/s, v_num=3, train/loss_step=0.0301, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 106/547 [01:55<08:01,  0.92it/s, v_num=3, train/loss_step=0.0301, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  19%|█▉        | 106/547 [01:56<08:05,  0.91it/s, v_num=3, train/loss_step=0.0207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|█▉        | 107/547 [01:56<08:00,  0.92it/s, v_num=3, train/loss_step=0.0207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|█▉        | 107/547 [01:57<08:04,  0.91it/s, v_num=3, train/loss_step=0.0145, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|█▉        | 108/547 [01:57<07:59,  0.92it/s, v_num=3, train/loss_step=0.0145, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|█▉        | 108/547 [01:58<08:03,  0.91it/s, v_num=3, train/loss_step=0.161, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|█▉        | 109/547 [01:59<07:58,  0.92it/s, v_num=3, train/loss_step=0.161, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|█▉        | 109/547 [01:59<08:01,  0.91it/s, v_num=3, train/loss_step=0.0263, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|██        | 110/547 [02:00<07:57,  0.92it/s, v_num=3, train/loss_step=0.0263, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|██        | 110/547 [02:01<08:00,  0.91it/s, v_num=3, train/loss_step=0.033, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|██        | 111/547 [02:01<07:56,  0.92it/s, v_num=3, train/loss_step=0.033, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|██        | 111/547 [02:02<07:59,  0.91it/s, v_num=3, train/loss_step=0.063, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|██        | 112/547 [02:02<07:55,  0.92it/s, v_num=3, train/loss_step=0.063, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  20%|██        | 112/547 [02:03<07:58,  0.91it/s, v_num=3, train/loss_step=0.257, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 113/547 [02:03<07:54,  0.92it/s, v_num=3, train/loss_step=0.257, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 113/547 [02:04<07:57,  0.91it/s, v_num=3, train/loss_step=0.0272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 114/547 [02:04<07:53,  0.92it/s, v_num=3, train/loss_step=0.0272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 114/547 [02:05<07:56,  0.91it/s, v_num=3, train/loss_step=0.030, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 115/547 [02:05<07:52,  0.92it/s, v_num=3, train/loss_step=0.030, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 115/547 [02:06<07:55,  0.91it/s, v_num=3, train/loss_step=0.0337, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 116/547 [02:06<07:51,  0.91it/s, v_num=3, train/loss_step=0.0337, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██        | 116/547 [02:07<07:54,  0.91it/s, v_num=3, train/loss_step=0.148, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██▏       | 117/547 [02:07<07:49,  0.91it/s, v_num=3, train/loss_step=0.148, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  21%|██▏       | 117/547 [02:08<07:53,  0.91it/s, v_num=3, train/loss_step=0.272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 118/547 [02:08<07:48,  0.91it/s, v_num=3, train/loss_step=0.272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 118/547 [02:09<07:52,  0.91it/s, v_num=3, train/loss_step=0.0265, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 119/547 [02:10<07:47,  0.91it/s, v_num=3, train/loss_step=0.0265, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 119/547 [02:10<07:50,  0.91it/s, v_num=3, train/loss_step=0.0598, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 120/547 [02:11<07:46,  0.91it/s, v_num=3, train/loss_step=0.0598, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 120/547 [02:12<07:49,  0.91it/s, v_num=3, train/loss_step=0.0444, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 121/547 [02:12<07:45,  0.91it/s, v_num=3, train/loss_step=0.0444, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 121/547 [02:13<07:48,  0.91it/s, v_num=3, train/loss_step=0.211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 122/547 [02:13<07:44,  0.91it/s, v_num=3, train/loss_step=0.211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 122/547 [02:14<07:47,  0.91it/s, v_num=3, train/loss_step=0.0113, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 123/547 [02:14<07:43,  0.91it/s, v_num=3, train/loss_step=0.0113, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  22%|██▏       | 123/547 [02:15<07:46,  0.91it/s, v_num=3, train/loss_step=0.0429, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 124/547 [02:15<07:42,  0.91it/s, v_num=3, train/loss_step=0.0429, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 124/547 [02:16<07:45,  0.91it/s, v_num=3, train/loss_step=0.0499, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 125/547 [02:16<07:41,  0.91it/s, v_num=3, train/loss_step=0.0499, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 125/547 [02:17<07:44,  0.91it/s, v_num=3, train/loss_step=0.0816, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 126/547 [02:17<07:40,  0.91it/s, v_num=3, train/loss_step=0.0816, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 126/547 [02:18<07:43,  0.91it/s, v_num=3, train/loss_step=0.0138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 127/547 [02:18<07:39,  0.91it/s, v_num=3, train/loss_step=0.0138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 127/547 [02:19<07:42,  0.91it/s, v_num=3, train/loss_step=0.343, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 128/547 [02:19<07:38,  0.91it/s, v_num=3, train/loss_step=0.343, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  23%|██▎       | 128/547 [02:20<07:41,  0.91it/s, v_num=3, train/loss_step=0.00957, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▎       | 129/547 [02:21<07:37,  0.91it/s, v_num=3, train/loss_step=0.00957, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▎       | 129/547 [02:21<07:39,  0.91it/s, v_num=3, train/loss_step=0.00546, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 130/547 [02:22<07:36,  0.91it/s, v_num=3, train/loss_step=0.00546, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 130/547 [02:23<07:38,  0.91it/s, v_num=3, train/loss_step=0.00634, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 131/547 [02:23<07:34,  0.91it/s, v_num=3, train/loss_step=0.00634, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 131/547 [02:24<07:37,  0.91it/s, v_num=3, train/loss_step=0.0798, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 132/547 [02:24<07:33,  0.91it/s, v_num=3, train/loss_step=0.0798, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 132/547 [02:25<07:36,  0.91it/s, v_num=3, train/loss_step=0.0597, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 133/547 [02:25<07:32,  0.91it/s, v_num=3, train/loss_step=0.0597, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 133/547 [02:26<07:35,  0.91it/s, v_num=3, train/loss_step=0.0194, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 134/547 [02:26<07:31,  0.91it/s, v_num=3, train/loss_step=0.0194, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  24%|██▍       | 134/547 [02:27<07:34,  0.91it/s, v_num=3, train/loss_step=0.0062, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▍       | 135/547 [02:27<07:30,  0.91it/s, v_num=3, train/loss_step=0.0062, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▍       | 135/547 [02:28<07:33,  0.91it/s, v_num=3, train/loss_step=0.0211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▍       | 136/547 [02:28<07:29,  0.91it/s, v_num=3, train/loss_step=0.0211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▍       | 136/547 [02:29<07:32,  0.91it/s, v_num=3, train/loss_step=0.0818, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▌       | 137/547 [02:29<07:28,  0.91it/s, v_num=3, train/loss_step=0.0818, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▌       | 137/547 [02:30<07:31,  0.91it/s, v_num=3, train/loss_step=0.00576, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▌       | 138/547 [02:30<07:27,  0.91it/s, v_num=3, train/loss_step=0.00576, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▌       | 138/547 [02:31<07:29,  0.91it/s, v_num=3, train/loss_step=0.00578, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▌       | 139/547 [02:32<07:26,  0.91it/s, v_num=3, train/loss_step=0.00578, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  25%|██▌       | 139/547 [02:32<07:28,  0.91it/s, v_num=3, train/loss_step=0.00644, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 140/547 [02:33<07:25,  0.91it/s, v_num=3, train/loss_step=0.00644, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 140/547 [02:34<07:27,  0.91it/s, v_num=3, train/loss_step=0.00793, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 141/547 [02:34<07:24,  0.91it/s, v_num=3, train/loss_step=0.00793, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 141/547 [02:35<07:26,  0.91it/s, v_num=3, train/loss_step=0.201, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 142/547 [02:35<07:23,  0.91it/s, v_num=3, train/loss_step=0.201, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 142/547 [02:36<07:25,  0.91it/s, v_num=3, train/loss_step=0.0406, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 143/547 [02:36<07:22,  0.91it/s, v_num=3, train/loss_step=0.0406, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▌       | 143/547 [02:37<07:24,  0.91it/s, v_num=3, train/loss_step=0.0217, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▋       | 144/547 [02:37<07:20,  0.91it/s, v_num=3, train/loss_step=0.0217, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  26%|██▋       | 144/547 [02:38<07:23,  0.91it/s, v_num=3, train/loss_step=0.014, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 145/547 [02:38<07:19,  0.91it/s, v_num=3, train/loss_step=0.014, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 145/547 [02:39<07:22,  0.91it/s, v_num=3, train/loss_step=0.0204, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 146/547 [02:39<07:18,  0.91it/s, v_num=3, train/loss_step=0.0204, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 146/547 [02:40<07:21,  0.91it/s, v_num=3, train/loss_step=0.160, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 147/547 [02:40<07:17,  0.91it/s, v_num=3, train/loss_step=0.160, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 147/547 [02:41<07:20,  0.91it/s, v_num=3, train/loss_step=0.0081, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 148/547 [02:41<07:16,  0.91it/s, v_num=3, train/loss_step=0.0081, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 148/547 [02:42<07:18,  0.91it/s, v_num=3, train/loss_step=0.0106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 149/547 [02:43<07:15,  0.91it/s, v_num=3, train/loss_step=0.0106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 149/547 [02:43<07:17,  0.91it/s, v_num=3, train/loss_step=0.0111, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 150/547 [02:44<07:14,  0.91it/s, v_num=3, train/loss_step=0.0111, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  27%|██▋       | 150/547 [02:45<07:16,  0.91it/s, v_num=3, train/loss_step=0.0177, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 151/547 [02:45<07:13,  0.91it/s, v_num=3, train/loss_step=0.0177, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 151/547 [02:46<07:15,  0.91it/s, v_num=3, train/loss_step=0.0716, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 152/547 [02:46<07:12,  0.91it/s, v_num=3, train/loss_step=0.0716, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 152/547 [02:47<07:14,  0.91it/s, v_num=3, train/loss_step=0.0144, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 153/547 [02:47<07:11,  0.91it/s, v_num=3, train/loss_step=0.0144, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 153/547 [02:48<07:13,  0.91it/s, v_num=3, train/loss_step=0.0648, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 154/547 [02:48<07:10,  0.91it/s, v_num=3, train/loss_step=0.0648, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 154/547 [02:49<07:12,  0.91it/s, v_num=3, train/loss_step=0.117, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 155/547 [02:49<07:09,  0.91it/s, v_num=3, train/loss_step=0.117, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  28%|██▊       | 155/547 [02:50<07:11,  0.91it/s, v_num=3, train/loss_step=0.120, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▊       | 156/547 [02:50<07:08,  0.91it/s, v_num=3, train/loss_step=0.120, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▊       | 156/547 [02:51<07:10,  0.91it/s, v_num=3, train/loss_step=0.237, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▊       | 157/547 [02:51<07:06,  0.91it/s, v_num=3, train/loss_step=0.237, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▊       | 157/547 [02:52<07:09,  0.91it/s, v_num=3, train/loss_step=0.382, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 158/547 [02:52<07:05,  0.91it/s, v_num=3, train/loss_step=0.382, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 158/547 [02:53<07:07,  0.91it/s, v_num=3, train/loss_step=0.0832, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 159/547 [02:54<07:04,  0.91it/s, v_num=3, train/loss_step=0.0832, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 159/547 [02:54<07:06,  0.91it/s, v_num=3, train/loss_step=0.0476, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 160/547 [02:55<07:03,  0.91it/s, v_num=3, train/loss_step=0.0476, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 160/547 [02:56<07:05,  0.91it/s, v_num=3, train/loss_step=0.603, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 161/547 [02:56<07:02,  0.91it/s, v_num=3, train/loss_step=0.603, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  29%|██▉       | 161/547 [02:57<07:04,  0.91it/s, v_num=3, train/loss_step=0.0124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|██▉       | 162/547 [02:57<07:01,  0.91it/s, v_num=3, train/loss_step=0.0124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|██▉       | 162/547 [02:58<07:03,  0.91it/s, v_num=3, train/loss_step=0.0484, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|██▉       | 163/547 [02:58<07:00,  0.91it/s, v_num=3, train/loss_step=0.0484, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|██▉       | 163/547 [02:59<07:02,  0.91it/s, v_num=3, train/loss_step=0.0754, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|██▉       | 164/547 [02:59<06:59,  0.91it/s, v_num=3, train/loss_step=0.0754, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|██▉       | 164/547 [03:00<07:01,  0.91it/s, v_num=3, train/loss_step=0.027, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|███       | 165/547 [03:00<06:58,  0.91it/s, v_num=3, train/loss_step=0.027, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|███       | 165/547 [03:01<07:00,  0.91it/s, v_num=3, train/loss_step=0.092, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|███       | 166/547 [03:01<06:57,  0.91it/s, v_num=3, train/loss_step=0.092, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  30%|███       | 166/547 [03:02<06:59,  0.91it/s, v_num=3, train/loss_step=0.152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 167/547 [03:02<06:56,  0.91it/s, v_num=3, train/loss_step=0.152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 167/547 [03:03<06:58,  0.91it/s, v_num=3, train/loss_step=0.0103, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 168/547 [03:03<06:55,  0.91it/s, v_num=3, train/loss_step=0.0103, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 168/547 [03:04<06:57,  0.91it/s, v_num=3, train/loss_step=0.0223, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 169/547 [03:05<06:53,  0.91it/s, v_num=3, train/loss_step=0.0223, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 169/547 [03:05<06:55,  0.91it/s, v_num=3, train/loss_step=0.0319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 170/547 [03:06<06:52,  0.91it/s, v_num=3, train/loss_step=0.0319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███       | 170/547 [03:07<06:54,  0.91it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███▏      | 171/547 [03:07<06:51,  0.91it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███▏      | 171/547 [03:08<06:53,  0.91it/s, v_num=3, train/loss_step=0.255, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███▏      | 172/547 [03:08<06:50,  0.91it/s, v_num=3, train/loss_step=0.255, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  31%|███▏      | 172/547 [03:09<06:52,  0.91it/s, v_num=3, train/loss_step=0.473, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 173/547 [03:09<06:49,  0.91it/s, v_num=3, train/loss_step=0.473, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 173/547 [03:10<06:51,  0.91it/s, v_num=3, train/loss_step=0.116, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 174/547 [03:10<06:48,  0.91it/s, v_num=3, train/loss_step=0.116, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 174/547 [03:11<06:50,  0.91it/s, v_num=3, train/loss_step=0.011, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 175/547 [03:11<06:47,  0.91it/s, v_num=3, train/loss_step=0.011, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 175/547 [03:12<06:49,  0.91it/s, v_num=3, train/loss_step=0.0174, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 176/547 [03:12<06:46,  0.91it/s, v_num=3, train/loss_step=0.0174, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 176/547 [03:13<06:48,  0.91it/s, v_num=3, train/loss_step=0.0128, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 177/547 [03:13<06:45,  0.91it/s, v_num=3, train/loss_step=0.0128, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  32%|███▏      | 177/547 [03:14<06:47,  0.91it/s, v_num=3, train/loss_step=0.151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 178/547 [03:15<06:44,  0.91it/s, v_num=3, train/loss_step=0.151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 178/547 [03:15<06:46,  0.91it/s, v_num=3, train/loss_step=0.0542, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 179/547 [03:16<06:43,  0.91it/s, v_num=3, train/loss_step=0.0542, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 179/547 [03:16<06:44,  0.91it/s, v_num=3, train/loss_step=0.0196, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 180/547 [03:17<06:42,  0.91it/s, v_num=3, train/loss_step=0.0196, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 180/547 [03:18<06:43,  0.91it/s, v_num=3, train/loss_step=0.0621, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 181/547 [03:18<06:41,  0.91it/s, v_num=3, train/loss_step=0.0621, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 181/547 [03:19<06:42,  0.91it/s, v_num=3, train/loss_step=0.0419, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 182/547 [03:19<06:39,  0.91it/s, v_num=3, train/loss_step=0.0419, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 182/547 [03:20<06:41,  0.91it/s, v_num=3, train/loss_step=0.0412, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 183/547 [03:20<06:38,  0.91it/s, v_num=3, train/loss_step=0.0412, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  33%|███▎      | 183/547 [03:21<06:40,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▎      | 184/547 [03:21<06:37,  0.91it/s, v_num=3, train/loss_step=0.244, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▎      | 184/547 [03:22<06:39,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 185/547 [03:22<06:36,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 185/547 [03:23<06:38,  0.91it/s, v_num=3, train/loss_step=0.295, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 186/547 [03:23<06:35,  0.91it/s, v_num=3, train/loss_step=0.295, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 186/547 [03:24<06:37,  0.91it/s, v_num=3, train/loss_step=0.0643, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 187/547 [03:24<06:34,  0.91it/s, v_num=3, train/loss_step=0.0643, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 187/547 [03:25<06:36,  0.91it/s, v_num=3, train/loss_step=0.0086, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 188/547 [03:26<06:33,  0.91it/s, v_num=3, train/loss_step=0.0086, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  34%|███▍      | 188/547 [03:26<06:35,  0.91it/s, v_num=3, train/loss_step=0.221, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▍      | 189/547 [03:27<06:32,  0.91it/s, v_num=3, train/loss_step=0.221, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▍      | 189/547 [03:27<06:33,  0.91it/s, v_num=3, train/loss_step=0.338, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▍      | 190/547 [03:28<06:31,  0.91it/s, v_num=3, train/loss_step=0.338, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▍      | 190/547 [03:29<06:32,  0.91it/s, v_num=3, train/loss_step=0.0318, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▍      | 191/547 [03:29<06:30,  0.91it/s, v_num=3, train/loss_step=0.0318, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▍      | 191/547 [03:30<06:31,  0.91it/s, v_num=3, train/loss_step=0.343, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▌      | 192/547 [03:30<06:29,  0.91it/s, v_num=3, train/loss_step=0.343, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▌      | 192/547 [03:31<06:30,  0.91it/s, v_num=3, train/loss_step=0.016, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▌      | 193/547 [03:31<06:27,  0.91it/s, v_num=3, train/loss_step=0.016, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▌      | 193/547 [03:32<06:29,  0.91it/s, v_num=3, train/loss_step=0.0143, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▌      | 194/547 [03:32<06:26,  0.91it/s, v_num=3, train/loss_step=0.0143, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  35%|███▌      | 194/547 [03:33<06:28,  0.91it/s, v_num=3, train/loss_step=0.0551, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 195/547 [03:33<06:25,  0.91it/s, v_num=3, train/loss_step=0.0551, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 195/547 [03:34<06:27,  0.91it/s, v_num=3, train/loss_step=0.0258, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 196/547 [03:34<06:24,  0.91it/s, v_num=3, train/loss_step=0.0258, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 196/547 [03:35<06:26,  0.91it/s, v_num=3, train/loss_step=0.078, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 197/547 [03:35<06:23,  0.91it/s, v_num=3, train/loss_step=0.078, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 197/547 [03:36<06:25,  0.91it/s, v_num=3, train/loss_step=0.0885, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 198/547 [03:37<06:22,  0.91it/s, v_num=3, train/loss_step=0.0885, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▌      | 198/547 [03:37<06:24,  0.91it/s, v_num=3, train/loss_step=0.0996, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▋      | 199/547 [03:38<06:21,  0.91it/s, v_num=3, train/loss_step=0.0996, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  36%|███▋      | 199/547 [03:38<06:22,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 200/547 [03:39<06:20,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 200/547 [03:40<06:21,  0.91it/s, v_num=3, train/loss_step=0.0684, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 201/547 [03:40<06:19,  0.91it/s, v_num=3, train/loss_step=0.0684, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 201/547 [03:41<06:20,  0.91it/s, v_num=3, train/loss_step=0.0306, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 202/547 [03:41<06:18,  0.91it/s, v_num=3, train/loss_step=0.0306, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 202/547 [03:42<06:19,  0.91it/s, v_num=3, train/loss_step=0.126, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 203/547 [03:42<06:17,  0.91it/s, v_num=3, train/loss_step=0.126, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 203/547 [03:43<06:18,  0.91it/s, v_num=3, train/loss_step=0.0424, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 204/547 [03:43<06:16,  0.91it/s, v_num=3, train/loss_step=0.0424, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 204/547 [03:44<06:17,  0.91it/s, v_num=3, train/loss_step=0.0239, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 205/547 [03:44<06:14,  0.91it/s, v_num=3, train/loss_step=0.0239, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  37%|███▋      | 205/547 [03:45<06:16,  0.91it/s, v_num=3, train/loss_step=0.00609, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 206/547 [03:45<06:13,  0.91it/s, v_num=3, train/loss_step=0.00609, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 206/547 [03:46<06:15,  0.91it/s, v_num=3, train/loss_step=0.0222, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 207/547 [03:46<06:12,  0.91it/s, v_num=3, train/loss_step=0.0222, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 207/547 [03:47<06:14,  0.91it/s, v_num=3, train/loss_step=0.028, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 208/547 [03:48<06:11,  0.91it/s, v_num=3, train/loss_step=0.028, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 208/547 [03:48<06:13,  0.91it/s, v_num=3, train/loss_step=0.00467, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 209/547 [03:49<06:10,  0.91it/s, v_num=3, train/loss_step=0.00467, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 209/547 [03:49<06:11,  0.91it/s, v_num=3, train/loss_step=0.0162, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 210/547 [03:50<06:09,  0.91it/s, v_num=3, train/loss_step=0.0162, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  38%|███▊      | 210/547 [03:51<06:10,  0.91it/s, v_num=3, train/loss_step=0.0791, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▊      | 211/547 [03:51<06:08,  0.91it/s, v_num=3, train/loss_step=0.0791, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▊      | 211/547 [03:52<06:09,  0.91it/s, v_num=3, train/loss_step=0.00828, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 212/547 [03:52<06:07,  0.91it/s, v_num=3, train/loss_step=0.00828, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 212/547 [03:53<06:08,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 213/547 [03:53<06:06,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 213/547 [03:54<06:07,  0.91it/s, v_num=3, train/loss_step=0.0066, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 214/547 [03:54<06:05,  0.91it/s, v_num=3, train/loss_step=0.0066, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 214/547 [03:55<06:06,  0.91it/s, v_num=3, train/loss_step=0.0173, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 215/547 [03:55<06:04,  0.91it/s, v_num=3, train/loss_step=0.0173, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 215/547 [03:56<06:05,  0.91it/s, v_num=3, train/loss_step=0.193, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 216/547 [03:56<06:02,  0.91it/s, v_num=3, train/loss_step=0.193, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  39%|███▉      | 216/547 [03:57<06:04,  0.91it/s, v_num=3, train/loss_step=0.0671, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|███▉      | 217/547 [03:57<06:01,  0.91it/s, v_num=3, train/loss_step=0.0671, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|███▉      | 217/547 [03:58<06:03,  0.91it/s, v_num=3, train/loss_step=0.0106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|███▉      | 218/547 [03:59<06:00,  0.91it/s, v_num=3, train/loss_step=0.0106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|███▉      | 218/547 [03:59<06:02,  0.91it/s, v_num=3, train/loss_step=0.0104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|████      | 219/547 [04:00<05:59,  0.91it/s, v_num=3, train/loss_step=0.0104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|████      | 219/547 [04:01<06:00,  0.91it/s, v_num=3, train/loss_step=0.0639, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|████      | 220/547 [04:01<05:58,  0.91it/s, v_num=3, train/loss_step=0.0639, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|████      | 220/547 [04:02<05:59,  0.91it/s, v_num=3, train/loss_step=0.143, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|████      | 221/547 [04:02<05:57,  0.91it/s, v_num=3, train/loss_step=0.143, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  40%|████      | 221/547 [04:03<05:58,  0.91it/s, v_num=3, train/loss_step=0.0705, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 222/547 [04:03<05:56,  0.91it/s, v_num=3, train/loss_step=0.0705, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 222/547 [04:04<05:57,  0.91it/s, v_num=3, train/loss_step=0.0379, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 223/547 [04:04<05:55,  0.91it/s, v_num=3, train/loss_step=0.0379, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 223/547 [04:05<05:56,  0.91it/s, v_num=3, train/loss_step=0.0227, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 224/547 [04:05<05:54,  0.91it/s, v_num=3, train/loss_step=0.0227, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 224/547 [04:06<05:55,  0.91it/s, v_num=3, train/loss_step=0.0286, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 225/547 [04:06<05:53,  0.91it/s, v_num=3, train/loss_step=0.0286, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████      | 225/547 [04:07<05:54,  0.91it/s, v_num=3, train/loss_step=0.0321, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████▏     | 226/547 [04:07<05:52,  0.91it/s, v_num=3, train/loss_step=0.0321, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████▏     | 226/547 [04:08<05:53,  0.91it/s, v_num=3, train/loss_step=0.149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████▏     | 227/547 [04:08<05:50,  0.91it/s, v_num=3, train/loss_step=0.149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  41%|████▏     | 227/547 [04:09<05:52,  0.91it/s, v_num=3, train/loss_step=0.319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 228/547 [04:10<05:49,  0.91it/s, v_num=3, train/loss_step=0.319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 228/547 [04:10<05:51,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 229/547 [04:11<05:48,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 229/547 [04:12<05:50,  0.91it/s, v_num=3, train/loss_step=0.0233, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 230/547 [04:12<05:47,  0.91it/s, v_num=3, train/loss_step=0.0233, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 230/547 [04:13<05:48,  0.91it/s, v_num=3, train/loss_step=0.014, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 231/547 [04:13<05:46,  0.91it/s, v_num=3, train/loss_step=0.014, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 231/547 [04:14<05:47,  0.91it/s, v_num=3, train/loss_step=0.0195, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 232/547 [04:14<05:45,  0.91it/s, v_num=3, train/loss_step=0.0195, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  42%|████▏     | 232/547 [04:15<05:46,  0.91it/s, v_num=3, train/loss_step=0.0288, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 233/547 [04:15<05:44,  0.91it/s, v_num=3, train/loss_step=0.0288, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 233/547 [04:16<05:45,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 234/547 [04:16<05:43,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 234/547 [04:17<05:44,  0.91it/s, v_num=3, train/loss_step=0.0236, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 235/547 [04:17<05:42,  0.91it/s, v_num=3, train/loss_step=0.0236, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 235/547 [04:18<05:43,  0.91it/s, v_num=3, train/loss_step=0.570, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 236/547 [04:18<05:41,  0.91it/s, v_num=3, train/loss_step=0.570, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 236/547 [04:19<05:42,  0.91it/s, v_num=3, train/loss_step=0.0798, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 237/547 [04:20<05:40,  0.91it/s, v_num=3, train/loss_step=0.0798, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  43%|████▎     | 237/547 [04:20<05:41,  0.91it/s, v_num=3, train/loss_step=0.297, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▎     | 238/547 [04:21<05:39,  0.91it/s, v_num=3, train/loss_step=0.297, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▎     | 238/547 [04:21<05:40,  0.91it/s, v_num=3, train/loss_step=0.0317, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▎     | 239/547 [04:22<05:37,  0.91it/s, v_num=3, train/loss_step=0.0317, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▎     | 239/547 [04:23<05:39,  0.91it/s, v_num=3, train/loss_step=0.223, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 240/547 [04:23<05:36,  0.91it/s, v_num=3, train/loss_step=0.223, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 240/547 [04:24<05:37,  0.91it/s, v_num=3, train/loss_step=0.0564, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 241/547 [04:24<05:35,  0.91it/s, v_num=3, train/loss_step=0.0564, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 241/547 [04:25<05:36,  0.91it/s, v_num=3, train/loss_step=0.0345, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 242/547 [04:25<05:34,  0.91it/s, v_num=3, train/loss_step=0.0345, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 242/547 [04:26<05:35,  0.91it/s, v_num=3, train/loss_step=0.187, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 243/547 [04:26<05:33,  0.91it/s, v_num=3, train/loss_step=0.187, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  44%|████▍     | 243/547 [04:27<05:34,  0.91it/s, v_num=3, train/loss_step=0.00866, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▍     | 244/547 [04:27<05:32,  0.91it/s, v_num=3, train/loss_step=0.00866, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▍     | 244/547 [04:28<05:33,  0.91it/s, v_num=3, train/loss_step=0.021, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▍     | 245/547 [04:28<05:31,  0.91it/s, v_num=3, train/loss_step=0.021, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▍     | 245/547 [04:29<05:32,  0.91it/s, v_num=3, train/loss_step=0.0419, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▍     | 246/547 [04:29<05:30,  0.91it/s, v_num=3, train/loss_step=0.0419, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▍     | 246/547 [04:30<05:31,  0.91it/s, v_num=3, train/loss_step=0.137, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▌     | 247/547 [04:31<05:29,  0.91it/s, v_num=3, train/loss_step=0.137, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▌     | 247/547 [04:31<05:30,  0.91it/s, v_num=3, train/loss_step=0.138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▌     | 248/547 [04:32<05:28,  0.91it/s, v_num=3, train/loss_step=0.138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  45%|████▌     | 248/547 [04:33<05:29,  0.91it/s, v_num=3, train/loss_step=0.485, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 249/547 [04:33<05:27,  0.91it/s, v_num=3, train/loss_step=0.485, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 249/547 [04:34<05:28,  0.91it/s, v_num=3, train/loss_step=0.0216, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 250/547 [04:34<05:25,  0.91it/s, v_num=3, train/loss_step=0.0216, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 250/547 [04:35<05:26,  0.91it/s, v_num=3, train/loss_step=0.117, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 251/547 [04:35<05:24,  0.91it/s, v_num=3, train/loss_step=0.117, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 251/547 [04:36<05:25,  0.91it/s, v_num=3, train/loss_step=0.209, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 252/547 [04:36<05:23,  0.91it/s, v_num=3, train/loss_step=0.209, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▌     | 252/547 [04:37<05:24,  0.91it/s, v_num=3, train/loss_step=0.0285, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▋     | 253/547 [04:37<05:22,  0.91it/s, v_num=3, train/loss_step=0.0285, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▋     | 253/547 [04:38<05:23,  0.91it/s, v_num=3, train/loss_step=0.436, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▋     | 254/547 [04:38<05:21,  0.91it/s, v_num=3, train/loss_step=0.436, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  46%|████▋     | 254/547 [04:39<05:22,  0.91it/s, v_num=3, train/loss_step=0.151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 255/547 [04:39<05:20,  0.91it/s, v_num=3, train/loss_step=0.151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 255/547 [04:40<05:21,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 256/547 [04:40<05:19,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 256/547 [04:41<05:20,  0.91it/s, v_num=3, train/loss_step=0.0131, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 257/547 [04:42<05:18,  0.91it/s, v_num=3, train/loss_step=0.0131, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 257/547 [04:42<05:19,  0.91it/s, v_num=3, train/loss_step=0.0189, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 258/547 [04:43<05:17,  0.91it/s, v_num=3, train/loss_step=0.0189, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 258/547 [04:44<05:18,  0.91it/s, v_num=3, train/loss_step=0.121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 259/547 [04:44<05:16,  0.91it/s, v_num=3, train/loss_step=0.121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  47%|████▋     | 259/547 [04:45<05:17,  0.91it/s, v_num=3, train/loss_step=0.397, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 260/547 [04:45<05:14,  0.91it/s, v_num=3, train/loss_step=0.397, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 260/547 [04:46<05:15,  0.91it/s, v_num=3, train/loss_step=0.0178, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 261/547 [04:46<05:13,  0.91it/s, v_num=3, train/loss_step=0.0178, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 261/547 [04:47<05:14,  0.91it/s, v_num=3, train/loss_step=0.0145, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 262/547 [04:47<05:12,  0.91it/s, v_num=3, train/loss_step=0.0145, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 262/547 [04:48<05:13,  0.91it/s, v_num=3, train/loss_step=0.0813, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 263/547 [04:48<05:11,  0.91it/s, v_num=3, train/loss_step=0.0813, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 263/547 [04:49<05:12,  0.91it/s, v_num=3, train/loss_step=0.0108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 264/547 [04:49<05:10,  0.91it/s, v_num=3, train/loss_step=0.0108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 264/547 [04:50<05:11,  0.91it/s, v_num=3, train/loss_step=0.0953, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 265/547 [04:50<05:09,  0.91it/s, v_num=3, train/loss_step=0.0953, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  48%|████▊     | 265/547 [04:51<05:10,  0.91it/s, v_num=3, train/loss_step=0.0427, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▊     | 266/547 [04:51<05:08,  0.91it/s, v_num=3, train/loss_step=0.0427, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▊     | 266/547 [04:52<05:09,  0.91it/s, v_num=3, train/loss_step=0.0327, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 267/547 [04:53<05:07,  0.91it/s, v_num=3, train/loss_step=0.0327, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 267/547 [04:53<05:08,  0.91it/s, v_num=3, train/loss_step=0.111, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 268/547 [04:54<05:06,  0.91it/s, v_num=3, train/loss_step=0.111, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 268/547 [04:55<05:07,  0.91it/s, v_num=3, train/loss_step=0.0123, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 269/547 [04:55<05:05,  0.91it/s, v_num=3, train/loss_step=0.0123, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 269/547 [04:56<05:06,  0.91it/s, v_num=3, train/loss_step=0.0635, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 270/547 [04:56<05:04,  0.91it/s, v_num=3, train/loss_step=0.0635, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  49%|████▉     | 270/547 [04:57<05:04,  0.91it/s, v_num=3, train/loss_step=0.0638, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|████▉     | 271/547 [04:57<05:02,  0.91it/s, v_num=3, train/loss_step=0.0638, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|████▉     | 271/547 [04:58<05:03,  0.91it/s, v_num=3, train/loss_step=0.169, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|████▉     | 272/547 [04:58<05:01,  0.91it/s, v_num=3, train/loss_step=0.169, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|████▉     | 272/547 [04:59<05:02,  0.91it/s, v_num=3, train/loss_step=0.0212, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|████▉     | 273/547 [04:59<05:00,  0.91it/s, v_num=3, train/loss_step=0.0212, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|████▉     | 273/547 [05:00<05:01,  0.91it/s, v_num=3, train/loss_step=0.00722, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|█████     | 274/547 [05:00<04:59,  0.91it/s, v_num=3, train/loss_step=0.00722, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|█████     | 274/547 [05:01<05:00,  0.91it/s, v_num=3, train/loss_step=0.0364, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|█████     | 275/547 [05:01<04:58,  0.91it/s, v_num=3, train/loss_step=0.0364, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|█████     | 275/547 [05:02<04:59,  0.91it/s, v_num=3, train/loss_step=0.0547, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|█████     | 276/547 [05:03<04:57,  0.91it/s, v_num=3, train/loss_step=0.0547, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  50%|█████     | 276/547 [05:03<04:58,  0.91it/s, v_num=3, train/loss_step=0.0831, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 277/547 [05:04<04:56,  0.91it/s, v_num=3, train/loss_step=0.0831, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 277/547 [05:04<04:57,  0.91it/s, v_num=3, train/loss_step=0.00851, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 278/547 [05:05<04:55,  0.91it/s, v_num=3, train/loss_step=0.00851, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 278/547 [05:06<04:56,  0.91it/s, v_num=3, train/loss_step=0.00571, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 279/547 [05:06<04:54,  0.91it/s, v_num=3, train/loss_step=0.00571, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 279/547 [05:07<04:55,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 280/547 [05:07<04:53,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████     | 280/547 [05:08<04:53,  0.91it/s, v_num=3, train/loss_step=0.00665, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████▏    | 281/547 [05:08<04:52,  0.91it/s, v_num=3, train/loss_step=0.00665, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  51%|█████▏    | 281/547 [05:09<04:52,  0.91it/s, v_num=3, train/loss_step=0.170, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 282/547 [05:09<04:50,  0.91it/s, v_num=3, train/loss_step=0.170, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 282/547 [05:10<04:51,  0.91it/s, v_num=3, train/loss_step=0.0341, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 283/547 [05:10<04:49,  0.91it/s, v_num=3, train/loss_step=0.0341, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 283/547 [05:11<04:50,  0.91it/s, v_num=3, train/loss_step=0.107, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 284/547 [05:11<04:48,  0.91it/s, v_num=3, train/loss_step=0.107, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 284/547 [05:12<04:49,  0.91it/s, v_num=3, train/loss_step=0.137, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 285/547 [05:12<04:47,  0.91it/s, v_num=3, train/loss_step=0.137, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 285/547 [05:13<04:48,  0.91it/s, v_num=3, train/loss_step=0.365, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 286/547 [05:14<04:46,  0.91it/s, v_num=3, train/loss_step=0.365, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 286/547 [05:14<04:47,  0.91it/s, v_num=3, train/loss_step=0.049, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 287/547 [05:15<04:45,  0.91it/s, v_num=3, train/loss_step=0.049, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  52%|█████▏    | 287/547 [05:15<04:46,  0.91it/s, v_num=3, train/loss_step=0.0103, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 288/547 [05:16<04:44,  0.91it/s, v_num=3, train/loss_step=0.0103, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 288/547 [05:17<04:45,  0.91it/s, v_num=3, train/loss_step=0.169, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 289/547 [05:17<04:43,  0.91it/s, v_num=3, train/loss_step=0.169, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 289/547 [05:18<04:44,  0.91it/s, v_num=3, train/loss_step=0.615, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 290/547 [05:18<04:42,  0.91it/s, v_num=3, train/loss_step=0.615, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 290/547 [05:19<04:42,  0.91it/s, v_num=3, train/loss_step=0.0272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 291/547 [05:19<04:41,  0.91it/s, v_num=3, train/loss_step=0.0272, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 291/547 [05:20<04:41,  0.91it/s, v_num=3, train/loss_step=0.0788, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 292/547 [05:20<04:40,  0.91it/s, v_num=3, train/loss_step=0.0788, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  53%|█████▎    | 292/547 [05:21<04:40,  0.91it/s, v_num=3, train/loss_step=0.471, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▎    | 293/547 [05:21<04:38,  0.91it/s, v_num=3, train/loss_step=0.471, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▎    | 293/547 [05:22<04:39,  0.91it/s, v_num=3, train/loss_step=0.0757, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▎    | 294/547 [05:22<04:37,  0.91it/s, v_num=3, train/loss_step=0.0757, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▎    | 294/547 [05:23<04:38,  0.91it/s, v_num=3, train/loss_step=0.0979, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 295/547 [05:23<04:36,  0.91it/s, v_num=3, train/loss_step=0.0979, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 295/547 [05:24<04:37,  0.91it/s, v_num=3, train/loss_step=0.0354, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 296/547 [05:25<04:35,  0.91it/s, v_num=3, train/loss_step=0.0354, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 296/547 [05:25<04:36,  0.91it/s, v_num=3, train/loss_step=0.0448, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 297/547 [05:26<04:34,  0.91it/s, v_num=3, train/loss_step=0.0448, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 297/547 [05:27<04:35,  0.91it/s, v_num=3, train/loss_step=0.031, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 298/547 [05:27<04:33,  0.91it/s, v_num=3, train/loss_step=0.031, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  54%|█████▍    | 298/547 [05:28<04:34,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▍    | 299/547 [05:28<04:32,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▍    | 299/547 [05:29<04:33,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▍    | 300/547 [05:29<04:31,  0.91it/s, v_num=3, train/loss_step=0.164, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▍    | 300/547 [05:30<04:31,  0.91it/s, v_num=3, train/loss_step=0.0431, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▌    | 301/547 [05:30<04:30,  0.91it/s, v_num=3, train/loss_step=0.0431, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▌    | 301/547 [05:31<04:30,  0.91it/s, v_num=3, train/loss_step=0.0302, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▌    | 302/547 [05:31<04:29,  0.91it/s, v_num=3, train/loss_step=0.0302, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▌    | 302/547 [05:32<04:29,  0.91it/s, v_num=3, train/loss_step=0.0416, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▌    | 303/547 [05:32<04:28,  0.91it/s, v_num=3, train/loss_step=0.0416, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  55%|█████▌    | 303/547 [05:33<04:28,  0.91it/s, v_num=3, train/loss_step=0.0368, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 304/547 [05:33<04:26,  0.91it/s, v_num=3, train/loss_step=0.0368, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 304/547 [05:34<04:27,  0.91it/s, v_num=3, train/loss_step=0.0652, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 305/547 [05:35<04:25,  0.91it/s, v_num=3, train/loss_step=0.0652, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 305/547 [05:35<04:26,  0.91it/s, v_num=3, train/loss_step=0.0221, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 306/547 [05:36<04:24,  0.91it/s, v_num=3, train/loss_step=0.0221, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 306/547 [05:36<04:25,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 307/547 [05:37<04:23,  0.91it/s, v_num=3, train/loss_step=0.0246, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▌    | 307/547 [05:38<04:24,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▋    | 308/547 [05:38<04:22,  0.91it/s, v_num=3, train/loss_step=0.101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▋    | 308/547 [05:39<04:23,  0.91it/s, v_num=3, train/loss_step=0.0274, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▋    | 309/547 [05:39<04:21,  0.91it/s, v_num=3, train/loss_step=0.0274, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  56%|█████▋    | 309/547 [05:40<04:22,  0.91it/s, v_num=3, train/loss_step=0.0976, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 310/547 [05:40<04:20,  0.91it/s, v_num=3, train/loss_step=0.0976, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 310/547 [05:41<04:21,  0.91it/s, v_num=3, train/loss_step=0.0405, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 311/547 [05:41<04:19,  0.91it/s, v_num=3, train/loss_step=0.0405, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 311/547 [05:42<04:19,  0.91it/s, v_num=3, train/loss_step=0.0274, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 312/547 [05:42<04:18,  0.91it/s, v_num=3, train/loss_step=0.0274, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 312/547 [05:43<04:18,  0.91it/s, v_num=3, train/loss_step=0.0136, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 313/547 [05:43<04:17,  0.91it/s, v_num=3, train/loss_step=0.0136, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 313/547 [05:44<04:17,  0.91it/s, v_num=3, train/loss_step=0.290, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 314/547 [05:44<04:15,  0.91it/s, v_num=3, train/loss_step=0.290, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  57%|█████▋    | 314/547 [05:45<04:16,  0.91it/s, v_num=3, train/loss_step=0.0131, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 315/547 [05:46<04:14,  0.91it/s, v_num=3, train/loss_step=0.0131, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 315/547 [05:46<04:15,  0.91it/s, v_num=3, train/loss_step=0.444, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 316/547 [05:47<04:13,  0.91it/s, v_num=3, train/loss_step=0.444, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 316/547 [05:48<04:14,  0.91it/s, v_num=3, train/loss_step=0.0183, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 317/547 [05:48<04:12,  0.91it/s, v_num=3, train/loss_step=0.0183, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 317/547 [05:49<04:13,  0.91it/s, v_num=3, train/loss_step=0.114, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 318/547 [05:49<04:11,  0.91it/s, v_num=3, train/loss_step=0.114, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 318/547 [05:50<04:12,  0.91it/s, v_num=3, train/loss_step=0.0412, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 319/547 [05:50<04:10,  0.91it/s, v_num=3, train/loss_step=0.0412, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  58%|█████▊    | 319/547 [05:51<04:11,  0.91it/s, v_num=3, train/loss_step=0.0115, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▊    | 320/547 [05:51<04:09,  0.91it/s, v_num=3, train/loss_step=0.0115, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▊    | 320/547 [05:52<04:10,  0.91it/s, v_num=3, train/loss_step=0.0303, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▊    | 321/547 [05:52<04:08,  0.91it/s, v_num=3, train/loss_step=0.0303, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▊    | 321/547 [05:53<04:08,  0.91it/s, v_num=3, train/loss_step=0.0501, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 322/547 [05:53<04:07,  0.91it/s, v_num=3, train/loss_step=0.0501, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 322/547 [05:54<04:07,  0.91it/s, v_num=3, train/loss_step=0.0052, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 323/547 [05:54<04:06,  0.91it/s, v_num=3, train/loss_step=0.0052, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 323/547 [05:55<04:06,  0.91it/s, v_num=3, train/loss_step=0.0177, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 324/547 [05:56<04:05,  0.91it/s, v_num=3, train/loss_step=0.0177, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 324/547 [05:56<04:05,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 325/547 [05:57<04:03,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  59%|█████▉    | 325/547 [05:57<04:04,  0.91it/s, v_num=3, train/loss_step=0.163, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|█████▉    | 326/547 [05:58<04:02,  0.91it/s, v_num=3, train/loss_step=0.163, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|█████▉    | 326/547 [05:59<04:03,  0.91it/s, v_num=3, train/loss_step=0.165, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|█████▉    | 327/547 [05:59<04:01,  0.91it/s, v_num=3, train/loss_step=0.165, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|█████▉    | 327/547 [06:00<04:02,  0.91it/s, v_num=3, train/loss_step=0.100, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|█████▉    | 328/547 [06:00<04:00,  0.91it/s, v_num=3, train/loss_step=0.100, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|█████▉    | 328/547 [06:01<04:01,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|██████    | 329/547 [06:01<03:59,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|██████    | 329/547 [06:02<04:00,  0.91it/s, v_num=3, train/loss_step=0.0909, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|██████    | 330/547 [06:02<03:58,  0.91it/s, v_num=3, train/loss_step=0.0909, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  60%|██████    | 330/547 [06:03<03:59,  0.91it/s, v_num=3, train/loss_step=0.0114, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 331/547 [06:03<03:57,  0.91it/s, v_num=3, train/loss_step=0.0114, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 331/547 [06:04<03:57,  0.91it/s, v_num=3, train/loss_step=0.273, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 332/547 [06:04<03:56,  0.91it/s, v_num=3, train/loss_step=0.273, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 332/547 [06:05<03:56,  0.91it/s, v_num=3, train/loss_step=0.0447, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 333/547 [06:05<03:55,  0.91it/s, v_num=3, train/loss_step=0.0447, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 333/547 [06:06<03:55,  0.91it/s, v_num=3, train/loss_step=0.0219, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 334/547 [06:07<03:54,  0.91it/s, v_num=3, train/loss_step=0.0219, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 334/547 [06:07<03:54,  0.91it/s, v_num=3, train/loss_step=0.0153, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 335/547 [06:08<03:52,  0.91it/s, v_num=3, train/loss_step=0.0153, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████    | 335/547 [06:09<03:53,  0.91it/s, v_num=3, train/loss_step=0.0515, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████▏   | 336/547 [06:09<03:51,  0.91it/s, v_num=3, train/loss_step=0.0515, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  61%|██████▏   | 336/547 [06:10<03:52,  0.91it/s, v_num=3, train/loss_step=0.300, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 337/547 [06:10<03:50,  0.91it/s, v_num=3, train/loss_step=0.300, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 337/547 [06:11<03:51,  0.91it/s, v_num=3, train/loss_step=0.00604, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 338/547 [06:11<03:49,  0.91it/s, v_num=3, train/loss_step=0.00604, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 338/547 [06:12<03:50,  0.91it/s, v_num=3, train/loss_step=0.0179, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 339/547 [06:12<03:48,  0.91it/s, v_num=3, train/loss_step=0.0179, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 339/547 [06:13<03:49,  0.91it/s, v_num=3, train/loss_step=0.0328, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 340/547 [06:13<03:47,  0.91it/s, v_num=3, train/loss_step=0.0328, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 340/547 [06:14<03:48,  0.91it/s, v_num=3, train/loss_step=0.0117, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 341/547 [06:14<03:46,  0.91it/s, v_num=3, train/loss_step=0.0117, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  62%|██████▏   | 341/547 [06:15<03:46,  0.91it/s, v_num=3, train/loss_step=0.0825, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 342/547 [06:15<03:45,  0.91it/s, v_num=3, train/loss_step=0.0825, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 342/547 [06:16<03:45,  0.91it/s, v_num=3, train/loss_step=0.00808, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 343/547 [06:16<03:44,  0.91it/s, v_num=3, train/loss_step=0.00808, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 343/547 [06:17<03:44,  0.91it/s, v_num=3, train/loss_step=0.0121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 344/547 [06:18<03:43,  0.91it/s, v_num=3, train/loss_step=0.0121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 344/547 [06:18<03:43,  0.91it/s, v_num=3, train/loss_step=0.0108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 345/547 [06:19<03:42,  0.91it/s, v_num=3, train/loss_step=0.0108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 345/547 [06:20<03:42,  0.91it/s, v_num=3, train/loss_step=0.00583, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 346/547 [06:20<03:40,  0.91it/s, v_num=3, train/loss_step=0.00583, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 346/547 [06:21<03:41,  0.91it/s, v_num=3, train/loss_step=0.0709, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 347/547 [06:21<03:39,  0.91it/s, v_num=3, train/loss_step=0.0709, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  63%|██████▎   | 347/547 [06:22<03:40,  0.91it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▎   | 348/547 [06:22<03:38,  0.91it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▎   | 348/547 [06:23<03:39,  0.91it/s, v_num=3, train/loss_step=0.252, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 349/547 [06:23<03:37,  0.91it/s, v_num=3, train/loss_step=0.252, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 349/547 [06:24<03:38,  0.91it/s, v_num=3, train/loss_step=0.211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 350/547 [06:24<03:36,  0.91it/s, v_num=3, train/loss_step=0.211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 350/547 [06:25<03:37,  0.91it/s, v_num=3, train/loss_step=0.00695, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 351/547 [06:25<03:35,  0.91it/s, v_num=3, train/loss_step=0.00695, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 351/547 [06:26<03:35,  0.91it/s, v_num=3, train/loss_step=0.0232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 352/547 [06:26<03:34,  0.91it/s, v_num=3, train/loss_step=0.0232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  64%|██████▍   | 352/547 [06:27<03:34,  0.91it/s, v_num=3, train/loss_step=0.353, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▍   | 353/547 [06:28<03:33,  0.91it/s, v_num=3, train/loss_step=0.353, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▍   | 353/547 [06:28<03:33,  0.91it/s, v_num=3, train/loss_step=0.186, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▍   | 354/547 [06:29<03:32,  0.91it/s, v_num=3, train/loss_step=0.186, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▍   | 354/547 [06:30<03:32,  0.91it/s, v_num=3, train/loss_step=0.0138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▍   | 355/547 [06:30<03:31,  0.91it/s, v_num=3, train/loss_step=0.0138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▍   | 355/547 [06:31<03:31,  0.91it/s, v_num=3, train/loss_step=0.0595, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▌   | 356/547 [06:31<03:29,  0.91it/s, v_num=3, train/loss_step=0.0595, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▌   | 356/547 [06:32<03:30,  0.91it/s, v_num=3, train/loss_step=0.0118, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▌   | 357/547 [06:32<03:28,  0.91it/s, v_num=3, train/loss_step=0.0118, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▌   | 357/547 [06:33<03:29,  0.91it/s, v_num=3, train/loss_step=0.319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▌   | 358/547 [06:33<03:27,  0.91it/s, v_num=3, train/loss_step=0.319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  65%|██████▌   | 358/547 [06:34<03:28,  0.91it/s, v_num=3, train/loss_step=0.0918, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 359/547 [06:34<03:26,  0.91it/s, v_num=3, train/loss_step=0.0918, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 359/547 [06:35<03:27,  0.91it/s, v_num=3, train/loss_step=0.102, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 360/547 [06:35<03:25,  0.91it/s, v_num=3, train/loss_step=0.102, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 360/547 [06:36<03:26,  0.91it/s, v_num=3, train/loss_step=0.0158, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 361/547 [06:36<03:24,  0.91it/s, v_num=3, train/loss_step=0.0158, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 361/547 [06:37<03:24,  0.91it/s, v_num=3, train/loss_step=0.0891, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 362/547 [06:38<03:23,  0.91it/s, v_num=3, train/loss_step=0.0891, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▌   | 362/547 [06:38<03:23,  0.91it/s, v_num=3, train/loss_step=0.118, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▋   | 363/547 [06:39<03:22,  0.91it/s, v_num=3, train/loss_step=0.118, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  66%|██████▋   | 363/547 [06:39<03:22,  0.91it/s, v_num=3, train/loss_step=0.156, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 364/547 [06:40<03:21,  0.91it/s, v_num=3, train/loss_step=0.156, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 364/547 [06:41<03:21,  0.91it/s, v_num=3, train/loss_step=0.297, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 365/547 [06:41<03:20,  0.91it/s, v_num=3, train/loss_step=0.297, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 365/547 [06:42<03:20,  0.91it/s, v_num=3, train/loss_step=0.149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 366/547 [06:42<03:19,  0.91it/s, v_num=3, train/loss_step=0.149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 366/547 [06:43<03:19,  0.91it/s, v_num=3, train/loss_step=0.0245, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 367/547 [06:43<03:17,  0.91it/s, v_num=3, train/loss_step=0.0245, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 367/547 [06:44<03:18,  0.91it/s, v_num=3, train/loss_step=0.0248, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 368/547 [06:44<03:16,  0.91it/s, v_num=3, train/loss_step=0.0248, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 368/547 [06:45<03:17,  0.91it/s, v_num=3, train/loss_step=0.0836, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 369/547 [06:45<03:15,  0.91it/s, v_num=3, train/loss_step=0.0836, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  67%|██████▋   | 369/547 [06:46<03:16,  0.91it/s, v_num=3, train/loss_step=0.0478, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 370/547 [06:46<03:14,  0.91it/s, v_num=3, train/loss_step=0.0478, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 370/547 [06:47<03:15,  0.91it/s, v_num=3, train/loss_step=0.00567, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 371/547 [06:47<03:13,  0.91it/s, v_num=3, train/loss_step=0.00567, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 371/547 [06:48<03:13,  0.91it/s, v_num=3, train/loss_step=0.507, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 372/547 [06:49<03:12,  0.91it/s, v_num=3, train/loss_step=0.507, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 372/547 [06:49<03:12,  0.91it/s, v_num=3, train/loss_step=0.00801, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 373/547 [06:50<03:11,  0.91it/s, v_num=3, train/loss_step=0.00801, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 373/547 [06:51<03:11,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 374/547 [06:51<03:10,  0.91it/s, v_num=3, train/loss_step=0.352, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  68%|██████▊   | 374/547 [06:52<03:10,  0.91it/s, v_num=3, train/loss_step=0.0394, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▊   | 375/547 [06:52<03:09,  0.91it/s, v_num=3, train/loss_step=0.0394, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▊   | 375/547 [06:53<03:09,  0.91it/s, v_num=3, train/loss_step=0.0519, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▊   | 376/547 [06:53<03:08,  0.91it/s, v_num=3, train/loss_step=0.0519, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▊   | 376/547 [06:54<03:08,  0.91it/s, v_num=3, train/loss_step=0.00914, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 377/547 [06:54<03:06,  0.91it/s, v_num=3, train/loss_step=0.00914, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 377/547 [06:55<03:07,  0.91it/s, v_num=3, train/loss_step=0.0124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 378/547 [06:55<03:05,  0.91it/s, v_num=3, train/loss_step=0.0124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 378/547 [06:56<03:06,  0.91it/s, v_num=3, train/loss_step=0.068, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 379/547 [06:56<03:04,  0.91it/s, v_num=3, train/loss_step=0.068, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 379/547 [06:57<03:05,  0.91it/s, v_num=3, train/loss_step=0.0133, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 380/547 [06:57<03:03,  0.91it/s, v_num=3, train/loss_step=0.0133, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  69%|██████▉   | 380/547 [06:58<03:04,  0.91it/s, v_num=3, train/loss_step=0.247, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|██████▉   | 381/547 [06:59<03:02,  0.91it/s, v_num=3, train/loss_step=0.247, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|██████▉   | 381/547 [06:59<03:02,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|██████▉   | 382/547 [07:00<03:01,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|██████▉   | 382/547 [07:01<03:01,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|███████   | 383/547 [07:01<03:00,  0.91it/s, v_num=3, train/loss_step=0.089, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|███████   | 383/547 [07:02<03:00,  0.91it/s, v_num=3, train/loss_step=0.0539, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|███████   | 384/547 [07:02<02:59,  0.91it/s, v_num=3, train/loss_step=0.0539, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|███████   | 384/547 [07:03<02:59,  0.91it/s, v_num=3, train/loss_step=0.0277, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|███████   | 385/547 [07:03<02:58,  0.91it/s, v_num=3, train/loss_step=0.0277, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  70%|███████   | 385/547 [07:04<02:58,  0.91it/s, v_num=3, train/loss_step=0.104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 386/547 [07:04<02:57,  0.91it/s, v_num=3, train/loss_step=0.104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 386/547 [07:05<02:57,  0.91it/s, v_num=3, train/loss_step=0.0151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 387/547 [07:05<02:56,  0.91it/s, v_num=3, train/loss_step=0.0151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 387/547 [07:06<02:56,  0.91it/s, v_num=3, train/loss_step=0.0344, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 388/547 [07:06<02:54,  0.91it/s, v_num=3, train/loss_step=0.0344, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 388/547 [07:07<02:55,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 389/547 [07:07<02:53,  0.91it/s, v_num=3, train/loss_step=0.0916, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████   | 389/547 [07:08<02:54,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████▏  | 390/547 [07:09<02:52,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████▏  | 390/547 [07:09<02:53,  0.91it/s, v_num=3, train/loss_step=0.151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████▏  | 391/547 [07:10<02:51,  0.91it/s, v_num=3, train/loss_step=0.151, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  71%|███████▏  | 391/547 [07:11<02:51,  0.91it/s, v_num=3, train/loss_step=0.0119, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 392/547 [07:11<02:50,  0.91it/s, v_num=3, train/loss_step=0.0119, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 392/547 [07:12<02:50,  0.91it/s, v_num=3, train/loss_step=0.0195, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 393/547 [07:12<02:49,  0.91it/s, v_num=3, train/loss_step=0.0195, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 393/547 [07:13<02:49,  0.91it/s, v_num=3, train/loss_step=0.332, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 394/547 [07:13<02:48,  0.91it/s, v_num=3, train/loss_step=0.332, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 394/547 [07:14<02:48,  0.91it/s, v_num=3, train/loss_step=0.0259, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 395/547 [07:14<02:47,  0.91it/s, v_num=3, train/loss_step=0.0259, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 395/547 [07:15<02:47,  0.91it/s, v_num=3, train/loss_step=0.094, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 396/547 [07:15<02:46,  0.91it/s, v_num=3, train/loss_step=0.094, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  72%|███████▏  | 396/547 [07:16<02:46,  0.91it/s, v_num=3, train/loss_step=0.00457, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 397/547 [07:16<02:45,  0.91it/s, v_num=3, train/loss_step=0.00457, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 397/547 [07:17<02:45,  0.91it/s, v_num=3, train/loss_step=0.0207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 398/547 [07:17<02:43,  0.91it/s, v_num=3, train/loss_step=0.0207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 398/547 [07:18<02:44,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 399/547 [07:19<02:42,  0.91it/s, v_num=3, train/loss_step=0.232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 399/547 [07:19<02:43,  0.91it/s, v_num=3, train/loss_step=0.0109, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 400/547 [07:20<02:41,  0.91it/s, v_num=3, train/loss_step=0.0109, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 400/547 [07:20<02:42,  0.91it/s, v_num=3, train/loss_step=0.0315, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 401/547 [07:21<02:40,  0.91it/s, v_num=3, train/loss_step=0.0315, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 401/547 [07:22<02:40,  0.91it/s, v_num=3, train/loss_step=0.0572, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 402/547 [07:22<02:39,  0.91it/s, v_num=3, train/loss_step=0.0572, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  73%|███████▎  | 402/547 [07:23<02:39,  0.91it/s, v_num=3, train/loss_step=0.400, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▎  | 403/547 [07:23<02:38,  0.91it/s, v_num=3, train/loss_step=0.400, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▎  | 403/547 [07:24<02:38,  0.91it/s, v_num=3, train/loss_step=0.0465, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 404/547 [07:24<02:37,  0.91it/s, v_num=3, train/loss_step=0.0465, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 404/547 [07:25<02:37,  0.91it/s, v_num=3, train/loss_step=0.0804, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 405/547 [07:25<02:36,  0.91it/s, v_num=3, train/loss_step=0.0804, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 405/547 [07:26<02:36,  0.91it/s, v_num=3, train/loss_step=0.0439, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 406/547 [07:26<02:35,  0.91it/s, v_num=3, train/loss_step=0.0439, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 406/547 [07:27<02:35,  0.91it/s, v_num=3, train/loss_step=0.152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 407/547 [07:27<02:34,  0.91it/s, v_num=3, train/loss_step=0.152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  74%|███████▍  | 407/547 [07:28<02:34,  0.91it/s, v_num=3, train/loss_step=0.0443, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▍  | 408/547 [07:28<02:32,  0.91it/s, v_num=3, train/loss_step=0.0443, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▍  | 408/547 [07:29<02:33,  0.91it/s, v_num=3, train/loss_step=0.0351, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▍  | 409/547 [07:30<02:31,  0.91it/s, v_num=3, train/loss_step=0.0351, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▍  | 409/547 [07:30<02:32,  0.91it/s, v_num=3, train/loss_step=0.0939, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▍  | 410/547 [07:31<02:30,  0.91it/s, v_num=3, train/loss_step=0.0939, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▍  | 410/547 [07:32<02:31,  0.91it/s, v_num=3, train/loss_step=0.0598, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▌  | 411/547 [07:32<02:29,  0.91it/s, v_num=3, train/loss_step=0.0598, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▌  | 411/547 [07:33<02:29,  0.91it/s, v_num=3, train/loss_step=0.296, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▌  | 412/547 [07:33<02:28,  0.91it/s, v_num=3, train/loss_step=0.296, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  75%|███████▌  | 412/547 [07:34<02:28,  0.91it/s, v_num=3, train/loss_step=0.00893, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 413/547 [07:34<02:27,  0.91it/s, v_num=3, train/loss_step=0.00893, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 413/547 [07:35<02:27,  0.91it/s, v_num=3, train/loss_step=0.00898, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 414/547 [07:35<02:26,  0.91it/s, v_num=3, train/loss_step=0.00898, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 414/547 [07:36<02:26,  0.91it/s, v_num=3, train/loss_step=0.0101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 415/547 [07:36<02:25,  0.91it/s, v_num=3, train/loss_step=0.0101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 415/547 [07:37<02:25,  0.91it/s, v_num=3, train/loss_step=0.00966, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 416/547 [07:37<02:24,  0.91it/s, v_num=3, train/loss_step=0.00966, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 416/547 [07:38<02:24,  0.91it/s, v_num=3, train/loss_step=0.0359, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 417/547 [07:38<02:23,  0.91it/s, v_num=3, train/loss_step=0.0359, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▌  | 417/547 [07:39<02:23,  0.91it/s, v_num=3, train/loss_step=0.0149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▋  | 418/547 [07:40<02:21,  0.91it/s, v_num=3, train/loss_step=0.0149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  76%|███████▋  | 418/547 [07:40<02:22,  0.91it/s, v_num=3, train/loss_step=0.013, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 419/547 [07:41<02:20,  0.91it/s, v_num=3, train/loss_step=0.013, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 419/547 [07:42<02:21,  0.91it/s, v_num=3, train/loss_step=0.0075, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 420/547 [07:42<02:19,  0.91it/s, v_num=3, train/loss_step=0.0075, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 420/547 [07:43<02:20,  0.91it/s, v_num=3, train/loss_step=0.0104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 421/547 [07:43<02:18,  0.91it/s, v_num=3, train/loss_step=0.0104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 421/547 [07:44<02:18,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 422/547 [07:44<02:17,  0.91it/s, v_num=3, train/loss_step=0.134, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 422/547 [07:45<02:17,  0.91it/s, v_num=3, train/loss_step=0.188, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 423/547 [07:45<02:16,  0.91it/s, v_num=3, train/loss_step=0.188, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  77%|███████▋  | 423/547 [07:46<02:16,  0.91it/s, v_num=3, train/loss_step=0.317, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 424/547 [07:46<02:15,  0.91it/s, v_num=3, train/loss_step=0.317, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 424/547 [07:47<02:15,  0.91it/s, v_num=3, train/loss_step=0.0836, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 425/547 [07:47<02:14,  0.91it/s, v_num=3, train/loss_step=0.0836, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 425/547 [07:48<02:14,  0.91it/s, v_num=3, train/loss_step=0.0555, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 426/547 [07:48<02:13,  0.91it/s, v_num=3, train/loss_step=0.0555, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 426/547 [07:49<02:13,  0.91it/s, v_num=3, train/loss_step=0.0149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 427/547 [07:50<02:12,  0.91it/s, v_num=3, train/loss_step=0.0149, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 427/547 [07:50<02:12,  0.91it/s, v_num=3, train/loss_step=0.0172, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 428/547 [07:51<02:11,  0.91it/s, v_num=3, train/loss_step=0.0172, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 428/547 [07:52<02:11,  0.91it/s, v_num=3, train/loss_step=0.0282, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 429/547 [07:52<02:09,  0.91it/s, v_num=3, train/loss_step=0.0282, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  78%|███████▊  | 429/547 [07:53<02:10,  0.91it/s, v_num=3, train/loss_step=0.0416, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▊  | 430/547 [07:53<02:08,  0.91it/s, v_num=3, train/loss_step=0.0416, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▊  | 430/547 [07:54<02:09,  0.91it/s, v_num=3, train/loss_step=0.0136, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 431/547 [07:54<02:07,  0.91it/s, v_num=3, train/loss_step=0.0136, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 431/547 [07:55<02:07,  0.91it/s, v_num=3, train/loss_step=0.0156, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 432/547 [07:55<02:06,  0.91it/s, v_num=3, train/loss_step=0.0156, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 432/547 [07:56<02:06,  0.91it/s, v_num=3, train/loss_step=0.0689, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 433/547 [07:56<02:05,  0.91it/s, v_num=3, train/loss_step=0.0689, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 433/547 [07:57<02:05,  0.91it/s, v_num=3, train/loss_step=0.104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 434/547 [07:57<02:04,  0.91it/s, v_num=3, train/loss_step=0.104, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  79%|███████▉  | 434/547 [07:58<02:04,  0.91it/s, v_num=3, train/loss_step=0.0101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|███████▉  | 435/547 [07:58<02:03,  0.91it/s, v_num=3, train/loss_step=0.0101, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|███████▉  | 435/547 [07:59<02:03,  0.91it/s, v_num=3, train/loss_step=0.010, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|███████▉  | 436/547 [08:00<02:02,  0.91it/s, v_num=3, train/loss_step=0.010, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|███████▉  | 436/547 [08:00<02:02,  0.91it/s, v_num=3, train/loss_step=0.0693, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|███████▉  | 437/547 [08:01<02:01,  0.91it/s, v_num=3, train/loss_step=0.0693, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|███████▉  | 437/547 [08:02<02:01,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|████████  | 438/547 [08:02<02:00,  0.91it/s, v_num=3, train/loss_step=0.0546, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|████████  | 438/547 [08:03<02:00,  0.91it/s, v_num=3, train/loss_step=0.00783, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|████████  | 439/547 [08:03<01:58,  0.91it/s, v_num=3, train/loss_step=0.00783, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|████████  | 439/547 [08:04<01:59,  0.91it/s, v_num=3, train/loss_step=0.0107, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|████████  | 440/547 [08:04<01:57,  0.91it/s, v_num=3, train/loss_step=0.0107, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  80%|████████  | 440/547 [08:05<01:58,  0.91it/s, v_num=3, train/loss_step=0.362, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 441/547 [08:05<01:56,  0.91it/s, v_num=3, train/loss_step=0.362, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 441/547 [08:06<01:56,  0.91it/s, v_num=3, train/loss_step=0.00599, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 442/547 [08:06<01:55,  0.91it/s, v_num=3, train/loss_step=0.00599, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 442/547 [08:07<01:55,  0.91it/s, v_num=3, train/loss_step=0.00799, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 443/547 [08:07<01:54,  0.91it/s, v_num=3, train/loss_step=0.00799, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 443/547 [08:08<01:54,  0.91it/s, v_num=3, train/loss_step=0.0115, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 444/547 [08:08<01:53,  0.91it/s, v_num=3, train/loss_step=0.0115, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████  | 444/547 [08:09<01:53,  0.91it/s, v_num=3, train/loss_step=0.0121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████▏ | 445/547 [08:10<01:52,  0.91it/s, v_num=3, train/loss_step=0.0121, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  81%|████████▏ | 445/547 [08:10<01:52,  0.91it/s, v_num=3, train/loss_step=0.404, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 446/547 [08:11<01:51,  0.91it/s, v_num=3, train/loss_step=0.404, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 446/547 [08:12<01:51,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 447/547 [08:12<01:50,  0.91it/s, v_num=3, train/loss_step=0.108, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 447/547 [08:13<01:50,  0.91it/s, v_num=3, train/loss_step=0.00625, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 448/547 [08:13<01:49,  0.91it/s, v_num=3, train/loss_step=0.00625, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 448/547 [08:14<01:49,  0.91it/s, v_num=3, train/loss_step=0.0232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 449/547 [08:14<01:47,  0.91it/s, v_num=3, train/loss_step=0.0232, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 449/547 [08:15<01:48,  0.91it/s, v_num=3, train/loss_step=0.185, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 450/547 [08:15<01:46,  0.91it/s, v_num=3, train/loss_step=0.185, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 450/547 [08:16<01:47,  0.91it/s, v_num=3, train/loss_step=0.200, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 451/547 [08:16<01:45,  0.91it/s, v_num=3, train/loss_step=0.200, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  82%|████████▏ | 451/547 [08:17<01:45,  0.91it/s, v_num=3, train/loss_step=0.0646, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 452/547 [08:17<01:44,  0.91it/s, v_num=3, train/loss_step=0.0646, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 452/547 [08:18<01:44,  0.91it/s, v_num=3, train/loss_step=0.0142, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 453/547 [08:18<01:43,  0.91it/s, v_num=3, train/loss_step=0.0142, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 453/547 [08:19<01:43,  0.91it/s, v_num=3, train/loss_step=0.243, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 454/547 [08:20<01:42,  0.91it/s, v_num=3, train/loss_step=0.243, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 454/547 [08:20<01:42,  0.91it/s, v_num=3, train/loss_step=0.0254, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 455/547 [08:21<01:41,  0.91it/s, v_num=3, train/loss_step=0.0254, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 455/547 [08:22<01:41,  0.91it/s, v_num=3, train/loss_step=0.297, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 456/547 [08:22<01:40,  0.91it/s, v_num=3, train/loss_step=0.297, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  83%|████████▎ | 456/547 [08:23<01:40,  0.91it/s, v_num=3, train/loss_step=0.0155, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▎ | 457/547 [08:23<01:39,  0.91it/s, v_num=3, train/loss_step=0.0155, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▎ | 457/547 [08:24<01:39,  0.91it/s, v_num=3, train/loss_step=0.032, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▎ | 458/547 [08:24<01:38,  0.91it/s, v_num=3, train/loss_step=0.032, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▎ | 458/547 [08:25<01:38,  0.91it/s, v_num=3, train/loss_step=0.356, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 459/547 [08:25<01:36,  0.91it/s, v_num=3, train/loss_step=0.356, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 459/547 [08:26<01:37,  0.91it/s, v_num=3, train/loss_step=0.0289, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 460/547 [08:26<01:35,  0.91it/s, v_num=3, train/loss_step=0.0289, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 460/547 [08:27<01:35,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 461/547 [08:27<01:34,  0.91it/s, v_num=3, train/loss_step=0.112, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 461/547 [08:28<01:34,  0.91it/s, v_num=3, train/loss_step=0.0326, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 462/547 [08:28<01:33,  0.91it/s, v_num=3, train/loss_step=0.0326, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  84%|████████▍ | 462/547 [08:29<01:33,  0.91it/s, v_num=3, train/loss_step=0.207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▍ | 463/547 [08:30<01:32,  0.91it/s, v_num=3, train/loss_step=0.207, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▍ | 463/547 [08:30<01:32,  0.91it/s, v_num=3, train/loss_step=0.256, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▍ | 464/547 [08:31<01:31,  0.91it/s, v_num=3, train/loss_step=0.256, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▍ | 464/547 [08:32<01:31,  0.91it/s, v_num=3, train/loss_step=0.00655, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▌ | 465/547 [08:32<01:30,  0.91it/s, v_num=3, train/loss_step=0.00655, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▌ | 465/547 [08:33<01:30,  0.91it/s, v_num=3, train/loss_step=0.0125, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▌ | 466/547 [08:33<01:29,  0.91it/s, v_num=3, train/loss_step=0.0125, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▌ | 466/547 [08:34<01:29,  0.91it/s, v_num=3, train/loss_step=0.00712, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▌ | 467/547 [08:34<01:28,  0.91it/s, v_num=3, train/loss_step=0.00712, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  85%|████████▌ | 467/547 [08:35<01:28,  0.91it/s, v_num=3, train/loss_step=0.00809, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 468/547 [08:35<01:27,  0.91it/s, v_num=3, train/loss_step=0.00809, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 468/547 [08:36<01:27,  0.91it/s, v_num=3, train/loss_step=0.335, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 469/547 [08:36<01:25,  0.91it/s, v_num=3, train/loss_step=0.335, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 469/547 [08:37<01:26,  0.91it/s, v_num=3, train/loss_step=0.0351, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 470/547 [08:37<01:24,  0.91it/s, v_num=3, train/loss_step=0.0351, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 470/547 [08:38<01:24,  0.91it/s, v_num=3, train/loss_step=0.071, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 471/547 [08:38<01:23,  0.91it/s, v_num=3, train/loss_step=0.071, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▌ | 471/547 [08:39<01:23,  0.91it/s, v_num=3, train/loss_step=0.0389, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▋ | 472/547 [08:40<01:22,  0.91it/s, v_num=3, train/loss_step=0.0389, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▋ | 472/547 [08:40<01:22,  0.91it/s, v_num=3, train/loss_step=0.366, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▋ | 473/547 [08:41<01:21,  0.91it/s, v_num=3, train/loss_step=0.366, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  86%|████████▋ | 473/547 [08:42<01:21,  0.91it/s, v_num=3, train/loss_step=0.0505, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 474/547 [08:42<01:20,  0.91it/s, v_num=3, train/loss_step=0.0505, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 474/547 [08:43<01:20,  0.91it/s, v_num=3, train/loss_step=0.0983, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 475/547 [08:43<01:19,  0.91it/s, v_num=3, train/loss_step=0.0983, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 475/547 [08:44<01:19,  0.91it/s, v_num=3, train/loss_step=0.0358, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 476/547 [08:44<01:18,  0.91it/s, v_num=3, train/loss_step=0.0358, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 476/547 [08:45<01:18,  0.91it/s, v_num=3, train/loss_step=0.0235, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 477/547 [08:45<01:17,  0.91it/s, v_num=3, train/loss_step=0.0235, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 477/547 [08:46<01:17,  0.91it/s, v_num=3, train/loss_step=0.019, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 478/547 [08:46<01:16,  0.91it/s, v_num=3, train/loss_step=0.019, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  87%|████████▋ | 478/547 [08:47<01:16,  0.91it/s, v_num=3, train/loss_step=0.0298, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 479/547 [08:47<01:14,  0.91it/s, v_num=3, train/loss_step=0.0298, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 479/547 [08:48<01:15,  0.91it/s, v_num=3, train/loss_step=0.00697, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 480/547 [08:48<01:13,  0.91it/s, v_num=3, train/loss_step=0.00697, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 480/547 [08:49<01:13,  0.91it/s, v_num=3, train/loss_step=0.0442, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 481/547 [08:50<01:12,  0.91it/s, v_num=3, train/loss_step=0.0442, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 481/547 [08:50<01:12,  0.91it/s, v_num=3, train/loss_step=0.0917, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 482/547 [08:51<01:11,  0.91it/s, v_num=3, train/loss_step=0.0917, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 482/547 [08:52<01:11,  0.91it/s, v_num=3, train/loss_step=0.084, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 483/547 [08:52<01:10,  0.91it/s, v_num=3, train/loss_step=0.084, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 483/547 [08:53<01:10,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 484/547 [08:53<01:09,  0.91it/s, v_num=3, train/loss_step=0.139, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  88%|████████▊ | 484/547 [08:54<01:09,  0.91it/s, v_num=3, train/loss_step=0.0687, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▊ | 485/547 [08:54<01:08,  0.91it/s, v_num=3, train/loss_step=0.0687, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▊ | 485/547 [08:55<01:08,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 486/547 [08:55<01:07,  0.91it/s, v_num=3, train/loss_step=0.166, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 486/547 [08:56<01:07,  0.91it/s, v_num=3, train/loss_step=0.0123, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 487/547 [08:56<01:06,  0.91it/s, v_num=3, train/loss_step=0.0123, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 487/547 [08:57<01:06,  0.91it/s, v_num=3, train/loss_step=0.0289, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 488/547 [08:57<01:05,  0.91it/s, v_num=3, train/loss_step=0.0289, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 488/547 [08:58<01:05,  0.91it/s, v_num=3, train/loss_step=0.0213, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 489/547 [08:58<01:03,  0.91it/s, v_num=3, train/loss_step=0.0213, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  89%|████████▉ | 489/547 [08:59<01:04,  0.91it/s, v_num=3, train/loss_step=0.0243, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|████████▉ | 490/547 [09:00<01:02,  0.91it/s, v_num=3, train/loss_step=0.0243, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|████████▉ | 490/547 [09:00<01:02,  0.91it/s, v_num=3, train/loss_step=0.0069, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|████████▉ | 491/547 [09:01<01:01,  0.91it/s, v_num=3, train/loss_step=0.0069, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|████████▉ | 491/547 [09:02<01:01,  0.91it/s, v_num=3, train/loss_step=0.00797, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|████████▉ | 492/547 [09:02<01:00,  0.91it/s, v_num=3, train/loss_step=0.00797, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|████████▉ | 492/547 [09:03<01:00,  0.91it/s, v_num=3, train/loss_step=0.00971, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|█████████ | 493/547 [09:03<00:59,  0.91it/s, v_num=3, train/loss_step=0.00971, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|█████████ | 493/547 [09:04<00:59,  0.91it/s, v_num=3, train/loss_step=0.027, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|█████████ | 494/547 [09:04<00:58,  0.91it/s, v_num=3, train/loss_step=0.027, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|█████████ | 494/547 [09:05<00:58,  0.91it/s, v_num=3, train/loss_step=0.0253, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|█████████ | 495/547 [09:05<00:57,  0.91it/s, v_num=3, train/loss_step=0.0253, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  90%|█████████ | 495/547 [09:06<00:57,  0.91it/s, v_num=3, train/loss_step=0.0132, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 496/547 [09:06<00:56,  0.91it/s, v_num=3, train/loss_step=0.0132, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 496/547 [09:07<00:56,  0.91it/s, v_num=3, train/loss_step=0.0643, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 497/547 [09:07<00:55,  0.91it/s, v_num=3, train/loss_step=0.0643, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 497/547 [09:08<00:55,  0.91it/s, v_num=3, train/loss_step=0.0529, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 498/547 [09:09<00:54,  0.91it/s, v_num=3, train/loss_step=0.0529, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 498/547 [09:09<00:54,  0.91it/s, v_num=3, train/loss_step=0.0171, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 499/547 [09:10<00:52,  0.91it/s, v_num=3, train/loss_step=0.0171, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████ | 499/547 [09:10<00:53,  0.91it/s, v_num=3, train/loss_step=0.0325, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████▏| 500/547 [09:11<00:51,  0.91it/s, v_num=3, train/loss_step=0.0325, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  91%|█████████▏| 500/547 [09:12<00:51,  0.91it/s, v_num=3, train/loss_step=0.0352, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 501/547 [09:12<00:50,  0.91it/s, v_num=3, train/loss_step=0.0352, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 501/547 [09:13<00:50,  0.91it/s, v_num=3, train/loss_step=0.0851, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 502/547 [09:13<00:49,  0.91it/s, v_num=3, train/loss_step=0.0851, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 502/547 [09:14<00:49,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 503/547 [09:14<00:48,  0.91it/s, v_num=3, train/loss_step=0.110, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 503/547 [09:15<00:48,  0.91it/s, v_num=3, train/loss_step=0.392, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 504/547 [09:15<00:47,  0.91it/s, v_num=3, train/loss_step=0.392, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 504/547 [09:16<00:47,  0.91it/s, v_num=3, train/loss_step=0.00565, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 505/547 [09:16<00:46,  0.91it/s, v_num=3, train/loss_step=0.00565, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  92%|█████████▏| 505/547 [09:17<00:46,  0.91it/s, v_num=3, train/loss_step=0.0225, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 506/547 [09:17<00:45,  0.91it/s, v_num=3, train/loss_step=0.0225, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 506/547 [09:18<00:45,  0.91it/s, v_num=3, train/loss_step=0.0343, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 507/547 [09:19<00:44,  0.91it/s, v_num=3, train/loss_step=0.0343, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 507/547 [09:19<00:44,  0.91it/s, v_num=3, train/loss_step=0.00637, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 508/547 [09:20<00:43,  0.91it/s, v_num=3, train/loss_step=0.00637, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 508/547 [09:21<00:43,  0.91it/s, v_num=3, train/loss_step=0.581, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 509/547 [09:21<00:41,  0.91it/s, v_num=3, train/loss_step=0.581, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 509/547 [09:22<00:41,  0.91it/s, v_num=3, train/loss_step=0.0128, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 510/547 [09:22<00:40,  0.91it/s, v_num=3, train/loss_step=0.0128, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 510/547 [09:23<00:40,  0.91it/s, v_num=3, train/loss_step=0.00728, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 511/547 [09:23<00:39,  0.91it/s, v_num=3, train/loss_step=0.00728, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  93%|█████████▎| 511/547 [09:24<00:39,  0.91it/s, v_num=3, train/loss_step=0.468, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▎| 512/547 [09:24<00:38,  0.91it/s, v_num=3, train/loss_step=0.468, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▎| 512/547 [09:25<00:38,  0.91it/s, v_num=3, train/loss_step=0.0495, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 513/547 [09:25<00:37,  0.91it/s, v_num=3, train/loss_step=0.0495, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 513/547 [09:26<00:37,  0.91it/s, v_num=3, train/loss_step=0.028, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 514/547 [09:26<00:36,  0.91it/s, v_num=3, train/loss_step=0.028, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 514/547 [09:27<00:36,  0.91it/s, v_num=3, train/loss_step=0.0535, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 515/547 [09:27<00:35,  0.91it/s, v_num=3, train/loss_step=0.0535, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 515/547 [09:28<00:35,  0.91it/s, v_num=3, train/loss_step=0.0183, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 516/547 [09:29<00:34,  0.91it/s, v_num=3, train/loss_step=0.0183, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  94%|█████████▍| 516/547 [09:29<00:34,  0.91it/s, v_num=3, train/loss_step=0.00742, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▍| 517/547 [09:30<00:33,  0.91it/s, v_num=3, train/loss_step=0.00742, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▍| 517/547 [09:31<00:33,  0.91it/s, v_num=3, train/loss_step=0.0233, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▍| 518/547 [09:31<00:31,  0.91it/s, v_num=3, train/loss_step=0.0233, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▍| 518/547 [09:32<00:32,  0.91it/s, v_num=3, train/loss_step=0.0076, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▍| 519/547 [09:32<00:30,  0.91it/s, v_num=3, train/loss_step=0.0076, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▍| 519/547 [09:33<00:30,  0.91it/s, v_num=3, train/loss_step=0.00945, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▌| 520/547 [09:33<00:29,  0.91it/s, v_num=3, train/loss_step=0.00945, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▌| 520/547 [09:34<00:29,  0.91it/s, v_num=3, train/loss_step=0.0919, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▌| 521/547 [09:34<00:28,  0.91it/s, v_num=3, train/loss_step=0.0919, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▌| 521/547 [09:35<00:28,  0.91it/s, v_num=3, train/loss_step=0.303, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▌| 522/547 [09:35<00:27,  0.91it/s, v_num=3, train/loss_step=0.303, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  95%|█████████▌| 522/547 [09:36<00:27,  0.91it/s, v_num=3, train/loss_step=0.0107, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 523/547 [09:36<00:26,  0.91it/s, v_num=3, train/loss_step=0.0107, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 523/547 [09:37<00:26,  0.91it/s, v_num=3, train/loss_step=0.124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 524/547 [09:38<00:25,  0.91it/s, v_num=3, train/loss_step=0.124, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 524/547 [09:38<00:25,  0.91it/s, v_num=3, train/loss_step=0.116, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 525/547 [09:39<00:24,  0.91it/s, v_num=3, train/loss_step=0.116, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 525/547 [09:39<00:24,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 526/547 [09:40<00:23,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▌| 526/547 [09:41<00:23,  0.91it/s, v_num=3, train/loss_step=0.111, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▋| 527/547 [09:41<00:22,  0.91it/s, v_num=3, train/loss_step=0.111, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  96%|█████████▋| 527/547 [09:42<00:22,  0.91it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 528/547 [09:42<00:20,  0.91it/s, v_num=3, train/loss_step=0.0152, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 528/547 [09:43<00:20,  0.91it/s, v_num=3, train/loss_step=0.0211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 529/547 [09:43<00:19,  0.91it/s, v_num=3, train/loss_step=0.0211, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 529/547 [09:44<00:19,  0.91it/s, v_num=3, train/loss_step=0.0429, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 530/547 [09:44<00:18,  0.91it/s, v_num=3, train/loss_step=0.0429, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 530/547 [09:45<00:18,  0.91it/s, v_num=3, train/loss_step=0.113, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 531/547 [09:45<00:17,  0.91it/s, v_num=3, train/loss_step=0.113, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 531/547 [09:46<00:17,  0.91it/s, v_num=3, train/loss_step=0.190, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 532/547 [09:46<00:16,  0.91it/s, v_num=3, train/loss_step=0.190, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 532/547 [09:47<00:16,  0.91it/s, v_num=3, train/loss_step=0.0138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 533/547 [09:48<00:15,  0.91it/s, v_num=3, train/loss_step=0.0138, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  97%|█████████▋| 533/547 [09:48<00:15,  0.91it/s, v_num=3, train/loss_step=0.122, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 534/547 [09:49<00:14,  0.91it/s, v_num=3, train/loss_step=0.122, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 534/547 [09:50<00:14,  0.91it/s, v_num=3, train/loss_step=0.0389, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 535/547 [09:50<00:13,  0.91it/s, v_num=3, train/loss_step=0.0389, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 535/547 [09:51<00:13,  0.91it/s, v_num=3, train/loss_step=0.0141, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 536/547 [09:51<00:12,  0.91it/s, v_num=3, train/loss_step=0.0141, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 536/547 [09:52<00:12,  0.90it/s, v_num=3, train/loss_step=0.0427, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 537/547 [09:52<00:11,  0.91it/s, v_num=3, train/loss_step=0.0427, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 537/547 [09:53<00:11,  0.90it/s, v_num=3, train/loss_step=0.106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 538/547 [09:53<00:09,  0.91it/s, v_num=3, train/loss_step=0.106, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  98%|█████████▊| 538/547 [09:54<00:09,  0.90it/s, v_num=3, train/loss_step=0.157, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▊| 539/547 [09:54<00:08,  0.91it/s, v_num=3, train/loss_step=0.157, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▊| 539/547 [09:55<00:08,  0.90it/s, v_num=3, train/loss_step=0.0547, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▊| 540/547 [09:55<00:07,  0.91it/s, v_num=3, train/loss_step=0.0547, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▊| 540/547 [09:56<00:07,  0.90it/s, v_num=3, train/loss_step=0.319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 541/547 [09:56<00:06,  0.91it/s, v_num=3, train/loss_step=0.319, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 541/547 [09:57<00:06,  0.90it/s, v_num=3, train/loss_step=0.215, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 542/547 [09:58<00:05,  0.91it/s, v_num=3, train/loss_step=0.215, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 542/547 [09:58<00:05,  0.90it/s, v_num=3, train/loss_step=0.032, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 543/547 [09:59<00:04,  0.91it/s, v_num=3, train/loss_step=0.032, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 543/547 [10:00<00:04,  0.90it/s, v_num=3, train/loss_step=0.0447, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 544/547 [10:00<00:03,  0.91it/s, v_num=3, train/loss_step=0.0447, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2:  99%|█████████▉| 544/547 [10:01<00:03,  0.90it/s, v_num=3, train/loss_step=0.0348, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2: 100%|█████████▉| 545/547 [10:01<00:02,  0.91it/s, v_num=3, train/loss_step=0.0348, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2: 100%|█████████▉| 545/547 [10:02<00:02,  0.90it/s, v_num=3, train/loss_step=0.0931, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2: 100%|█████████▉| 546/547 [10:02<00:01,  0.91it/s, v_num=3, train/loss_step=0.0931, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2: 100%|█████████▉| 546/547 [10:03<00:01,  0.90it/s, v_num=3, train/loss_step=0.173, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2: 100%|██████████| 547/547 [10:03<00:00,  0.91it/s, v_num=3, train/loss_step=0.173, val/loss=0.206, train/loss_epoch=0.174]\n",
      "Epoch 2: 100%|██████████| 547/547 [10:04<00:00,  0.90it/s, v_num=3, train/loss_step=0.00617, val/loss=0.206, train/loss_epoch=0.174]\n",
      "\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   1%|          | 1/118 [00:00<00:16,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   2%|▏         | 2/118 [00:00<00:16,  6.89it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 3/118 [00:00<00:16,  6.89it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   3%|▎         | 4/118 [00:00<00:16,  6.89it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   4%|▍         | 5/118 [00:00<00:16,  6.90it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   5%|▌         | 6/118 [00:00<00:16,  6.91it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   6%|▌         | 7/118 [00:01<00:16,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   7%|▋         | 8/118 [00:01<00:15,  6.92it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 9/118 [00:01<00:15,  6.93it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 10/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   9%|▉         | 11/118 [00:01<00:15,  6.93it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 12/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  11%|█         | 13/118 [00:01<00:15,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 14/118 [00:02<00:14,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  13%|█▎        | 15/118 [00:02<00:14,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  14%|█▎        | 16/118 [00:02<00:14,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 17/118 [00:02<00:14,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 18/118 [00:02<00:14,  6.94it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  16%|█▌        | 19/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  17%|█▋        | 20/118 [00:02<00:14,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 21/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  19%|█▊        | 22/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  19%|█▉        | 23/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 24/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  21%|██        | 25/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 26/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  23%|██▎       | 27/118 [00:03<00:13,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  24%|██▎       | 28/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  25%|██▍       | 29/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 30/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  26%|██▋       | 31/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  27%|██▋       | 32/118 [00:04<00:12,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 33/118 [00:04<00:12,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  29%|██▉       | 34/118 [00:04<00:12,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  30%|██▉       | 35/118 [00:05<00:11,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  31%|███       | 36/118 [00:05<00:11,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  31%|███▏      | 37/118 [00:05<00:11,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 38/118 [00:05<00:11,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  33%|███▎      | 39/118 [00:05<00:11,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  34%|███▍      | 40/118 [00:05<00:11,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  35%|███▍      | 41/118 [00:05<00:11,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 42/118 [00:06<00:10,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  36%|███▋      | 43/118 [00:06<00:10,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  37%|███▋      | 44/118 [00:06<00:10,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 45/118 [00:06<00:10,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  39%|███▉      | 46/118 [00:06<00:10,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  40%|███▉      | 47/118 [00:06<00:10,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  41%|████      | 48/118 [00:06<00:10,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 49/118 [00:07<00:09,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 50/118 [00:07<00:09,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  43%|████▎     | 51/118 [00:07<00:09,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  44%|████▍     | 52/118 [00:07<00:09,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  45%|████▍     | 53/118 [00:07<00:09,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  46%|████▌     | 54/118 [00:07<00:09,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 55/118 [00:07<00:09,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  47%|████▋     | 56/118 [00:08<00:08,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 57/118 [00:08<00:08,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  49%|████▉     | 58/118 [00:08<00:08,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 59/118 [00:08<00:08,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  51%|█████     | 60/118 [00:08<00:08,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 61/118 [00:08<00:08,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 62/118 [00:08<00:08,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  53%|█████▎    | 63/118 [00:09<00:07,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▍    | 64/118 [00:09<00:07,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 65/118 [00:09<00:07,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▌    | 66/118 [00:09<00:07,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  57%|█████▋    | 67/118 [00:09<00:07,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 68/118 [00:09<00:07,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 69/118 [00:09<00:07,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  59%|█████▉    | 70/118 [00:10<00:06,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 71/118 [00:10<00:06,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  61%|██████    | 72/118 [00:10<00:06,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 73/118 [00:10<00:06,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  63%|██████▎   | 74/118 [00:10<00:06,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▎   | 75/118 [00:10<00:06,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 76/118 [00:10<00:06,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 77/118 [00:11<00:05,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▌   | 78/118 [00:11<00:05,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  67%|██████▋   | 79/118 [00:11<00:05,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 80/118 [00:11<00:05,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▊   | 81/118 [00:11<00:05,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  69%|██████▉   | 82/118 [00:11<00:05,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 83/118 [00:11<00:05,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  71%|███████   | 84/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 85/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  73%|███████▎  | 86/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▎  | 87/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▍  | 88/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 89/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▋  | 90/118 [00:12<00:04,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  77%|███████▋  | 91/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 92/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  79%|███████▉  | 93/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  80%|███████▉  | 94/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  81%|████████  | 95/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  81%|████████▏ | 96/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 97/118 [00:13<00:03,  6.96it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  83%|████████▎ | 98/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▍ | 99/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  85%|████████▍ | 100/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 101/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▋ | 102/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  87%|████████▋ | 103/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 104/118 [00:14<00:02,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  89%|████████▉ | 105/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  90%|████████▉ | 106/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  91%|█████████ | 107/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 108/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 109/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  93%|█████████▎| 110/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▍| 111/118 [00:15<00:01,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  95%|█████████▍| 112/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▌| 113/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 114/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  97%|█████████▋| 115/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 116/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  99%|█████████▉| 117/118 [00:16<00:00,  6.95it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 118/118 [00:16<00:00,  7.00it/s]\u001b[A\n",
      "\n",
      "                                                                          \u001b[A\n",
      "Epoch 2: 100%|██████████| 547/547 [10:42<00:00,  0.85it/s, v_num=3, train/loss_step=0.00617, val/loss=0.219, train/loss_epoch=0.174]\n",
      "Epoch 2: 100%|██████████| 547/547 [10:42<00:00,  0.85it/s, v_num=3, train/loss_step=0.00617, val/loss=0.219, train/loss_epoch=0.0905]Epoch 2, global step 1641: 'val/loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\n",
      "Epoch 2: 100%|██████████| 547/547 [10:42<00:00,  0.85it/s, v_num=3, train/loss_step=0.00617, val/loss=0.219, train/loss_epoch=0.0905]\n",
      "Entrenamiento completado!\n",
      "Entrenamiento completado!\n",
      "Entrenamiento completado!\n",
      "Entrenamiento completado!\n",
      "Entrenamiento completado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Ejecuta el script de entrenamiento usando subprocess\n",
    "script_path = \"training.py\"\n",
    "\n",
    "# Verificar que el script existe\n",
    "if not os.path.exists(script_path):\n",
    "    print(f\"Error: No se encontró el archivo {script_path}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Iniciando entrenamiento del modelo BERT...\")\n",
    "\n",
    "try:\n",
    "    # Ejecutar el script con subprocess\n",
    "    process = subprocess.Popen(\n",
    "        [sys.executable, script_path],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    # Mostrar la salida en tiempo real\n",
    "    for line in process.stdout:\n",
    "        print(line.rstrip())\n",
    "    \n",
    "    # Esperar a que termine el proceso\n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(\"Entrenamiento completado exitosamente!\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Error durante el entrenamiento. Código de salida: {process.returncode}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error al ejecutar el script: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc72bd252f53429f92bbb47e110a4d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7025397419929504     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7025397419929504    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo guardado en: \n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/eaguayo/workspace/DeepLearning/Week3/bert-classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-92d789d8df38>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Cargar el mejor modelo para predicciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m trained_model = SentimentClassifier.load_from_checkpoint(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/model_helpers.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;34m\" Please call it on the class type and make sure the return value is used.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 )\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m         \"\"\"\n\u001b[0;32m-> 1662\u001b[0;31m         loaded = _load_from_checkpoint(\n\u001b[0m\u001b[1;32m   1663\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_default_map_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpl_legacy_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# convert legacy checkpoints to the new format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location, weights_only)\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         return torch.load(\n\u001b[1;32m     62\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/eaguayo/workspace/DeepLearning/Week3/bert-classifier'"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo en el conjunto de prueba\n",
    "# # Configurar Trainer de Lightning\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"sentiment_checkpoints\",\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val/loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val/loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"sentiment_logs\", name=\"imdb_sentiment\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator=\"auto\",  # Usa GPU si está disponible\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=10,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# print(\"Trainer configurado para clasificación de sentimientos IMDB\")\n",
    "\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Mejor modelo guardado en: {best_model_path}\")\n",
    "\n",
    "# Cargar el mejor modelo para predicciones\n",
    "trained_model = SentimentClassifier.load_from_checkpoint(\n",
    "    best_model_path,\n",
    "    n_warmup_steps=warmup_steps,\n",
    "    n_training_steps=total_training_steps\n",
    ")\n",
    "trained_model.eval()\n",
    "\n",
    "# Determinar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "# print(\"Modelo cargado y listo para predicciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones en nuevos textos:\n",
      "--------------------------------------------------------------------------------\n",
      "Text: This movie was absolutely fantastic! Great acting and amazing storyline.\n",
      "Predicted sentiment: NEGATIVE\n",
      "Confidence: 0.9933\n",
      "--------------------------------------------------------------------------------\n",
      "Text: I hated this film. It was boring and poorly made.\n",
      "Predicted sentiment: POSITIVE\n",
      "Confidence: 0.9965\n",
      "--------------------------------------------------------------------------------\n",
      "Text: The movie was okay, nothing special but not terrible either.\n",
      "Predicted sentiment: POSITIVE\n",
      "Confidence: 0.9787\n",
      "--------------------------------------------------------------------------------\n",
      "Text: One of the best movies I've ever seen! Highly recommended!\n",
      "Predicted sentiment: NEGATIVE\n",
      "Confidence: 0.9968\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Función para hacer predicciones en nuevos textos\n",
    "def predict_sentiment(text, model, tokenizer, device, max_length=512):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(input_ids, attention_mask)\n",
    "        prediction = torch.sigmoid(outputs).cpu().numpy()[0][0]\n",
    "    \n",
    "    sentiment = \"positive\" if prediction > 0.5 else \"negative\" \n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Ejemplos de predicción\n",
    "test_texts = [\n",
    "    \"This movie was absolutely fantastic! Great acting and amazing storyline.\",\n",
    "    \"I hated this film. It was boring and poorly made.\",\n",
    "    \"The movie was okay, nothing special but not terrible either.\",\n",
    "    \"One of the best movies I've ever seen! Highly recommended!\"\n",
    "]\n",
    "\n",
    "print(\"Predicciones en nuevos textos:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment, confidence = predict_sentiment(text, trained_model, tokenizer, device)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted sentiment: {sentiment.upper()}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0de9fee8328e49d48625d0645aa64c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b5c47d88a6040b9914073315b0979cb",
      "max": 4867,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ef45960a58b46f1ab4297303510a0c1",
      "value": 4867
     }
    },
    "0eefaf0239474e9290c1cf7aafa34e3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18ec3ca75ef547b08205157cddf65e62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_338af3b5451c402cbaafbb4ddeb8b231",
      "placeholder": "​",
      "style": "IPY_MODEL_0eefaf0239474e9290c1cf7aafa34e3c",
      "value": " 4867/4867 [01:46&lt;00:00, 40.76it/s]"
     }
    },
    "2b5c47d88a6040b9914073315b0979cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "326de4c08b8341d4975bd8a696c2e531": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "338af3b5451c402cbaafbb4ddeb8b231": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39e5f3f049014efd99c30830c4bcf017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ef45960a58b46f1ab4297303510a0c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44a47c7bdae64bf99dc826b251c16107": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65167eb675ef49d393e8283d19456fe8": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_cd229db92be4474b9a4c6258cfa3aaa9",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 1/9 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1420/1420</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:08:09 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.92it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_loss: 0.068    </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_loss: 0.175                   </span>\n</pre>\n",
         "text/plain": "\u001b[37mEpoch 1/9 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1420/1420\u001b[0m \u001b[38;5;245m0:08:09 • 0:00:00\u001b[0m \u001b[38;5;249m2.92it/s\u001b[0m \u001b[37mv_num: 0.000 train_loss: 0.068    \u001b[0m\n                                                                                 \u001b[37mval_loss: 0.175                   \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "6ec47037a37d4903bfb7f9ae80ba4925": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_39e5f3f049014efd99c30830c4bcf017",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 2/9 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1420/1420</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:08:09 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.92it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_loss: 0.170    </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_loss: 0.154                   </span>\n</pre>\n",
         "text/plain": "\u001b[37mEpoch 2/9 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1420/1420\u001b[0m \u001b[38;5;245m0:08:09 • 0:00:00\u001b[0m \u001b[38;5;249m2.92it/s\u001b[0m \u001b[37mv_num: 0.000 train_loss: 0.170    \u001b[0m\n                                                                                 \u001b[37mval_loss: 0.154                   \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "8ed03fd78e5048f08fd1371c89a22a86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90e9a1f4ab674337923b01f3677ce420": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af231bb0ad0f4290855525d9131d6bab": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_90e9a1f4ab674337923b01f3677ce420",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/9 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1420/1420</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:08:08 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.92it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_loss: 0.219</span>\n</pre>\n",
         "text/plain": "\u001b[37mEpoch 0/9 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1420/1420\u001b[0m \u001b[38;5;245m0:08:08 • 0:00:00\u001b[0m \u001b[38;5;249m2.92it/s\u001b[0m \u001b[37mv_num: 0.000 train_loss: 0.219\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "cbf5ced9f59247d2808d0b2a76918f29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd229db92be4474b9a4c6258cfa3aaa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6c3bb4f082d41bc8480549613b9ba78": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_326de4c08b8341d4975bd8a696c2e531",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 3/9 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1420/1420</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:08:09 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.92it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_loss: 0.055    </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_loss: 0.161                   </span>\n</pre>\n",
         "text/plain": "\u001b[37mEpoch 3/9 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1420/1420\u001b[0m \u001b[38;5;245m0:08:09 • 0:00:00\u001b[0m \u001b[38;5;249m2.92it/s\u001b[0m \u001b[37mv_num: 0.000 train_loss: 0.055    \u001b[0m\n                                                                                 \u001b[37mval_loss: 0.161                   \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "ff051ddfeb69414cb1acdcc21dc36775": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff7195102aa54cd78fbe3ff3e40b3733",
       "IPY_MODEL_0de9fee8328e49d48625d0645aa64c93",
       "IPY_MODEL_18ec3ca75ef547b08205157cddf65e62"
      ],
      "layout": "IPY_MODEL_8ed03fd78e5048f08fd1371c89a22a86"
     }
    },
    "ff7195102aa54cd78fbe3ff3e40b3733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbf5ced9f59247d2808d0b2a76918f29",
      "placeholder": "​",
      "style": "IPY_MODEL_44a47c7bdae64bf99dc826b251c16107",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
